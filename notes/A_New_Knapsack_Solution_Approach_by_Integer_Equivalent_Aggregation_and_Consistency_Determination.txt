Eight pages. 1997. Authors: Djangir A. Babayev; Fred Glover; Jennifer Ryan;
PDF of bad quality.

Equation (13), p. 3, "w > c_" simply states that if we multiply the items weight to by a value 'w' that's higher than the upper bound on the profit value, we satisfie inequality C^2_{22} (that will be used by the algorithm)?

The summary at the last paragraph of the first column of page 3 seems important.

Check https://github.com/henriquebecker91/masters/blob/master/codes/cpp/lib/babayev.hpp for more info on how to implement the method (implementation left unifinished).

The approach 2 for solving the consistency check (that was used to obtain the times presented on the paper's tables), isn't described on the own article.

IMPORTANT: The experiments described on the sections "First group of experiments -- [...]" and "The second group of experiments: [...]" seem very flawed.

The "First group of experiments - Problems with correlated random coefficients" have many identical items. The items are generated with weights between 1 and 1000. This alone already reduce the number of non-simple-dominated item types to 1000. The profits are uniformly distributed between floor(0.9*weight) and floor(1.1*weight). So the biggest possible efficiency is 1.1. An item type with weight 10 has 1/1000 odds of being generated; and (1/1000)*(1/3) = (1/3000) odds of being w = 10 & p = 11. An item type with those values would be an small item with the best possible efficiency (p/w = 1.1). With n = 1000, the chance of an instance having this item is 0.28350 (28%), with n = 5000 is already 0.81117 (81%), with n = 10000, is already 0.96434 (96%). For n = 250K, the odds of one instance don't having this item type are 6.34979 * 10^-37. Such item will multiple-dominate all items that are perfectly divisible by 10 and therefore can have 1.1 efficiency; all other items whose weight isn't perfectly divisible by 10 will be less efficient than it. 
The amount of different item types can be obtained by sum_{i \in [1..1000]}{floor(1.1*w_i - 0.9*w_i) + 1} what is about 100K different item types, so any excess of n over 100K will be made of repeated item types (as the types are random generated, a instance with 100K items will already have many duplicated items, and some item types missing). This makes the benchmarks with n equal to 150K, 200K and 250K not significantly harder than the ones with n = 100K, as much of the extra item will be only copies of the same item that could be removed by a O(n*log n) scan.
Except by the n = 1000 instances, the majority of the instances will have a solution composed of the 11/10 item, and one item filling the remaining space.

The "Second group of experiments - Problems with Uncorrelated Random Coefficients" is trivial to solve, as its pointed in the paper. Almost all instances are solved by the greedy procedure that simply fill the knapsack with copies of the first, and maybe the second too, most efficient items. This is again caused by the small coefficients used (weights and profits restricted to 1..1000). An item with weight 1 has 1/1000 odds of being generated; and has 500/1000 = 1/2 odds of having p > 5000; so for each item generated there's 1/2000 (0.05%) odds of generating an item that multiple-dominated ALL other items. While the max efficiency of an item with weight 1 is 1000; the max efficiency of an item with weight 2 is 500; 3 is 333 and so on. An 501/1 item will be better than the 1000/2 item (the best possible item with weight two); and therefore better than any other possible item with weight greater than one.
With n = 1000, the chance of an instance having this item is 0.39354 (39%), with n = 5000 is already 0.91796 (91%), with n = 10000, is already 0.99327 (99%). For n = 250K, the odds of one instance don't having this item type are 5.00741 * 10^-55.
Therefore for the majority of the instances, the solution was probably comprised of c copies of the same item with weight one.

