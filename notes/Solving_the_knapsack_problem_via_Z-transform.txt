Seven pages. 2002. Authors: Jean B. Lasserre; Eduardo S. Zeron
PDF of good quality.

To truly understand the paper seems to be necessary to have a mathematical background/knowledge that I (Henrique Becker) don't have at the current time (year of 2016). My interest on the paper is associated with the research of exact and efficient methods for solving the UKP (unbounded knapsack problem) as an subproblem of the BPP/CSP; in this regard, the following observations about the paper can be made:
  * "The primary goal here is not to compete with the efficient heuristics already available for hard knapsack problems as developed in [1,5] and the many references therein, not to mention the powerful LLL-type lattice reduction techniques of Lenstra, Lenstra, and LovBasz for 0-1 knapsack problems in cryptography (see, e.g. [12])." (p. 1) this excerpt combined with the lack of any computational result greatly diminish its relevance for my previously cited interests.
  * It's strange that the paper uses the term "heuristics" for the algorithms presented on references [1-5] (of the above citation) this would suggest they aren't exact methods (I need to review the [1-5] full text but by their abstracts their methods seems to be exact), and if this is the case (they aren't exact) comparing this paper's method (that seems to be exact) to them ("heuristic"/inexact methods presented on [1-5]) is comparing apples to oranges.
  * It's strange that methods for solving the 0-1 KP are mentioned on the above citation, as they were possible "competitors". The paper's seems to be about solving the UKP variant; to compare solving methods of different variants is again a case of comparing apples to oranges (while we can transform an instance of UKP to an instance of 0-1 KP; it's well known that methods designed specifically for the UKP can be much faster than using this transformation-to-0-1-KP approach).
  * The UKP is defined on a strange fashion (I'm not even completely sure it's the same problem). The problem is defined as a minimization of the profit of the items inside the knapsack (instead of a maximization); and the constraint restricts the valid solutions to the ones where the solution weight (the sum of the weight of all items that make up the solution) is 'exactly equal' to the knapsack size/capacity (instead of 'less than or equal to'). If the problem was restricted to the integers would be easy to convert an instance from one definition to the other. The minimization of negative item profits is the same as the maximization of positive item profits. Also, we can add a dummy item with weight one and profit zero; this would make the equality constraint equivalent to the 'lesser than or equal to' constraint (as a slack variable in integer programming). However, item profits are restricted to the naturals (N) and, consequently, none of those two conversions can be used (the profit can't be negative to the first conversion to work; nor it can be zero to the second conversion to work, or it can be zero, I didn't see if they specify if the naturals contain or not zero). I don't know if this is intentional (and they are using the same name for a different problem); a simple mistake (they wanted to restrict to the integers but restricted to the naturals instead); or if my poor mathematical background don't let me see something that's obvious to them.
  * "The computational complexity depends on the sum of the item weights, not on the magnitude of b as in dynamic programming based approaches." I'm not sure if this is an improvement. It's clear that exists an infinite number of UKP instances with the following property: the knapsack size (b) is bigger than the sum of the weights of all distinct item types. This family of instances seems to be the one that would benefit the most from the fact that the algorithm complexity is dependent on the 'items weight sum' and not on the 'knapsack size'. However, this family of instances will have 'number of items' * 'mean of the weight of the items types' <= 'knapsack size', therefore for a fixed 'knapsack size' we need to reduce the number of items types to increase the item types weights (and vice-versa). If we have a big number of items types, we have a small range of possible weights, and simple dominance states that if two items have the same weight (very probable on this setting) we can discard one of them without affecting the solution (we can make the problem easier by sorting the items by weight and removing all but one of the items with the same weight). If we have items types with a big weight, we will have a small number of them; and the instance will be "easy" to solve by any non-naive DP (as UKP5/GG66 or PYAsUKP/EDUK2) because: few items types mean less effort to solve each subproblem; items with big weight means a smaller number of subproblems that we need to solve (i.e. a small number of distinct knapsack sizes that we need to solve).

