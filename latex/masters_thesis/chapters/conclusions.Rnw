\chapter{Conclusions and Future Work}

\section{Conclusions}

The author believes that the major contribution of this thesis is the critical review of the UKP literature, which is summarized in the following subsection.
The review expresses and discusses statements that can be generalized to all the experimental research in computer science.
Besides the review, the more objective and UKP-specific knowledge contributions and technical contributions are listed in the subsequent subsections.

\subsection{A critical review}

\emph{An algorithm is dominated by other in the context of a dataset.}
An example presented in this thesis is MTU2.
MTU2 is the best algorithm between eight algorithms for one dataset (see Section \ref{sec:breq_exp}), and is not competitive for other five datasets (see Section \ref{sec:pya_exp}).
The literature review, and the further discussion of instance datasets and solving approaches, have shown \emph{how the choice of datasets defined the best algorithm through the last fifty years}.
The unfolding of the events can be summarized as:

\begin{description}
\item[\cite{gg-61}] A real-world application of the UKP (pricing problem for BPP and CSP) is proposed.
\item[\cite{gg-66}] Some dynamic programming algorithms for such application are proposed.
\item[\cite{mtu1}] A B\&B method is proposed, and then compared with the DP algorithms over small artificial instances, obtaining marginally better results
\item[\cite{mtu2}] The goal changes to solving larger instances faster. Artificial datasets are used for this. For such datasets, DP methods are clearly dominated by B\&B methods.
\item[\cite{zhu_dominated}] The large artificial instances are shown to have only a little amount of relevant/undominated items and are discredited.
\item[\cite{ukp_new_results}] A new DP method is proposed, together with new artificial datasets (without the same flaws of the previous datasets).
\item[\cite{eduk}] The new DP method only compares to B\&B and naive DP, the old non-naive DP algorithms were forgotten or excluded because of previous experiments.
\item[\cite{book_ukp_2004}] The new DP method is considered the state-of-the-art.
\item[\cite{pya}] The new DP method is hybridized with B\&B, and the datasets are updated to be `harder'. Such datasets are hard for B\&B, and the hybrid method is only compared to B\&B. The hybrid method is the new state-of-the-art.
\item[\cite{sea2016}] An old DP method is reinvented and outperforms the hybrid method in the updated datasets.
\item[(this thesis)] Old algorithms are revised, reimplemented and tested. The influence of the datasets and the historical context becomes apparent.
\end{description}
It is worth mentioning that, taking into account the historical context, the choices made by previous researchers were sound and reasonable.
Despite this, an efficient DP algorithm was ignored for decades when it was relevant for the comparisons and experiments realized.

This leads to another conclusion: \emph{the bibliographical research is important, and should be followed by a reevaluation of the evidence chain.}
The author first reinvented an old DP algorithm, published a paper about how it surpassed the state-of-the-art, and then discovered that his bibliographical research was faulty.
While the author does not intend to justify this overlook, it is important to note that, in the context, reading papers about algorithms from fifty years ago, and implementing them to compare with a recent state-of-the-art algorithm did not seem to be as a good use of time, as they were already compared and discarded by works of the same period.
If the author did not end up accidentally reinventing the algorithm, and then recognizing it in an old paper, it is possible that the \emph{terminating/ordered step-off} would remain ignored.
A possibility that can only be considered more worrying by the fact that the algorithm pseudocode is available in \cite{gg-66}, a paper that is cited by many of the works mentioned above.
Implementing (or obtaining) all algorithms already proposed for a problem clearly is not a viable strategy.
However, this work shows that the context of one experiment that concludes that an algorithm is dominated by other has to be critically evaluated.
% TODO: reformular a frase acima

Also, \emph{papers and experiments do not always provide all relevant context.}
One example of context that is not always provided, or given much attention, is if the runs were executed in parallel, or not.
The experiments from Section \ref{sec:serial_parallel_exp} show a significant difference between performing an experiment with runs in parallel, or serially.
The magnitude of the difference vary with the specific algorithm, computer and dataset.
In one of the computer settings, the average times of UKP5 when executed in parallel were about the double of the average serial times.
In an indirect comparison between results presented by different papers, this detail could lead to a method being considered significantly faster than other only because one author executed serial runs and the other parallel runs.

\section{UKP-specific knowledge contributions}

An outline of the UKP-specific knowledge contributions follows. It is ordered by the level of importance the author gives to them.
\begin{enumerate}
\item The knowledge that an old DP algorithm outperforms a state-of-the-art hybrid algorithm, in the most recent benchmark dataset.
\item The concept of solution dominance, and its implications for periodicity bounds and the four previously established dominance relations.
\item Evidence that the B\&B approach worst-case can arise when solving pricing subproblems of BPP/CSP.
\item Evidence that variations in the optimal solution returned by the pricing problem solver can have a strong effect in the number of pricing problems generated.
\item Evidence that UKP algorithms that are memory intensive should be executed serially for comparison.
\item Evidence that converting the pricing problem profit values to large integers do not cause significant loss.
\end{enumerate}

The author believes that the first contribution is already well exposed, either in Section \ref{sec:pya_exp}, or in \cite{sea2016}.%, and there's nothing to add to its discussion.
The technical details of the second contribution (i.e. the concept of solution dominance) were already discussed (see Section \ref{sec:ukp5_sol_dom_expl}), but not how it impacts the previous techniques described in the literature.
The weak solution dominance reduces the further improvement that can be found by applying the four dominance relations and/or periodicity bounds in the same algorithm.
The weak solution dominance and the four dominance relations are two different ways of dealing with the same task.
The first involves keeping an index in each solution to mark which items can still be added to the solution.
The second involves keeping a global list of undominated items, 
%Applying one of the approaches reduce how much the other can further improve the algorithm.

The approach used by EDUK gives a strong guarantee that any dominated item will be discarded and never used again.
However, the weak solution dominance described in Section \ref{sec:ukp5} is implemented with almost no overhead, and seems to have a very similar impact in the removal of dominated items/solutions.
One could argue that EDUK can be at disadvantage for being implemented in a functional language, or that better implementations of the algorithm could be written, the author can not refute such claims.
Maybe new implementations of the EDUK approach can show the superiority of applying the four dominances in the EDUK fashion.
However, for the tested datasets, the weak solution dominance approach seems to be the most efficient one.

The periodicity check exists both in algorithms like EDUK/EDUK2 and the old terminating step-off.
In EDUK/EDUK2 it is a consequence of applying all four dominances repeatedly, and in the terminating step-off it is a consequence of applying weak solution dominance.
A periodicity check can save effort by stopping the computation at a capacity~\(y < c\).
However, in all algorithms that implement the periodicity check, when this early termination happens, it is because the only item that could possibly be used is the best item.
Consequently, in each one of these last positions (between \(y\) and \(c\)), the algorithm would not execute \(O(n)\) steps anymore, but only \(O(1)\) steps.
The periodicity check only saves the effort of iterating these last \(c - y\) positions.
It is a minor improvement over the application of weak solution dominance, or the application of the four item dominances.

The periodicity check (and, by consequence, the dominances) also reduces the utility of periodicity bounds.
If an upper bound on \(y^+\) could be used to stop the computation before it reaches capacity \(c\), then the periodicity check would stop the computation even before the capacity predicted by the upper bound (with slightly more overhead).
In an algorithm with periodicity checking, the utility of upper bounds on the periodicity capacity~\(y^+\) is restricted to saving memory and saving the time spent initializing such saved memory.
Note that some algorithms would not even have such benefits, as they do not allocate or initialize the memory in advance.

The evidence that constitutes the third knowledge contribution can be found in Section \ref{sec:csp_experiments}.
In the majority of the instances, the times spent by both the B\&B and DP approaches when solving pricing problems was significantly smaller than the times solving the master problems of the same instance.
Nevertheless, for some instances, the B\&B approach has presented its worst-case behavior and, in these instances, more time was spent solving pricing subproblems then solving the master problem.

For a single example, we will focus on instance 201\_2500\_DI\_11.txt, for which MTU1\_CUTSTOCK ended in timeout.
The pricing problems generated when solving such instance have \(n < 200\) and \(c = 2472\).
Before timeout, MTU1 solved about 700 of those pricing problems in much less than a millisecond each.
However, there was also a few pricing problems that took ten seconds or more to solve; times like: 10, 12, 13, 13, 26, 32, 49, 54, and 351 seconds.
All instances shared the same \(n\), \(c\) and the same items weights, the only difference between them is the profit values.
Such behaviour corroborates with what was said about B\&B algorithms being strongly affected by the items distribution, and less by \(n\) and \(c\).

It is worth mentioning that almost all instances that ended in timeout are artificial instances created to be hard to solve.
They were proposed in \cite{survey2014}.
Unfortunately, the author of this thesis did not have the time to gather CSP problems from industrial sources, and the experimentation had to stop at the literature instances.
Consequently, we cannot state that B\&B algorithms are not viable for solving pricing problems, only that there is evidence that the B\&B worst-case can arise in such circumstances.

While the following quote was written in the context of the 0-1 KP, the author found it relevant to complement what was just said: ``Dynamic programming is one of our best approaches for solving difficult (KP), since this is the only solution method which gives us a worst-case guarantee on the running time, independently on whether the upper bounding tests will work well.''~\cite[p.~13]{where_hard}.

%In \cite{gg-63}, an B\&B algorithm was used for solving pricing subproblems, it's said that DP used too much time and had to be abandoned.
%However, in \cite{gg-66}, the same authors only presented DP methods for solving the UKP.
%The author of this thesis did not contact Gilmore and Gomory and can only speculate if they reached the same conclusions.

The last three contributions are thoroughly discussed in the context of the experiments that originated them (for the fourth and sixth contributions see Section \ref{sec:csp_experiments}, and for the fifth, Section \ref{sec:serial_parallel_exp}).
The fourth contribution is specially important to a researcher testing an algorithm that needs to solve unbounded knapsack pricing problems.
Minor changes in the operation of the UKP solver, as the order it receives the items, can change the exact optimal solution returned, and consequently the number of pricing problems generated and the whole algorithm runtime.
Such chaotic behaviour can lull an experimenter into believing that irrelevant changes have some deeper meaning, or add noise to relevant changes.
The same can be said about executing memory intensive algorithms in serial, or parallel (fifth contribution).
The sixth and last contribution, while corroborated by the vast majority of the runs, maybe has contributed to our only outlier, and needs further validation.

\section{Technological UKP-specific contributions}

An outline of the UKP-specific technological contributions follows.

\begin{itemize}
\item The only known implementations of the GREENDP and GREENDP1 algorithms, modernized to use loops.
\item The UKP5 implementation, that can be seen as a variant of the terminating step-off.
\item New implementations for MTU1 and MTU2. Such implementations are coded in C++11 (instead of Fortran77) and use generic templates. Both implementations are slight faster than the respective Fortran77 implementations, and the MTU2 C++ implementation does not have the problem with the subset-sum instances presented by the Fortran77 implementation.
\item A copy\footnote{Available at: \url{https://drive.google.com/open?id=0B30vAxj_5eaFSUNHQk53NmFXbkE}} of the exact PYAsUKP benchmark used, and scripts\footnote{Available at: \url{https://github.com/henriquebecker91/masters/tree/f5bbabf47d6852816615315c8839d3f74013af5f/codes/sh/pyasukp_paper_bench}. Note that a PYAsUKP binary compiled from the version in \url{https://github.com/henriquebecker91/masters/blob/f5bbabf47d6852816615315c8839d3f74013af5f/codes/ocaml/pyasukp_mail.tgz} has to be in the executable path to the scripts work.} to generate it (based on PYAsUKP).
\item The BREQ 128-16 Standard Benchmark, which is a new dataset.
\end{itemize}

The author will not discuss such technical contributions as they are only a byproduct of the research.
It is only relevant to note that the author does not suggests that the BREQ 128-16 Standard Benchmark should be used for new experiments comparing algorithms for the UKP.
The dataset can yet be used to study the behaviour of new solving approaches, maybe giving some insight about the approach inner workings.
``If the operations research profession is to be successful, it is because it helps solve real problems, not imagined problems.''~\cite[p.~7]{portable_rng}.

\section{Future Work}
\label{sec:future_works}

Unfortunately, many questions could not be answered in the scope of this thesis.
The author selected and listed some of such questions in this section.

\begin{itemize}
\item Which other real-world problems have the UKP as a pricing subproblem? Are there real-world problems that can be modelled as the UKP? Would instances of those problems be easy for the current state-of-the-art algorithm?
\item How similar are the datasets of the UKP and the BPP/CSP presented in the literature to the ones existent in the real-world?
\item Do the instances found in the real-world benefit some approaches over others?
\item Would a hybrid algorithm based on the UKP5/`terminating step-off' and MTU2 would present the same level of improvement that EDUK2 has over EDUK?
\item Regarding the many times mentioned but never reimplemented CA algorithm from \cite{babayev}: what would be its performance in real-world instances? Or, at least, in the most recent benchmark datasets?
\item How do the traits of the optimal solution for the pricing subproblem affect the master problem? Does always returning an optimal solution with minimal weight has a negative effect? What about adding all patterns that improve the master problem solution, and not only the best pattern (i.e. optimal solution)?
\item How often do pricing subproblems have multiple optimal solutions? How many of these optimal solutions are cut down because of dominance?
\item How would an optimized C++ implementation of EDUK(2) perform? How would EDUK(2) perform over real-world instances?
\item Are the profit values (and, consequently, the items distributions) of the pricing subproblems uniform between similar BPP/CSP instances, and/or the same BPP/CSP instance? Is it possible that they converge to a specific distribution at each iteration of the column generation?
\end{itemize}

