\chapter{Conclusions and Future Work}

\section{Conclusions}

the concept of periodicity is of little relevance for algorithms that apply threshold or solution dominance, in the end, the reduction of the knapsack size will only reduce the number of iterations where the code was already only adding copies of the best item instead of iterating over all items

basically there's a lot to do if someone wants to organize the work about this single problem, the question if such effort would pay off for such a easy NP-hard problem with a shortage of real world problems that need to solve it (or if they exist, need to solve it faster, maybe the instances are very easy anyway). The problem is NP-hard, the worst-cases are well-know and studied for the most common approachs (B\&B and DP), so the relevant would the average-case of the specific methods over instances with specific useful distributions
Many problems are grouped in similar complexity classes and the quality of the implementation (and optimizations that don't change assimptotic complexity) make all the difference, so experimental analysis is relevant, yet, experimental analysis over artificially generated that don't mimic real-world problems is hardly defensable.
The lack of literature review and implementation of old methods for comparison is a real problem.

solution dominance make the other domincances little relevant?

%That the whole UKP literature has many problems: David Johnson - Pitfall 4 "Start by using randomly generated instances to evaluate the behaviour of algorithms, but end up using algorithms to investigate the properties of randomly-generated instances."; David Johnson "[...] problems without applications do not have real-world instances, so the experimenter is left to invent (and justify) test data in a vacuum. And referees may well question the testing of code for algorithms that will never be used in practice." (p. 5). We are researching instances that are hard for UKP, not that this has a utility on itself.

\section{Future Works}

There's lots of methods for UKP solving, any interessed should do a good review before thinking in creating a new solution
while a survey implementing all algorithms could be interesting, it only would be valid if tackled real-world instances
	for the main real-world problem cited at the UKP literature (CSP pricing), it isn't used so much anymore, and there's questions that precede the problem of the fastest UKP solving algorithm: as if adding multiple solutions simulstaneosly is a good idea, or if we should only try to get the first solutions better than 1 (and use a exact method only after failing in this prospect), etc...

Many algorithms weren't re-implemented; so much comparison is lacking. Specially Fred Glover's algorithm.

% Thing interesting to analyze would be: how often knapsacks generated by cutting stock have different optimal solutions (or specifically distinct optimal solutions with the same weight, also, the specific case where the weight is the smallest possible, seems to be very common as the change in the ordering shown so many CSP instances where at least one knapsack diverged); would be adding all optimal solutions (or even, all solutions with profit value over one) at each single knapsack a way to speed up the computation? (there would be any negative effects, as the master model bloating? the trade-off would be valid? someone has already proposed this?) Columns can end up not being used even if choosen between the optimal ones. Dominance exclude solutions with the guarantee at least one optimal solution will remain, disabling dominance would pay off? Varying the CPLEX seed and the choosen optimal solution would make differences of wich magnitude? Could code make use of the gigantic profit dominance of the instances?
% If we could discover what property the solutions given by the CPlex knapsack solver have that makes the master model needs less iterations, we could adapt the other programming methods to return optimal solution with this property?

% CITATIONS OF Should_We_Use_a_Portable_Generator_in_an_Emergency.txt
% "We can think of three reasons for not using a random problem generator: [...] b) solving lots of randomly generated problems gives a false sense of having thoroughly tested an algorithm [...]" (p. 5) and "To illustrate (b), consider the unbounded or general integer knapsack problem." (p. 6) and "A plausible, and in fact common, way of generating random instances of this problem is to let [...]" (p. 7) and "Thus, even though we might ostensibly solve, say, 20 different big randomly generated problems, we are in fact only solving essentially one modest size underlying problem 20 times. This seems a less thorough test of an algorithm than one would like. Solving 20 different problems from 20 different industrial sources would be more reassuring." (p. 7)
%"If the operations research profession is to be successful, it is because it helps solve real problems, not imagined problems." (p. 7) 
%"It is expensive and time consuming to collect real industrial problems. A random problem generator may be the only alternative if you want a problem with a particular characteristic, e.g. large size, quickly." (p. 8) 
