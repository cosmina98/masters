\chapter{Approaches and Algorithms}

In this chapter, some approaches and algorithms for solving UKP will be discussed.
The objective of the chapter is to give the readers a base to understand the results of the experiments in Section~\ref{sec:exp_and_res}, and to explain the concept of solution dominance mentioned in Section~\ref{sec:dom_rel}.
The results of the experiments are many times aligned with what was said in last section, and more developed here, that some approaches favor some item distributions.

%Two techniques are often used for solving UKP: dynamic programming (DP)~\cite{eduk},~\cite[p. 214]{garfinkel},~\cite[p. 311]{tchu} and branch and bound (B\&B)~\cite{mtu2}. 
%The DP approach has a stable pseudo-polynomial time algorithm linear on the capacity and number of items. 
%The B\&B approach can be less stable. 
%It can be faster than DP on instances with some characteristics, such as when the remainder of the division between the weight of the best item by the capacity is small; or the items have a big efficiency variance. Nonetheless, B\&B has always the risk of an exponential time worst case.
% The state-of-the-art solver for the UKP, introduced by~\cite{pya}, is a hybrid solver that combines DP and B\&B. 
%It tries to solve the problem by B\&B, and if this fails to solve the problem quickly, it switches to DP using some data gathered by the B\&B to speed up the process. 
%The solver's name is PYAsUKP, and it is an implementation of the EDUK2 algorithm.

\section{Conversion between problem types}

In Section~\ref{sec:introduction}, it was pointed that UKP can be seen as a special case of BKP where, for each item type~\(i\), there's at least~\(\floor{\frac{c}{w_i}}\) copies of that item type available in an instance.
Consequently, it is possible to convert any instance of the UKP in an instance of the BKP, and solve it with an algorithm designed to solve the BKP.
In this work, this approach will not be used or thoroughly studied.
The rationale for this choice is that such approach can't yield competitive performance results, for reasons that are easy to explain.

An algorithm designed to solve the BKP needs a mechanism to prevent solutions from exceeding the available amount of each item.
An algorithm designed for the UKP doesn't have this overhead.
An algorithm designed for the UKP needs to keep track of the items used in the solutions, but doesn't need to frequently access this information (as to check if it can adds an additional copy of one item to a solution).
Also, an algorithm designed for the UKP can fully exploit the properties described in Section~\ref{sec:well_known_prop}.

In~\cite{mtu1}, experiments converting instances of the UKP into instances of the BKP were realized.
The experiments yielded the expected result (i.e. the BKP algorithms performed poorly in comparison to UKP-specific algorithms).
The experiment's evidence is fragile, because only small instances were used.
However, based in the rationale exposed in the last paragraph, the author believes it's safe to assume that, for the same instance of the UKP, a state-of-the-art algorithm for the UKP will greatly outperform a state-of-the-art algorithm for the BKP.

\section{Dynamic Programming}
\label{sec:dp_algs}

The Dynamic Programming (DP) approach is the oldest found in the literature review (Section~\ref{sec:prior_work}).
Its worst-case time complexity is~\(O(nc)\) (pseudo-polynomial).
The DP approach can be considered stable, or predictable, compared to other approaches.
Stable in the sense that its time variation when solving many instances with the same characteristics (i.e.~\(n\),~\(c\) and items' distribution) can be lower than other approaches. 
Predictable in the sense that it's easier to predict a reasonable time interval for solving an instance based in the characteristics just mentioned, than it's with other approaches.

The DP worst-case space complexity is~\(O(n + c)\), which can be considerably greater than other approaches (that don't allocate memory linear to~\(c\)).
However, the space needed can be reduced by many optimizations.
Some of these optimizations are: using a periodicity bound as explained in Section~\ref{sec:periodicity}; using modular arithmetic to reduce~\(c\) to~\(w_{max}\) in at least one array, see~\cite[p.~17]{gg-66}; using binary heaps instead of arrays, as the heap can use less memory than an array of~\(c\) positions if~\(w_{min}\) is sufficiently big.

Also, the DP approach often gives an optimal solution for each capacity smaller then~\(c\).
However, some space optimizations can remove such feature.

\subsection{The naive algorithm}

In this work, the author sometimes refer to the basic (or naive) DP algorithm for the UKP.
The naive algorithm is an algorithm specifically designed for the UKP, but that applies basically no optimization.
It's the most straightforward implementation of the recursion that describes the problem.
This implementation will always execute about~\(n\times c\) operations (the worst case).
It does not implement any dominance relation\footnote{The only exception is that when two or more distinct solutions have the same weight only one of them is kept.}, does not use periodicity, or prune symmetric solutions. 

\begin{algorithm}[!t]
\caption{Naive DP algorithm}\label{alg:naive_dp}
\begin{algorithmic}[1]
\Procedure{NaiveDP}{$n, c, w, p, w_{min}$}
  \State~\(g \gets\) array of~\(c + 1\) positions, range~\([0,~w_{min})\) initialized to~\(0\)
  \State~\(d \gets\) array of~\(c + 1\) positions, range~\([0,~w_{min})\) initialized to~\(0\)
  
  \For{\(y \gets w_{min},~c\)}
    \State~\(g[y] \gets 0\)
    \For{\(i \gets1,~n\)}
      \If{\(w_i > y\)}
      	\State \textbf{end inner loop}
      \EndIf
      \If{\(g[y - w_i] + p_i > g[y]\)}
        \State~\(g[y] \gets g[y - w_i] + p_i\)
        \State~\(d[y] \gets i\)
      \EndIf
    \EndFor
  \EndFor
  \State \textbf{return}~\(g[c]\)
\EndProcedure
\end{algorithmic}
\end{algorithm}

An implementation of the naive DP algorithm can be seen in Algorithm~\ref{alg:naive_dp}.
The notation is basically the one already introduced in Section~\ref{sec:formulation}.
The letter~\(y\) will be used in this and other algorithms to denote an arbitrary capacity value.
This implementation of the algorithm considers that the items~\(i \in \{1,~...,~n\}\) are ordered by non-decreasing weight (i.e.~\(w_1 \leq w_2 \leq ... \leq w_n\)).
The arrays will always be base zero, and the items list base one.
The procedure to recover the items that comprise the optimal solution will not be given for this and the remaining DP algorithms because, in general, it's the same procedure explained in UKP5.

\subsection{Garfinkel and Nemhauser Algorithm}

The algorithm given in~\cite[p.~221]{garfinkel} can be seen as variation of the naive DP algorithm (see Algorithm~\ref{alg:gar_dp}).
While it does not seems to be much of an improvement, the use of the~\(i \leq d[y - w_i]\) test eliminates solution symmetry.
The test, together with the items being ordered by non-increasing efficiency, can considerably improve the running times.
The condition can also be seen as: add new items to pre-existing solutions only and only if the new items are the most efficient item already in the solution, or an item even more efficient.

\begin{algorithm}[!t]
\caption{Garfinkel's DP algorithm}\label{alg:gar_dp}
\begin{algorithmic}[1]
\Procedure{GarDP}{$n, c, w, p, w_{min}$}
  \State~\(g \gets\) array of~\(c + 1\) positions, range~\([0,~w_{min})\) initialized to~\(0\)
  \State~\(d \gets\) array of~\(c + 1\) positions, range~\([0,~w_{min})\) initialized to~\(n\)
  
  \For{\(y \gets w_{min},~c\)}
    \State~\(g[y] \gets 0\)
    \For{\(i \gets1,~n\)}
      \If{\(w_i \leq y~\) \textbf{and}~\(i \leq d[y - w_i]\) \textbf{and}~\(g[y - w_i] + p_i > g[y]\)}
        \State~\(g[y] \gets g[y - w_i] + p_i\)
        \State~\(d[y] \gets i\)
      \EndIf
    \EndFor
  \EndFor
  \State \textbf{return}~\(g[c]\)
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{The Step-Off Algorithms from Gilmore and Gomory}
\label{sec:gg_algs}

Four DP algorithms for the UKP are described in~\cite[p.~14~to~17]{gg-66}. 
With the exception of the first algorithm, each one of the three remaining algorithms is an improvement of the previous one.
The second and third algorithms (respectively, the `ordered step-off' and the `terminating step-off') are very similar to UKP5.
The second algorithm is basically UKP5 without a periodicity check, and the third algorithm is an UKP5 with a different periodicity check.
To avoid replication, we will ignore small implementation differences, and present only UKP5, in the next section. 
The first and the fourth DP algorithms can be ignored because: the first is dominated by the second/third versions; the fourth algorithm reduce memory usage at cost of a little extra processing (not an interesting trade-off in the context of this work).

As already mentioned, the author of this thesis proposed UKP5 in~\cite{sea2016}, thinking it was novel.
The UKP5 algorithm was thought as an improvement of the Garfinkel's DP Algorithm presented in last section. 
The~\cite{gg-66} was not checked at time because it was cited in~\cite{garfinkel}, and the author did not expect the book to provide a worsened version of the algorithm in purpose.
In the Section 6.4 of the book, a DP algorithm is presented as the last of a series of improvements over the naive DP algorithm.
However, if we check the chapter notes, there's the comment ``6.4: The recursion of this section is based on Gilmore and Gomory (1966).
See Exercise 21 for a variation that will be more efficient for some data sets.''.
The algorithm presented in Section 6.4 was a version of the algorithm in~\cite{gg-66} with \emph{many} of its relevant optimizations removed, and in exercise 21 it is expected of the reader to recreate \emph{one} of these optimizations based on hints given at the exercise.
The author of this thesis believes that this fact went unnoticed by previous authors that cited the book.
The book didn't feature the exercises' answers.

% After the 1.B Ordered Step-Off, there's a `terminating' version of it that uses a different version of my periodicity check, The gg check needs a vector with the max weight between the first and the item ix when they are ordered by efficiency (ex: for w = {3, 5, 2, 8}, the array would be w' = {3, 5, 5, 8}), at position y it checks if y < y' + w'[d[y]]. The y' variable is update when y reaches a solution where d[y] > 0 && g[y] > 0 (a solution that the most efficientitem used isn't the best item). The gg way is better as solutions stored that are changed to use the best item don't count. The UKP5 version don't use the extra array, but if a position is first saved with a bad solution and after changed to a good solution that uses the best item, tha position is yet marked as used by a non-best item, while the gg version will not. 
% And there's the 1.C that uses a w_max positions for one of the arrays. Saving memory and using the cache memory better.
% MAYBE FAST IMPLEMENT BOTH METHODS AND ADD THEM TO THE THESIS? IF NOT TO AN REVISED VERSION THAT WILL BE PUBLISHED ONLINE, AND SHOW THE BEST VERSION OF THE GILMORE AND GOMORY ALGORITHM?

\subsection{UKP5}
\label{sec:ukp5}

UKP5 was inspired by the DP algorithm described by Garfinkel~\cite[p. 221]{garfinkel}. 
The name ``UKP5'' is due to five improvements applied over that algorithm:

\begin{enumerate}
  \item \textbf{Symmetry pruning}: symmetric solutions are pruned in a more efficient fashion than in~\cite{garfinkel};
  \item \textbf{Sparsity}: not every position of the optimal solutions value array has to be computed;
  \item \textbf{Dominated solutions pruning}: dominated solutions are pruned;
  \item \textbf{Time/memory trade-off}: the test~\(w_i \leq y\) from the algorithm in~~\cite{garfinkel} was removed, the trade-off was using O(\(w_{max}\)) memory;
  \item \textbf{Periodicity}: the periodicity check suggested in~\cite{garfinkel}, but not implemented there, was adapted and implemented.
\end{enumerate}

As already pointed, UKP5 is very similar to the ordered step-off algorithm from~\cite{gg-66}.
Aside for minor adaptations, this section is the same as the one presented in~\cite{sea2016}, written when the author was not yet aware of those similarities.
The discussion about the about the similarities between UKP5 and the DP algorithms from Gilmore and Gomory is restricted to Section~\ref{sec:gg_algs}.

\begin{algorithm}[!t]
\caption{UKP5 -- Computation of $opt$}\label{alg:ukp5}
\begin{algorithmic}[1]
\Procedure{UKP5}{$n, c, w, p, w_{min}, w_{max}$}
  \State~\(g \gets\) array of~\(c + w_{max} - w_{min}\) positions each one initialized with~\(0\)\label{create_g}
  \State~\(d \gets\) array of~\(c + w_{max} - w_{min}\) positions, not initialized
  
  \For{\(i \gets 1, n\)}\label{begin_trivial_bounds}\Comment{Stores one-item solutions}
    \If{\(g[w_i] < p_i\)}
      \State~\(g[w_i] \gets p_i\)
      \State~\(d[w_i] \gets i\)
    \EndIf
  \EndFor\label{end_trivial_bounds}

  \State~\(opt \gets 0\)\label{init_opt}

  \For{\(y \gets w_{min}, c - w_{min}\)}\label{main_ext_loop_begin}\Comment{Can end early because of periodicity check}
    \If{\(g[y] \leq opt\)}\label{if_less_than_opt_begin}\Comment{Handles sparsity and prunes dominated solutions}
    	\State \textbf{continue}\label{alg:continue}\Comment{Ends current iteration and begins the next}
    \EndIf\label{if_less_than_opt_end}
    
    \State~\(opt \gets g[y]\)\label{update_opt}
    
    \For{\(i=1,d[y]\)}\label{main_inner_loop_begin}\Comment{Creates new solutions (never symmetric)}
      \If{\(g[y + w_i] < g[y] + p_i\)}\label{if_new_lower_bound_begin}
        \State~\(g[y + w_i] \gets g[y] + p_i\)
        \State~\(d[y + w_i] \gets i\)
%      \ElsIf{\(g[y + w_i] = g[y] + p_i \land i < d[y + w_i]\)}
%        \State~\(d[y + w_i] \gets i\)
      \EndIf\label{if_new_lower_bound_end}
    \EndFor\label{main_inner_loop_end}
  \EndFor\label{main_ext_loop_end}

  \For{\(y \gets c - w_{min} + 1, c\)}
    \If{\(g[y] > opt\)}
      \State~\(opt \gets g[y]\)
    \EndIf
  \EndFor
  \State \textbf{return}~\(opt\)

%  \For{\(y \gets c-w_{min}+1, c\)}\label{get_y_opt_loop_begin}\Comment{Removal of dominated solutions}
%    \If{\(g[y] > opt\)}\label{last_loop_inner_if}
%      \State~\(opt \gets g[y]\)
%      \State~\(y_{opt} \gets y\)
%    \EndIf
%  \EndFor\label{get_y_opt_loop_end}
\EndProcedure
\end{algorithmic}
\end{algorithm}

A pseudocode of our algorithm is presented in Algorithm~\ref{alg:ukp5}.
We have two main data structures, the arrays~\(g\) and~\(d\), both with dimension~\(c + w_{max} - w_{min}\). 
The~\(g\) is a sparse array where we store solutions profit.
If~\(g[y] > 0\) then there exists a non-empty solution~\(s\) with~\(w_s = y\) and~\(p_s = g[y]\). 
The~\(d\) array stores the index of the last item used on a solution.
If~\(g[y] > 0 \land d[y] = i\) then the solution~\(s\) with~\(w_s = y\) and~\(p_s = g[y]\) has at least one copy of item~\(i\). 
This array makes it trivial to recover the optimal solution, but its main use is to prune solution symmetry.

Our first loop (lines~\ref{begin_trivial_bounds} to~\ref{end_trivial_bounds}) simply stores all solutions comprised of a single item in the arrays~\(g\) and~\(d\). 
For a moment, let's ignore lines~\ref{if_less_than_opt_begin} to~\ref{if_less_than_opt_end}, and replace~\(d[y]\) (at line~\ref{main_inner_loop_begin}) by~\(n\). 
With these changes, the second loop (between lines~\ref{main_ext_loop_begin} and~\ref{main_ext_loop_end}) 
iterates~\(g\) and when it finds a stored solution (\(g[y] > 0\)) it tests~\(n\) new solutions 
(the combinations of the current solution with every item). 
The new solutions are stored at~\(g\) and~\(d\), replacing solutions already stored if the new solution has the same weight but a greater profit value.

When we add the lines~\ref{if_less_than_opt_begin} to~\ref{if_less_than_opt_end} to the algorithm, it stops creating new solutions from dominated solutions. 
%We use the \textbf{continue} keyword at line~\ref{alg:continue} meaning the current iteration ends, and the next iteration begins (as is common in C-like programming languages). 
If a solution~\(s\) with a smaller weight (\(w_s < y\)) has a bigger profit (\(p_s = opt > p_t\), where~\(w_t = y \land p_t = g[y]\)), then~\(s\) dominates~\(t\). 
If a solution~\(s\) dominates~\(t\) then, for any item~\(i\), the~\(s \cap \{i\}\) solution will dominate the~\(t \cap \{i\}\) solution. 
This way, new solutions created from~\(t\) are guaranteed to be dominated by the solutions created from~\(s\). 
A whole superset of~\(t\) can be discarded without loss to solution optimality. 

The change from~\(n\) to~\(d[y]\) is based on the algorithm from~\cite{garfinkel} and it prunes symmetric solutions.
In a naive DP algorithm, if the item multiset~\(\{5, 3, 3\}\) is a valid solution, then every permutation of it is reached in different ways, wasting processing time. 
To avoid computing symmetric solutions, we enforce non-increasing order of the items index. 
Any item inserted on a solution~\(s\) has an index that is equal to or lower than the index of the last item inserted on~\(s\). 
This way, solution~\(\{5,3,10,3\}\) cannot be reached.
However, this is not a problem because this solution is equivalent to~\(\{10, 5, 3, 3\}\), and this solution can be reached. 
%No solution stops being generated, but they will always be generated from the greatest item index to the lowest item index.
%\footnote{The algorithm would compute~\(\{1, 2, 3\} \cap \{1\}\) and~\(\{1, 1, 2\} \cap \{3\}\), for example.}

When the two changes are combined, and the items are sorted by non-increasing efficiency, UKP5 gains in performance. 
The UKP5 iterates by the item list only when it finds a non-dominated solution, i.e,~\(g[y] > opt\) (line~\ref{if_less_than_opt_begin}). 
Non-dominated solutions are more efficient (larger ratio of profit by weight) than the skipped dominated solutions. 
%Efficient solutions are efficient because are composed by efficient items. 
%When sorted by non-increasing efficiency, efficient items have the lowest index values. 
Therefore, the UKP5 inner loop (lines~\ref{main_inner_loop_begin} to~\ref{main_inner_loop_end}) often iterates up to a low~\(d[y]\) value. 
Experimental results show that, after some threshold capacity, the UKP5 inner loop consistently iterates only for a small fraction of the item list.
%The threshold capacity depends on the value of~\(w_{max}\).
%, sometimes only the 2\% most efficient items, for the remaining capacities.

%The third loop (lines~\ref{get_y_opt_loop_begin} to~\ref{get_y_opt_loop_end}) gets the optimal solution value, and the corresponding optimal solution weight. 
%Any optimal solution %not excluded by our solution dominance test (\ref{if_less_than_opt_begin} to~\ref{if_less_than_opt_end}), 
%is guaranteed to have weight between~\(c - w_{min} + 1\) and~\(c\) (both inclusive). 
%The proof is simple. A valid solution can't weight more than~\(c\), and for any solution~\(s\) that weights less than~\(c - w_{min} + 1\), we can obtain a solution with a bigger profit inserting a copy of~\(i\) to~\(s\), where~\(w_i = w_{min}\). When the algorithm ends, the~\(opt\) variable holds the optimal solution value, and~\(y_{opt}\) holds the the corresponding weight. 

The algorithm ends with the optimal solution stored at~\(opt\).
The solution's assembly phase isn't described in Algorithm~\ref{alg:ukp5}, but it's similar to the one used by the DP method described in~\cite[p. 221, Steps 6-8]{garfinkel}, and can be used for the already described algorithms~\ref{alg:naive_dp} and~\ref{alg:gar_dp}.
Let~\(y_{opt}\) be a capacity where~\(g[y_{opt}] = opt\).
We add a copy of item~\(i = d[y_{opt}]\) to the solution, then we add a copy of item~\(j = d[y_{opt} - w_i]\), and so on, until~\(d[0]\) is reached. 
This phase has a~\(O(c)\) time complexity, as the solution can be composed of~\(c\) copies of an item~\(i\) with~\(w_i = 1\).

\subsubsection{A note about UKP5 performance}

In the computational results section we will show that UKP5 outperforms PYAsUKP by a considerable amount of time.
We grant the majority of the algorithm performance to the ability of applying sparsity, solution dominance and symmetry pruning with almost no overhead.
At each iteration of capacity~\(y\) sparsity and solution dominance are integrated in a single constant time test~(line~\ref{if_less_than_opt_begin}).
This test, when combined with an item list sorted by non-increasing efficiency, also helps to avoid propagating big index values for the next positions of~\(d\), benefiting the performance of the solution generation with symmetry pruning (the use of~\(d[y]\) on line~\ref{main_inner_loop_begin}).

\subsubsection{Solution dominance}
\label{sec:ukp5_sol_dom_expl}

In this section we will give a more detailed explanation of the workings of the previously cited solution dominance.
We use the~\(min_{ix}(s)\) notation to refers to the lowest index between the items that compose the solution~\(s\).
The~\(max_{ix}(s)\) notation has analogue meaning.

When a solution~\(t\) is pruned because~\(s\) dominates~\(t\) (lines~\ref{if_less_than_opt_begin} to~\ref{if_less_than_opt_end}), some solutions~\(u\), where~\(t \subset u\), are not generated. 
If~\(s\) dominates~\(t\), and~\(t \subset u\), and~\(max_{ix}(u - t) \leq min_{ix}(t)\), then~\(u\) is not generated by UKP5. 
For example, if~\(\{3, 2\}\) is dominated, then~\(\{3, 2, 2\}\) and~\(\{3, 2, 1\}\) will never be generated by UKP5, but~\(\{3,2,3\}\) or~\(\{3,2,5\}\) could yet be generated (note that, in truth, the equivalent~\(\{3,3,2\}\) and~\(\{5,3,2\}\) that will be generated).
Ideally, any~\(u\) where~\(t \subset u\) should not be generated as it will be dominated by a solution~\(u'\) where~\(s \subset u'\) anyway. 
It's interesting to note that this happens eventually, as any~\(t \cap \{i\}\) where~\(i > min_{ix}(t)\) will be dominated by~\(s \cap \{i\}\) (or by a solution that dominates~\(s \cap \{i\}\)), and at some point no solution that is a superset of~\(t\) is generated anymore.

%(yet increasing capacity for the same item set will not make considerable difference for an optimizaed algorithm, see solution dominance)

%We would like to point again that a dominated solution and its supersets can always be excluded from the problem without affecting the optimal solution value. 
%A dominated solution always have a dominant solution, and the dominant solution have the same or more profit value, and the same or less weight. This way, the dominant solution can always be used in place of the dominated one without loss to the optimal solution value. 
%\footnote{Note that, on ukp5, the lowest index of an item in a solution is also the index of the last item inserted in the solution (and consequently the value stored at~\(d[y]\) where~\(y\) is the solution weight).}

\subsubsection{Implementation details}
\label{sec:ukp5_periodicity}

With the purpose of making the initial explanation simpler, we have omitted some steps that are relevant to the algorithm performance, but not essential for assessing its correctness. 
A complete overview of the omitted steps is presented at this section.

% A QUESTAO DO NAO USO DAS DOMINANCIAS JA E DITA ANTES, REMOVER AQUI?
All the items are sorted by non-increasing efficiency and, between items with the same efficiency, by increasing weight. 
This speed ups the algorithm, but does not affect its correctness.
%The simple/multiple, collective or threshold dominances aren't used by UKP5, as this is often counterproductive for hard instances, where the undominated-to-all-items ratio is close to one, and superseded by our implicit solution dominance. 

The~\(y^{*}\) periodicity bound is computed as in~\cite[p. 223]{garfinkel}, and used to reduce the~\(c\) value. 
We further proposed an UKP5-specific periodicity check that was successfully applied. 
This periodicity check isn't used to reduce the~\(c\) capacity before starting UKP5, as~\(y^{*}\). 
The periodicity check is a stopping condition inside UKP5 main loop (\ref{main_ext_loop_begin} and~\ref{main_ext_loop_end}). 
Let~\(y\) be the value of the variable~\(y\) at line~\ref{main_ext_loop_begin}, and let~\(y'\) be the biggest capacity where~\(g[y'] \neq 0 \land d[y'] > 1\). 
If at some moment~\(y > y'\) then we can stop the computation and fill the remaining capacity with copies of the first item (that has index 1).
This periodicity check works only if the first item is the best item. 
If this assumption is false, then the described condition will never happen, and the algorithm will iterate until~\(y = c - w_{min}\) as usual.
The algorithm correctness isn't affected.

There's an \emph{else if} test at line~\ref{if_new_lower_bound_end}. 
If~\(g[y + w_i] = g[y] + p_i \land i < d[y + w_i]\) then~\(d[y] \gets i\). 
This may seem unnecessary, as appears to be an optimization of a rare case, where two solutions comprised from different item multisets have the same weight and profit. 
Nonetheless, without this test, the UKP5 was about 1800 (one thousand and eight hundreds) times slower on some subset-sum instance datasets.
%presented at Table~\ref{tab:times}.
%For other instance classes, no significant speedup was observed.

%We iterate only until~\(c-w_{min}\) (instead of~\(c\), in line~\ref{main_ext_loop_begin}), as it is the last~\(y\) value that can affect~\(g[c]\)). After this we search for a value greater than~\(opt\) in the range~\(g[c-w_{min}+1]\) to~\(g[c]\) and update~\(opt\).

% UKP5 algorithm explanation
%We used only a UKP5-specific periodicity bound described later and the~\(y^{*}\) bound described in~\cite[p. 223]{garfinkel}.
%The~\(y^*\) is~\(O(1)\) on an item list ordered by non-increasing efficiency,  and it is generic, being successfuly applied on instances of most classes.
%Assuming~\(i\) is the best item, and~\(j\) is the second most efficient item, then \mbox{\(y^{*} = p_i / (e_i - e_j)\)}.

\subsection{EDUK}
\label{sec:eduk}

The EDUK (Efficient Dynamic programming for the Unbounded Knapsack problem) is a complex DP algorithm for the UKP, first mentioned in~\cite{ukp_new_results}.
However, only in~\cite{eduk} the algorithm's essentials are described for the first time.
The author of this thesis, however, is partial to the algorithm's description to be found in~\cite[p.~223]{book_ukp_2004}.
Some basic ideas used by EDUK were already exposed by a simple and functional-oriented algorithm proposed in~\cite{algo_tech_cut}\footnote{The author of this thesis tried to implement this simple and functional-oriented algorithm in Haskell and in C++.
Both codes had a very poor performance and weren't even considered for the experiments.
The author of this thesis admits that the reason of the poor performance could be the poor quality of his implementations.
The C++ code can be accessed in \url{https://github.com/henriquebecker91/masters/blob/663324e4f071b5bca22ab5301e29273b9db88a41/codes/cpp/lib/eduk.hpp}, and the Haskell code can be accessed in \url{https://github.com/henriquebecker91/masters/blob/f5bbabf47d6852816615315c8839d3f74013af5f/codes/hs/ukp.hs}.}.
Before EDUK2 was proposed, EDUK was considered by some the state-of-the-art DP algorithm for the UKP.
An example is the comment in~\cite[p.~]{book_ukp_2004}: ``[...] EDUK [...] seems to be the most efficient dynamic programming based method available at the moment.''.

A version of the original code of the EDUK and EDUK2 algorithms is available here\footnote{PYAsUKP's official site: \url{http://download.gna.org/pyasukp/pyasukpsrc.html}}.
Unfortunately, this version isn't stable and has some bugs.
Consequently, the author of this thesis recommends the use of the version available here\footnote{The repository of this master's thesis: \url{https://github.com/henriquebecker91/masters/blob/f5bbabf47d6852816615315c8839d3f74013af5f/codes/ocaml/pyasukp_mail.tgz}.}.
The author of this thesis was given access to the latter version by Vincent Poirriez who kindly answered an e-mail in January 11, 2016.

The author must admit that he doesn't have full understanding of the EDUK algorithm inner workings.
The original code is written in OCaml (a functional language), and the author had difficulties in his attempts to analyze it.
Furthermore, EDUK is a more complex algorithm than any other algorithm described in this chapter (with the obvious exception of EDUK2).
A basic overview of the EDUK algorithm essentials is given here, the author recommend the sources mentioned in the first paragraph of this section for the reader interested in a deeper analysis. 

The authors of EDUK cite threshold dominance (that generalizes collective, multiple and simple dominances), a sparse representation of the iteration domain and the periodicity property to explain the efficiency of the algorithm.
In~\cite[p.~223~to~227]{book_ukp_2004}, the reasons listed are ``the detection of various notions of dominance not only as a preprocessing step but also during the dynamic programming computation'' and ``the test for collective dominance of an item type by a previously computed entry of the dynamic programming array''.
The EDUK algorithm sorts the item list in increasing weight order, different from the majority of the algorithms for the UKP that use the non-increasing efficiency order.

The sparse representation of the iteration domain is achieved by using lazy lists (a functional programming concept) instead of an array of size~\(c\) (or more) to store the solutions.
Consequently, the memory use is less dependent of~\(c\) and~\(w_{max}\) than other DP algorithms.
In~\cite{algo_tech_cut}, where the sparse representation idea is first presented, the solutions are represented as pairs of weight and profit value, as the solution was a pseudo-item.
For example, a solution~\(s\) comprised of the items~\(i\) and~\(j\) is represented by the following pair:~\((w_i + w_j, p_i + p_j)\).
Adding an item to a solution is equivalent to adding the weight and profit values of a pair to another.
For some instances, especially the ones with big~\(w_{min}\) and~\(c\) values, this sparse representation allows for saving time and memory.
In UKP5, for example, the algorithm allocates memory, initializes, and iterates many capacities~\(y\) that are accessed and then skipped immediately because a solution~\(s\) with~\(w_s = y\) doesn't exist.
In EDUK, such skipped capacities are never explicitly enumerated to begin with, and no memory is allocated for them, or time used iterating over them. 
A similar effect could be obtained in UKP5 by using a \verb+std::map+ instead of an \verb+std::vector+ for the data structures~\(g\) and~\(d\) .

Both the item list and the knapsack capacities are broken down and evaluated in slices.
Each slice of the item list, beginning with the ones with smallest items, is then processed.
The items inside the current slice are combined with each other to generate solutions with weight up to the slice's~\(w_{max}\).
The solutions are used to test collective dominance between the items inside the slice and in the next slices.
A global list of the undominated items is kept, and after an item is dominated, it's not used again.
After evaluating all slices of the item's list, and if~\(c > w_{max}\), then EDUK begins to evaluate slices of the capacity values, using the items that weren't eliminated by the collective dominance tests.
After each one of those capacity slices, the EDUK algorithm tests the items for threshold dominance, potentially removing some of them from the global list of undominated items. 
If this list ends up consisting only of the best item (that can never be threshold dominated), then the EDUK stops and fills the remaining capacity with copies of the best item.
Otherwise, EDUK solves the UKP up until capacity~\(c\).

\section{Branch-and-Bound Algorithms}

The B\&B approach was established in the seventies as the most efficient approach for solving the UKP\footnote{To the author's knowledge this claim is first made in~\cite{mtu2}, and after this other papers as~\cite{babayev} began repeating it.
The fact that only the code for MTU1 and MTU2 was readily available also didn't help the situation, as some began to claim that MTU2 was the \emph{de facto} standard algorithm for UKP, see~\cite{ukp_new_results}.}, what is greatly a consequence of the datasets, items' distributions and/or generation parameters used at the period (that favored B\&B in the experiments).

The author does not intend to gives a complete introduction to the B\&B approach, but he believes a quick overview is in order.
The B\&B approach can be seen as an improvement of the brute-force approach.
The brute-force approach would consist of exhaustively checking all solutions in the search space.
A B\&B algorithm will keep track of the best solution found so far.
In the case of a maximization problem like UKP, this solution is called a lower bound on the optimal solution (in the sense that `the optimal solution is at least this good').
A B\&B algorithm will divide the search space in two or more exclusive subdivisions.
In the case of the UKP, an example would be `all solutions with 4 or less copies of item~\(i\)' and `all solutions with 5 or more copies of item~\(i\)'.
An optimistic guess for the best value to be found in each subdivision of the search space is computed, these are called upper bounds.
An upper bound is a value that is guaranteed to be equal to or greater than the value of the best solution to be found in the correspondent subdivision of the search space.
The subdivisions are recursively and systematically divided in smaller subdivisions.
If the upper bound of any subdivision is smaller than or equal to the global lower bound, then the solutions of that subdivision of the search space don't need to be examined (i.e. the best solution known at the moment is already equal to or better than any solution that can be found in that subdivision of the search space).
If, by the use of the lower and upper bounds, the B\&B algorithm obtains proof that no solution in all the search space can be better than the best solution found so far, the B\&B algorithm stops.

Concerning the explanation above, one thing should be clear, the quality of a B\&B algorithm is directly correlated to the quality of its bounds.
Also, the time taken to solve an instance will vary based in how much of the search space can be safely skipped/ignored by the use of the bounds.
The time taken by a B\&B approach over an instance of the UKP can be hard to predict, and is not very dependent to the magnitude of~\(n\) and~\(c\), but more dependent of the items' distribution.
In the worst case, a B\&B algorithm can't eliminate a significant portion of the search space by the use of bounds and then, consequently, it needs to examine all search space.
In the case of the UKP, the solutions' search space is clearly combinatorial (all possible items combinations that fit the knapsack), so the worst-case of an B\&B approach for the UKP can be exponential.

The memory use of a B\&B algorithm can follow its worst case and be exponential too, as for many times a tree is used to keep the enumeration of the subdivisions.
However, some optimized algorithms can avoid enumerating the tree explicitly, and keep a constant memory use linear in~\(n\) (even in the worst case).
The B\&B algorithms for the UKP often aren't affect by the magnitude of~\(c\), however they can be affect by how close~\(c\) is from a capacity that can be entirely filled by copies of the best item.
The B\&B algorithms for the UKP will often solve the problem instantly if~\(c~mod~w_b\) is small, because the greedy heuristic lower bound will probably be optimal, and will exclude the remaining search space easily.

The fact that this approach is little affected by huge values of~\(n\) and~\(c\), and more by the distribution used, makes clear why it was considered the best approach at the seventies.
The datasets used back then had large~\(n\) and~\(c\) values, and items' distributions that made easy to exclude large portions of the search space with the greedy lower bound solution (the uncorrelated distribution is the perfect example).

%REMEMBER THAT B\&B ALGORITHMS BEFORE MTU1 and MTU2 can have been excluded unfairly because of big flaws at the experiment design. THIS WILL BE SAID AT PRIOR WORK PROBABLY

\subsection{MTU1}
\label{sec:mtu1}

The MTU1 algorithm is a B\&B algorithm for the UKP that avoids the explicit unfolding of the typical B\&B tree (proposed in~\cite{mtu1}).
The implicit tree used by MTU1 is described in the sequence, as this makes the algorithm easier to visualize and understand.
The MTU1 sorts the items in non-increasing efficiency order before beginning.
Such ordering is needed to assure the correctness of the bounds and, consequently, of the algorithm itself.
In the algorithm's description, it's to be assumed that the items are ordered in the mentioned way.
%MTU1 begins by creating a lower bound solution using a greedy heuristic procedure.
%Given that the items are ordered by non-increasing efficiency, the greedy procedure simply fills the knapsack with as much copies as possible of the first item, and then repeats the procedure for the remaining gap and the second item, until there's no gap or all items were tried
%This solution is duplicated and saved as the global lower bound, and as the current solution being enumerated.

The implicit enumeration tree of MTU1 has~\(n + 1\) levels.
The root node represents all the search space, for convenience the author will consider it level zero.
The first level of the tree contains~\(\floor{\frac{c}{w_1}} + 1\) nodes.
Each of those nodes represent a subdivision of the search space where the solution has a specific number of copies of the first item (from zero to~\(\floor{\frac{c}{w_1}}\) copies).
The nodes of the second level subdivide the search space by the amount of copies of the second item in a solution, and so on (until the last item, in level~\(n\)).
From the second level on, the levels have a variable number of nodes.
There are~\(\floor{\frac{c}{w_2}} + 1\) nodes in the second level that are children of the first level node that represents the solutions with zero copies of the first item, this because as all of the knapsack capacity~\(c\) is empty.
Consequently, there's only~\(\floor{\frac{c~mod~w_1}{w_2}} + 1\) nodes in the second level that are children of the first level node that represents the solutions with~\(\floor{\frac{c}{w_1}}\) copies of the first item.

The MTU1 algorithm can be seen as the application of a modified depth-first search over the implicit tree described above.
In each level, the first node to be visited will be always the one representing the use of the greatest amount of copies of the current level item type, and the last node to be visited will be the one representing zero copies.
This visiting order, together with the items non-increasing efficiency order, combine in an intuitive behaviour: the first solutions tried will be the ones with the greatest amount of copies of the most efficient items.

As is common for B\&B algorithms, at each visited node, MTU1 will compute an upper bound for the tree below the current node and, if this upper bound value is equal to or lower than the value of the global lower bound, MTU1 will skip the subtree and backtrack to the parent node.
These upper bounds consist in a solution with the amount of items already described by the path between the root node and the current node, and a pseudo-item with weight equal to the capacity gap and efficiency equal to the efficiency of the next level item type.

When visiting a leaf node, the MTU1 will check if the solution described by the path from root to the leaf node is better than the lower bound, and update the lower bound if this is the case.

For convenience and to reduce the size of the tree, if the capacity gap left by a node is smaller than~\(w_{min}\), then that node is a leaf node (no need for a list of nodes indicating zero copies of the remaining item types).
Consequently, every node (leaf or not) represents a unique solution (denoted by the path from the root node to it).
As the nodes/solutions are visited in a systematic order, for any given node/solution, it's possible to know what part of the 'search space'/tree was already visited or skipped, and what part has not yet been explored.
Consequently, the tree does not need to be enumerated explicitly, the current node/solution is sufficient to know which solutions should be tried next.

\subsection{MTU2}
\label{sec:mtu2}

The MTU2 algorithm was first proposed in~\cite{mtu2}.
The objective of MTU2 was to improve MTU1 times when it was used in very large instances (e.g. up to 250,000 items) of the items distributions from the period.
MTU2 calls the MTU1 internally to solve the UKP, it can be seen as a wrapper around MTU1 to avoid unnecessary computational effort.
The two main factors that motivated the creation of MTU2 were: 1\textsuperscript{st}) for the majority of the instances used at the period\footnote{If the reader wants an example, one of the datasets of the paper that introduced MTU2 was analyzed in Section~\ref{sec:martello_uncorrelated}.}, an optimal solution was comprised of a very small number of the most efficient item types; 2\textsuperscript{nd}) for some of those same instances, the sorting of the whole items list was more expensive than solving the same instance with MTU1.

The explanation of the inner workings of the MTU1 (Section~\ref{sec:mtu1}) should make easy to understand how solving UKP with MTU1 can use less time than the sorting phase, for instances with the characteristic above mentioned (i.e. only the most efficient items are present in an optimal solution).
The MTU1 investigates first the regions of the search space with the biggest amount of the most efficient items. 
If instances with a large amount of items have optimal solutions among the first ones tested by MTU1, then the implicit enumeration tree will never be explored in depth, and the vast majority of the items will be ignored.
The time spent sorting any items beside the most efficient ones was a waste of time.

To address this waste of computational effort, and to solve even larger instances, MTU2 was proposed.
The MTU2 was based in the concept of `core problem' that was already introduced by other works of the period, as~\cite{core_problem}.
Informally, the core problem would be a knapsack instance sharing an optimal solution, the knapsack capacity, and a small fraction of the items list with the original instance.

The size of the core problem can't be defined a priori.
Consequently, the idea is to guess a~\(k\) value, where~\(k \leq n\), find and sort only the~\(k\) most efficient items, and then solve this tentative core problem (in the specific case of MTU2, the solver used is MTU1).
If the optimal solution value of the tentative core problem is equal to an upper bound for the full instance, then the algorithm has found an optimal solution of the full instance and can stop.
Otherwise, the algorithm uses the solution found as a lower bound to remove items outside of the tentative core problem.
For each item~\(j\) that's not in the core problem, an upper bound is computed over the solutions with one single copy of item~\(j\), if this upper bound is equal to or smaller than the value of the solution found, then the item can be discarded without loss to the optimal solution value.
If all items not in the tentative core problem are discarded by this procedure, the algorithm also stops.
If this isn't the case, more items from outside the core problem are added to it, and the process restart with a larger core problem, and the reduced item list outside of it.

\subsection{Other B\&B algorithms}

Some B\&B algorithms were never implemented or deeply studied by the author of this thesis, but are cited here for completeness.
In~\cite{cabot}, a B\&B algorithm for UKP is presented.
In that period, the B\&B algorithms were often referred as `enumeration algorithms'.
As already said in Section~\ref{sec:prior_work}, Cabot's algorithm was indirectly compared with MTU1 in~\cite{mtu1}, and MTU1 has shown better times.
However, it would be interesting to see a comparison with more extensive and recent datasets.

Another B\&B algorithm is proposed in~\cite{gg-63}.
The author isn't aware of any work where the proposed algorithm was compared to any other algorithms.
Three years after proposing this algorithm, the same authors wrote~\cite{gg-66}, that focused on the one-dimensional and two-dimensional knapsack problems.
This last paper has presented four variations of a DP algorithm for the UKP, but doesn't appear to have mentioned the old B\&B algorithm.

\section{Hybrid DP and B\&B}

As would be expected, some algorithms tried to combine the best of two most popular approaches (DP and B\&B) for better results.

\subsection{An ordered step-off improvement by Greenberg and Feldman}

The algorithm presented in~\cite{green_improv} is an improvement of the ordered step-off from~\cite{gg-66}.
Consequently, it's very similar to UKP5.
The author doesn't know if it could be defined as a hybrid, but a good definition for it would be a `DP algorithm with bounds'.
The algorithm wasn't named at the paper, and will be called GREENDP for the rest of the paper.
The implementation of the GREENDP made by the author of this thesis, and used in the experiments section, will be called MGREENDP (Modernized GREENDP, in the sense that the algorithm now use loops instead of the \verb+goto+ directive). 

The GREENDP algorithm consists in solving the UKP item using the ordered step-off algorithm, but without using the best item in the DP, and with interruptions at each~\(w_b\) capacity positions, for checking if the DP can be stopped and the remaining capacity filled with copies of the reserved best item.
In those interruptions, two bounds are computed.
A lower bound for solutions using the best item is computed by combining the current best solution of the DP with as many copies of the best item as possible.
An upper bound for solutions not using the best item is computed by combining the current best solution of the DP with a pseudo-item that would fill the entire capacity gap and has the same efficiency as the second best item (it could also be seen as solving a continuous relaxation of the UKP without the best item, and only for the remaining capacity).
If the algorithm discovers that the lower bound with the best item is better than the upper bound without the best item, then the lower bound solution is optimal and the DP can be stopped.

This approach using bounds is fundamentally different from the periodicity check used by UKP5 (or the periodicity check used by the `terminating step-off').
For example, the bounds strategy makes it competitive in the BREQ instances in a way periodicity checks don't make UKP5 competitive (see experiments of the Section~\ref{sec:breq_exp}).
However this seems to have an impact in other families of instances (see experiments of the Section~\ref{sec:pya_exp}).

A weakness of the bounds calculation is that it fails automatically if the two most efficient items have the same efficiency.
In this case, the algorithm would be the same as running UKP5 with a little overhead.

\subsection{EDUK2}

The EDUK2 algorithm was proposed in~\cite{pya}, and it is an hybridization of EDUK (a DP algorithm) with MTU2 (a B\&B algorithm).
The author gives here a quick overview of the hybridization, more details can be found in the paper above mentioned.
As with EDUK, the author doesn't claim to fully comprehend the EDUK2 internals, and it's only summarizing what's said in the original paper.
The author recommends the reading of Sections~\ref{sec:eduk} (EDUK) and~\ref{sec:mtu2} (MTU2) before of the explanation below, as it's strongly based in both algorithms.
The comments made about EDUK's code in its own section also apply to EDUK2.

The description of the changes in EDUK caused by the hybridization follows.
The~\(k = min(n, max(100, \frac{n}{100}))\) most efficient items are gathered in a tentative core problem.
A B\&B algorithm ``similar to the one in MTU1'' tries to solve the tentative core problem.
This B\&B algorithm has the possibility of choosing among three bound formulas, and stops after exploring~\(B = 10,000\) nodes (of the implicit enumeration tree).
If the B\&B algorithm returns a solution with value equal to an upper bound for the whole instance, then the DP algorithm is never run.
Otherwise, the solution given by the B\&B algorithm is used as a global lower bound in the hybridized EDUK algorithm.
The hybridized EDUK algorithm works like EDUK would, with the addition of an extra phase between slices
The extra phase uses the lower bound to eliminate items \emph{and solutions of lesser capacities} (i.e. the critical points) from the algorithm.
This phase is very similar to a phase of MTU2: an upper bound is computed for solutions using one copy of the respective item (a solution~\(s\) can be treated as a pseudo-item (\(w_s, p_s\))), if this upper bound is equal to or smaller than the global lower bound, then the item (or solution) is abandoned by the algorithm.
A new lower bound is computed for each solution that wasn't removed by the process described above.
The lower bound consists in filling the remaining capacity with a greedy algorithm.
If this new lower bound is better than the global lower bound, it replaces it. 

%SKIP DIDI
%Basically eduk, but with a B\&B pre-phase that can help with bounds computation

\section{Consistency approach}

The Consistency Approach (CA) consists in combining both the objective function (i.e.~\(maximize~p_i x_i\)) and the UKP only constraint (i.e.~\(w_i x_i \leq c\)) in a single diophantine equation\footnote{``A Diophantine equation is an equation in which only integer solutions are allowed.''\cite{diophantine}.
In other words, an equation where the values of the variables/unknowns are restricted to the integer numbers.} (\(\alpha_i x_i = \beta\), where~\(\alpha_i\) are coefficients computed for each item, and~\(\beta\) is the analogue of an optimal solution upper bound).
The combination procedure preserves the set of valid solutions, consequently, an optimal solution for the UKP can be sought by testing values for the equation variables/unknowns until the equation holds true.
Such variables are often, in one side, the quantity of each item in a solution (\(x_i\)), and, on the other side, a tentative value derived from the optimal solution upper bound (\(\beta\)).

\subsection{GREENDP}

The only algorithm implemented by the author with this approach was the first algorithm described in~\cite{on_equivalent_greenberg}.
The algorithm, that wasn't named at the original paper, will be called GREENDP1 for the rest of the article (because it's the first of the two algorithms from the same paper, and because GREENDP is an older algorithm from the same author).
The implementation of GREENDP1 by the author of this thesis, and used in the experiments section, will be called MGREENDP1 (Modernized GREENDP1, in the sense that the algorithm now use loops instead of the \verb+goto+ directive)).
Unfortunately, the algorithm was meant as an theoretical experiment, and did not have performance as a priority.

The basic idea of the GREENDP1 algorithm consists in encoding both profit and weight in a single integer value.
Let's call the value given by this encoding for a weight and profit pair, the \emph{coefficient} of such pair.
If the coefficient of two items are summed, the result is a coefficient that encodes the weight and profit value of the solution comprised by the two items. 
The algorithm then enumerates all valid solutions by adding the items' coefficients to each other, and to the coefficients of the solutions it creates\footnote{It's important to note here, for future researchers in the subject, that in~\cite{on_equivalent_greenberg}, the algorithms description is incomplete.
The author believes that on step 2d of the first algorithm, the assignment `D(z) = k' should be executed together with the other assignments; and on step 2d of the second algorithm, the assignment `D(x) = k' should also be executed together with the other assignments.
If this isn't done, the algorithm can't backtrack the items that comprise the optimal solution after finding the optimal solution value.}.
The enumeration process is very similar to a basic DP algorithm with solution symmetry pruning, and dispensable extra arrays. 
After the process of enumerating all valid solution, the algorithm has to find the optimal solution.
The algorithm will start with an upper bound on the optimal solution value, and will check if some solution has this profit value (in~\(O(1)\)), if not, it will decrease the bound in one unity, and repeat the process.
A DP algorithm with solution symmetry pruning would probably outperform MGREENDP1 in most instances.

\subsection{Babayev's Algorithm}

In~\cite{babayev}, a CA algorithm for UKP designed with performance in mind is proposed.
Unfortunately, the author did not obtain access to the code, and the algorithm demanded considerable time and effort to implement\footnote{The author started to implement the algorithm, but because of time restraints had to abandon the implementation.
The current state of the implementation can be found at \url{https://github.com/henriquebecker91/masters/blob/2623472fad711bac10bf4d34c437b24b3fd7f30f/codes/cpp/lib/babayev.hpp}.}.
The algorithm is similar to GREENDP1, but presents some improvements as: the encoding tries to create the smallest coefficients as possible; the enumeration of the solutions (or `consistency check') can be done using the Dijkstra's algorithm (shortest path,~\(O(n + \alpha^2)\)), or a method for solving group problems developed in~\cite{glover1969integer} (\(O(n \times \alpha)\)).
As the algorithm can choose between the method with the best bound for a given instance, its overall complexity is~\(O(min(n + \alpha^2, n \times \alpha))\), where~\(\alpha\) is the value of the smallest coefficient for an item of the instance.

% discussed in garfinkel
% The first algorithm is exact, uses only integer arithmetic, and always find the solution. The ideia of the algorithm is to aggregate the capacity constraint and the objective (sum of the profits) on one equation. Instead of working over weights and profits, we work over numbers that aggregate both. The items are remade this way: the weight is multiplied by the upper bound + 1 and summed to the profit. The generated solutions are simple the sum of those item-numbers. As the sum of the profits will never become so big as the upper bound + 1 you can get the weight with 'solution-number / upper bound + 1' (the integer division) and the profit with 'solution-number % upper bound + 1' (the '%' is the modulo operator). A fully functional and commented version of the algorithm is available at https://github.com/henriquebecker91/masters/blob/e2beb54b579a84d291e0a47a6a993becd02d2c3a/codes/cpp/greendp.hpp (search for mgreendp1)
%IT'S VERY IMPORTANT TO NOTE THAT GREENBERG FORGOT TO INCLUDE ONE LINE ON BOTH ALGORITHMS: on step 2d of the algorithm 1 the assignment "D(z) = k" should be executed together with the other assignments, and on step 2d of algorithm 2 the assignment "D(x) = k" should be executed together with the other assignments. If this isn't done, the algorithm can't backtrack the solution after finding the optimal solution value.

% Hirschberg and Wong (1976) and Kannan (1980): in the end of the paper reading priority queue, as their solution is for an very special case (only two items).
% Garfinkel and Nemhauser (1972): Implemented, UKP5 was based on it. The original is many orders of magnitude slower than UKP5.
% Hu (1969): Only presents the most basic/naive algorithm. It's a textbook, and it's not worried over efficiency.

\section{Other approaches}

The UKP is a very simple optimization problem.
Probably, many approaches could be applied to it, and many will, even if with only theoretical interest.
The list presented in this chapter has no intent of being extensive, but only to give the readers a base to understand most of this work's discussion.

Examples of approaches that weren't covered in this chapter are the \emph{shortest path formulations} and \emph{group knapsack formulations}.
Such approaches are discussed in~\cite[p.~239]{garfinkel}, and~\cite{book_ukp_2004}, and internally used by the algorithm described in~\cite{babayev}, mentioned last section. % TODO XXX CHECK BOOK INFO
The author finds both approaches to be similar to DP, in the sense that they are more like ways to understand (or interpret) the problem and then solve it by a correspondent DP algorithm, and less like B\&B, that's a completely different approach when compared to DP.

