<<setup,results=hid,echo=false>>=
library(ggplot2)
@

% THE MARTELLO COMPARISON PROBLEM
% Martello and Toth (1977b): aka "Branch and bound algorithms for the solution of the general unidimensional knapsack problem". They assert that the method presented on the article isn't efficient, so the paper was moved to the end of the paper reading priority queue. But at the same time, is interesting to see if they did compare with something (seems like the article that introduced MTU). As don't seem to exist a PDF of it on the internet, I emailed Martello himself asking for a copy of the paper. UPDATE: In 'Algorithms for knapsack problems', p. 251, it's said by Martello: "Enumeration methods have been proposed by Gilmore and Gomory [26], Cabot [81] and Martello and Toth [51]. This last method experimentally turned out to be the fastest (see Martello and Toth [49, 51]).", where [51] is the same as "Martello and Toth (1977b)". Also, "Enumeration methods" seems to be a jargon in 1960~1990 for branch and bound methods.

\chapter{The instance classes}

%final
If the author of this work could suggest one single thing to be improved in the future works about UKP, it would be the selection of the instances.

%working
The UKP is a variation of the classic knapsack problem, with the difference that an unbounded number of each item is available.

The study of the UKP has fallen in one of the pitfalls described by David Johnson in his guide to 

%pending
the ukp seems like a problem that should have many applications, but <CITE PYAsUKP 'are hard to find'>
the most classical real-world use of the problem is the unidimensional cutting stock problem 
	explain the problem
	explain how branch and price can be used to solve the problem
	cite gilmore and gomory (and brevily point that UKP5 and ordered step-off are very similar, more will be said in the algorithms section)
point error in the article, the pure UKP can only be used to solve the root node of the cutting stock exact continuous relaxation
the best solver don't use UKP anymore at all, but one of the good solvers use it, and repoted that in some cases, about 70\% of the processing is done at the root node

the use of artificial instances was already strongly crticized in the past
the reasons for their use is the possibility of generating many instances, of many sizes (what allows for examinating the growth of the time with the growth of the instance)
	This is specially interesting for UKP because it's one of the easiest NP-Hard problems, so big instances or hard instances have to be used to get high execution times, small execution times are too much affected by variance
	yet, artificial instances should not be used at all, if there's no guarantee that the growth/time/results will translate for real world instances (i.e. artificial instances should follow )

	The paper are the hard knapsack problems shows that even for 0-1 knapsack instances (that are by default harder than UKP instances), the instances used were easy to solve


The major problem with an experimental analysis of the UKP solving methods over artificial instances is that the different solving approaches are affected by the items distributions (i.e. some methods are the best for some for some distributions and the worst for others). Consequently, the results aren't useful for someone that wants to tackle real world UKP instances, unless some artificial instance distribution ends up modelling those instances.

TODO: The use of testbeds is overrated?

We have already pointed (TODO: refer to timeline) that the choices of item distributions on the generation of artificial instances in the previous literature has defined what was considered the best algorithm. I have written two sections of this chapter to further illustrate this point. Those are sections are (TODO: refer to) uncorrelated instances, and the second is the BREQD instances. The former points (TODO: list things pointed in uncorrelated instances); the latter presents a new instance distribution, that's easy to solve by B\&B methods and hard to solve by DP methods (some experimental results over this classes of instances will be presented at TODO: refer).

\section{Uncorrelated Random Coeficients Instances}

% REMEMBER PROFIT AND WEIGHT DOMINANCE

In this work, the expression `uncorrelated instances' will be used to refer to a family of UKP instances where the weight and the profit of an item have no correlation. The most common way for generating those uncorrelated instances is generating a value between \(w_{min}\) and \(w_{max}\) for the weight, and a value between \(p_{min}\) and \(p_{max}\) for the profit, for each of the \(n\) items of the instance using a (pseudo-)random number generator with an uniform distribution. %Uncorrelated instances can be generated using a (pseudo-)random number generator to generate \(n\) numbers between \(w_{min}\) and \(w_{max}\), and another \(n\) numbers between \(p_{min}\) and \(p_{max}\). Sorting both arrays will give (TODO: refer to realistic instances), that is another instance type. Uncorrelated instances 

\begin{figure}[h]
\caption{An uncorrelated instance generated with \(n = 100\), \(w_{min} = 1\), \(w_{min} = 1000\), \(p_{min} = 1\), and \(p_{max} = 1000\).}
\begin{center}
<<uncorrelated,fig=true,echo=false>>=
n <- 100
wmax <- 1000
pmax <- 1000
s <- 42
set.seed(s)
w <- sample(1:wmax, n, replace=TRUE)
p <- sample(1:pmax, n, replace=TRUE)
t <- data.frame(w = w, p = p)
qplot(w, p, data = t,
  xlab = 'weight',
  ylab = 'profit')
@
\end{center}
\legend{Source: the author.}
\label{fig:uncorrelated_example}
\end{figure}

Expose why they are uninteresting: we do not know if they model any real world instance; it's a simple matter of running a good polynomial simple/multiple dominance algorithm first, or using MTU2 that was developed for this purpose. Increasing values of \(n\) increase the time taken by MTU2 or dominance removal in a polynomial fashion. The NP-hard part of the problem, that is finding an optimal solution between non-dominated items don't grow with \(n\). %(sorting or the \(O(n^2)\) dominance algorithms).

\subsection{Babayev's use of inadequate uncorrelated instances}

A dataset of uncorrelated instances was used for experiments in \cite{babayev}\footnote{Section ``Second group of experiments - Problems with Uncorrelated Random Coefficients'' of the paper.}. The set of parameters used to generate this dataset made almost all instances to be trivial. It's pointed in the paper that almost all uncorrelated instances are solved exactly by a greedy procedure use only the two most efficient items to fill the knapsack.

The reason for this behaviour is the small size of the coefficients, when compared with the instance's number of items. The weight and profit of the items are restricted to values between one and one thousand. An item with weight one has \(\frac{1}{1000}\) odds of being generated; and has \(\frac{500}{1000} = \frac{1}{2}\) odds of having \(p > 500\); so for each item generated there's \(\frac{1}{2000}\) (0.05\%) odds of generating an \emph{item that multiple-dominated all other items}.

While the maximum efficiency of an item with weight one is one thousand; the max efficiency of an item with weight two is five hundred; three is 333 and so on. An item \(i\) with \(w_i = 1\) and \(p_i = 501\) will multiple dominate an item \(j\) with \(w_i = 2\) and \(p_i = 1000\); and \(j\) is the most efficient item with weight two possible. Consequently, the item \(i\) will dominate any other item with weight greater than one.

With \(n = 5000\), the odds of an instance having item \(i\) are already of 91.79\%; with \(n = 10000\) they are of 99.32\%. The dataset was composed of 108 instances; 18 instances for each \(n\) value of 5,000; 10,000; 50,000; 100,000; 150,000, and 9 instances for the \(n\) values of 200,000 and 250,000. Therefore for the vast majority of the instances, the solution was probably comprised of \(c\) copies of the same item \(i\) with weight one, and \(p > 500\).

The values used for the knapsack capacities of the instances weren't reported in the paper. Making the exact experiment setting irreproducible.

\subsection{Babayev's use of inadequate use of correlated instances}

A dataset of uncorrelated instances was used for experiments in \cite{babayev}\footnote{Section ``First group of experiments - Problems with correlated random coefficients'' of the paper.}. The set of parameters used to generate this dataset caused the comparison between methods to not be meaningful, and the conclusions to be an atifact of the dataset idiosyncrasies.
 
The items are generated with weights between one and one thousand. This alone already reduce the number of item types to one thousand; as between the items that share the same weight, only the one item type with the greatest profit will be relevant. However, the dataset included 9 instances with \(n = 1000\) and 18 instances for each of the following \(n\) values: 5,000; 10,000; 50,000; 100,000; 150,000; 200,000 and 250,000 (totalling 135 instances). If one algorithm used a \(O(n log n)\) scan to reduce the item list to the one thousand items that are relevant, and the other algorithm didn't, this would already make more difference than the solving algorithm itself.

The profits of the items were uniformly distributed between \(\ceil{0.9 \times w}\) and \(\floor{1.1 \times w}\) (where \(w\) is the weight of the specific item). Consequently, the greatest possible efficiency is \(1.1\). An item type with weight \(10\) has \(\frac{1}{1000}\) odds of being generated; and \(\frac{1}{1000}\times\frac{1}{3} = \frac{1}{3000}\) odds of being an item with \(w = 10\) and \& \(p = 11\). An item type with those values would be an small item with the best possible efficiency \(\frac{p}{w} = 1.1\). With \(n = 1000\), the chance of an instance having this item is already 28.35\%; with \(n = 5000\) is already 81.11\%, with \(n = 10000\) is 96.43\%. Such item will multiple dominate all items that are divisible by ten, and all other items whose weight isn't perfectly divisible by ten would be less efficient than it.

The amount of different item types can be obtained by \(sum_{i \in [1..1000]}{\floor{1.1 w_i - 0.9 w_i} + 1}\) what is about \(10^6\) different item types, so any excess of \(n\) over this number will be made of repeated item types. As the types are random generated, a instance with \(10^6\) items will already have many duplicated items, and some item types missing. The datasets with \(n\) equal to 150,000, 200,000 and 250,000 are not significantly different than the ones with \(n = 10^6\) as the extra thousands of items types will be, in majority, duplicated item types. %Except by the \(n = 1000\) instances, the majority of the instances will have a solution composed of the \(w = 10\) \& \(p = 11\) item, and maybe one item filling the remaining space (if \(c\) isn't a multiple of ten).

The author's hypothesis is that the dataset idiosyncrasies exposed above have benefited Babayev's algorithm. Those idiosyncrasies aren't related to what makes the UKP a NP-hard problem. A polynomial algorithm filtering undominated items before calling the MTU2 or Babayev's algorithm would reduce all instances to one thousand items, and would have probably changed the results completely. Taking in consideration that the original implementation of MTU2 used about one tenth of the time of Babayev's method for the instances with \(n = 1000\), using a polynomial algorithm to filter relevant items would have probably changed the conclusions in favor of MTU2.
%and about half for the instances with \(n = 5000\). For each group of \(n\) sizes greater than \(n = 5000\), the MTU2 was stopped by the 600 seconds timeout between 22\% and 55\% of the times. The average times of MTU2 over the instances that it solved were, in general, bigger than the ones of Babayev's algorithm. However, the MTU2 average times were less than 20\% greater than the average times of Babayev's algorithm (the only exception is the \(n = 50000\) group that where MTU2 used about the double of Babayev's average time).
%An optimal solution for the majority of the instances with \(n > 1000\) would consinst in \(\floor{\frac{c}{w_b}}\) copies of the best item, \(w = 10\) and \(p = 11\), and an item with weight smaller than 10, and \(p = w\) (this if \(c\) wasn't perfectly divisible by 10). 
The values used for the knapsack capacities of the instances weren't reported in the paper. Making the exact experiment setting irreproducible.

\section{Bottom Right Ellipse Quadrant Instances}

The Bottom Right Ellipse Quadrant Instances (`BREQ instances', for short) is a new UKP instance item distribution proposed by the author in this work. This instance distribution was created to illustrate that different item distributions favor different solution approaches and, therefore, the choice of instances (or specifically, their item distribution) defines what is considered the \emph{best algorithm}.

Distributions that are easy to solve by the DP approach and hard to solve by the B\&B approach are common in the literature. This distribution has the opposite characteristic, it is hard to solve by DP and easy to solve by B\&B. It's important to point that this is an artificially generated distribution that don't model any real world instances (that the author have knowledge). The author discourages the use and study of such artificial instances (including this one), and created this distribution only as a tool to demonstrate that isn't hard to create a distribution that benefits a solving approach over another. It's the author's opinion that research should focus on real-world instances and/or artificially generated instances that clearly model real world instances.

The name given to this distribution is derived from the fact that, when plotted on a graph, the items show the form of a quarter of ellipse (specifically, the bottom right quadrant). All items in such distribution respect the following equation: \(p_i = p_{max} - \floor[\big]{\sqrt{p_{max}^2 - (w_i^2 \times \frac{p_{max}}{w_{max}}^2)}}\)\footnote{In this context, \(w_{max}\) and \(p_{max}\) define the quadrant top right corner, i.e. the possible maximum value for an item's weight and profit, an item with those exact values doesn't need to exist in a BREQ instance. The rounding down in the formulae can be dropped if the profit is a real number and not an integer.}.

\begin{figure}[h]
\caption{A 128-16 BREQ Standard instance with \(n = 2048\)}
\begin{center}
<<breq_inst,fig=true,echo=false>>=
library(ggplot2)

t <- read.csv('../data/128_16_std_breqd-n2048-s0.csv', sep = ';')
qplot(w, p, data = t,
  xlab = 'weight',
  ylab = 'profit')     
@
\end{center}
\legend{Source: the author.}
\label{fig:breq_example}
\end{figure}

A natural consequence of this distribution shape is that the item profit grows quadratically with the item weight. This leads to the inexistance of simple, multiple and collective dominance\footnote{In truth, if the profit is integer, the smallest items can display some of those three dominances because the profit precision loss by rounding, but this is of little relevance and can be safely ignored for most purposes. If profit is an infinite precision real, the statement has no exceptions.}. In other words, for any multiset of two or more items \(s\) and any single item \(i\) (with respective weigths \(w_s\) and \(w_t\), and respective profits \(p_t\) and \(p_i\)), if \(w_s \leq w_i\) then \(p_s < p_i\).

On the other hand, threshold dominance is very common in this instance type. With exception of the best item, any item of any instance (of any distribution, not only BREQ instance) will always be threshold dominated at some capacity. However, in many instances the knapsack capacity is smaller than those threshold values and therefore the threshold dominance isn't applied or relevant. On BREQ instances, as a consequence of the quadratic profit growth, an optimal solution will never include the item \(i\) two or more times if there's an item \(j\) such as that \(\sqrt{2} \times w_i \leq w_j \leq 2 \times w_i\). Consequently, each item have a good probability of being threshold dominated before the second use. 

The solutions of BREQ instances will often contain the maximum number of copies of the largest item (that is also the most profitable, and the most efficient) allowed by the instance capacity. Any gap left will probably be filled by the heaviest item that fits the gap, with this process repeated until no item fits the gap left (or there's no gap). The classical greedy heuristic procedure that follows those steps would probably yield an optimal solution. However, this is not always the case\footnote{A counter-example follows: consider an instance with \(n = 4\), \(c = 512\), \(w_1 = 384\), \(p_1 = 2774\), \(w_2 = 383\), \(p_2 = 2756\), \(w_3 = 129\), \(p_3 = 265\), \(w_4 = 32\) and \(p_4 = 17\); the optimal solution don't use the best item (\(w_1, p_1\)); the best solution when using the best item has a profit value of \(2842 = 2774 + 4\times17\) (weight \(512 = 384 + 4\times32\)) while the best solution when using the second most efficient item has the optimal profit value of \(3021 = 2756 + 265\) (weight \(512 = 383 + 129\)). In this case, between two solutions with the same weight, the one with the best item isn't the best one. The weight and profit values of this example follow the a BREQ distribution with \(w_{max} = 512\) and \(p_{max} = 8192\).}.

The reasons that make BREQ instances favor B\&B over DP can be undestood examinating the two approachs behaviour. The B\&B approach will begin creating a solution using some sort of greedy heuristic similar to the one described at last paragraph. This solution will be very good, if not optimal, and provide a good lower bound. With this lower bound, the search space will be greatly reduced, making the algorithm end almost instantly. In the other side, the DP approach is based in solving subproblems. Subproblems will be solved for increasing capacity values yielding optimal solutions to be reused (combined with other items). However, as seen before, small solutions and/or solutions comprised of small items will generally be less efficient than solutions comprised of the big items, and therefore subproblem solutions are likely to be discarded without affecting the final result.

Our objective is not to explore this distribution and its behaviour with different parameters, only to show that it favors the B\&B approach over the DP. We proposes a subset of the BREQ instances which the only parameters are \(n\) and the seed for the PRNG, with the other instance parameters being computed over the value of \(n\). We call this instance distribution the BREQ 128-16 Standard, it's a BREQ distribution where \(c = 128 \times n\), \(p_{min} = w_{min} = 1\), \(w_{max} = c\) and \(p_{max} = 16 \times w_{max}\). The PRNG seed is used to create \(n\) unique random weight values between \(w_{min}\) and \(w_{max}\) (the random number distribution is uniform); the profit values for each weight are computed using the first formulae presented at this section (and the \(w_{max}\) and \(p_{max}\) values for that \(n\)). 

The reasoning for the choices made when creating the BREQ 128-16 Standard follows. There was no reason to restrict the \(w_{min}\) to \(w_{max}\) interval to be smaller than \(c\) (there are \(c\) distinct valid weight values). The constant 128 used to compute the capacity \(c\) for a \(n\) value was chosen as the first power of two higher than a hundred. Consequently less than 1\% of all possible items will appear at an instance, making instances generated with different seeds significantly different. The \(p_{min}\) to \(p_{max}\) value interval was chosen to be sixteen times bigger than the \(w_{min}\) to \(w_{max}\) interval to alleviate the effects of rounding the profit value to an integer value (it would not need to be done if the profit was a floating point number with reasonable precision). The efficiency of the items in a BREQ distribution will vary between zero and \(\frac{p_{max}}{w_{max}}\), so if \(p_{max} = w_{min}\) the efficiency would vary between zero and one, giving more relevance to the rounding. At least, for the biggest \(n\) value that we were interested in testing (\(2^20\)), the highest possible value of an item profit is \(2^{31} = 2^{20}\times128\times16\) what keeps the values smaller than 32 bits.

The BREQ 128-16 Standard allows us to create a simple benchmark dataset, where we only need to worry about varying \(n\) and the seed. We propose a benchmark with a hundred instances, with all combinations of \(n = 2^{11} = 2048, 2^{12}, \dots, 2^{20} \approx 10^6\) (ten \(n\) values), and ten seed different values. We will refer to it as the BREQ 128-16 Standard Benchmark (or BREQ 128-16 SB).

\section{PYAsUKP instances}

This section will discuss the instance sets used in~\cite{pya}.
The distributions used by the instances weren't novel, but a 
Those instance sets were artificially generated with the purpose of being ``hard to solve'', what means a different thing for each one of them.

The same tool was used to generate the datasets (PYAsUKP), and the same parameters were used, otherwise noted the contrary. 
In Subsection 5.1.1 \emph{Known ``hard'' instances} of~\cite{pya} some sets of easy instances are used to allow comparison with MTU2. 
However, the authors reported integer overflow problems with MTU2 on harder instances. 
With exception of the subset-sum dataset, all datasets have a similar harder set (Subsection 5.2.1 \emph{New hard UKP instances}~\cite{pya}).
Thus, we considered in the runs only the harder ones. 
Each instance has a random capacity value within intervals shown in Table~\ref{tab:times}. 
The PYAsUKP parameters \mbox{\emph{-wmin \(w_{min}\) -cap c -n \textbf{n}}} were used in all instances generation. 
%When we found a discrepancy between the formula presented in \cite{pya} and the PYAsUKP code, or generated instances, we opted for changing the formula based on the observed behavior. 
%As our knowledge of OCaml is limited, we cannot guarantee that the formula presented here is a perfect match for the code; but, based by the generated instances, we believe it to be correct to a good extent.
We found some small discrepancies between the formulas presented in~\cite{pya} and the ones used in PYAsUKP code.
We opted for using the ones from  PYAsUKP code, and they are presented below.

\section{Chung}
% TEX COPIED FROM SEA 2016 article, CHANGE AND AMPLIFY

\subsubsection{Subset-Sum}\label{sec:subsetsum}
Instances generated with \(p_i = w_i = rand(w_{min}, w_{max})\). 
The majority of the subset-sum instances used in \cite{pya} were solved on less than a centisecond in our experiments. 
This makes it easy to have imprecise measuring. 
Because of this, in this paper, we use a similar dataset, but with each parameter multiplied by ten. 
Therefore, we generated 10 instances for each possible combination of: \(w_{min} \in \{10^3, 5\times10^3, 10^4, 5\times10^4, 10^5\}\); \(w_{max} \in \{5\times10^5, 10^6\}\) and \(n \in \{10^3, 2\times10^3, 5\times10^3, 10^4\}\), totaling 400 instances. We do not discriminate each combination in~Table \ref{tab:times} for brevity. The PYAsUKP \emph{-form ss -wmax \(w_{max}\)} parameters were used.

\subsubsection{Strong Correlation}
Instances generated using the following formula: \(w_i = w_{min} + i - 1\) and \(p_i = w_i + \alpha\), for a given \(w_{min}\) and \(\alpha\).  Note that, except by the random capacity, all instances with the same \(\alpha\), \(\mathbf{n}\), and \(w_{min}\) combination are equal. The formula doesn't rely on random numbers. The PYAsUKP \emph{-form chung -step \(\alpha\) } parameters were used.

\subsubsection{Postponed Periodicity}
This family of instances is generated by the following method: \textbf{n} distinct weights are generated with \(rand(w_{min}, w_{max})\) and then sorted by increasing order; \(p_1 = w_1 + rand(1, 500)\); and \(\forall i \in [2, n].~p_i = p_{i-1} + rand(1, 125)\). The \(w_{max}\) is computed as \(10\overline{n}\). The PYAsUKP \emph{-form nsds2 -step 500 -wmax \(w_{max}\)} parameters were used.

\subsubsection{No Collective Dominance}
This family of instances is generated by the following method: \textbf{n} distinct weights are generated with \(rand(w_{min}, w_{max})\) and then sorted by increasing order; \(p_1 = p_{min} + rand(0, 49)\); and \(\forall i \in [2, n].~p_i = \lfloor w_i \times ((p_{i-1}/w_{i-1}) + 0.01)\rfloor + rand(1, 10)\). The given values are: \(w_{min} = p_{min} = \mathbf{n}\) and \(w_{max} = 10\overline{n}\). The PYAsUKP \emph{-form hi -pmin \(p_{min}\) -wmax \(w_{max}\)} parameters were used.

\subsubsection{SAW}
This family of instances is generated by the following method: generate \textbf{n} random weights between \(w_{min}\) and \(w_{max} = 1\overline{n}\) with the following property: \(\forall i \in [2, n].~w_i~mod~w_1 > 0\) (\(w_1\) is the smallest weight); sort by increasing order; then \(p_1 = w_1 + \alpha\) where \(\alpha = rand(1,5)\), and \(\forall i \in [2, n].~p_i = rand(l_i, u_i)\) where \(l_i = max(p_{i-1}, q_i)\), \(u_i = q_i + m_i\), \(q_i = p_1 \times \lfloor w_i / w_1 \rfloor \), and \(m_i = w_i~mod~w_1\). The PYAsUKP \emph{-form saw -step \(\alpha\) -wmax \(w_{max}\)} parameters were used.

\section{CSP Subproblem Knapsack Instances}

Cite original 
cite pyasukp "UKP real world instances are hard to find"
Explain the real world use for solving the column generation of simplex of continuous relaxation of the bin-packing
Point that this is useful because of the IRUP/MIRUP property
Point that this can be no used by recent solvers, but can also be, and is a good approximation
The second paragraph of the SEA2015 article is a filthy lie (it says belov's algorithm needs to solve UKP as a subproblem, what I only found after to not be true). Point that on the thesis.

