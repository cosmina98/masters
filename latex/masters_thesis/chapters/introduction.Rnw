\chapter{Introduction}

the first thing after the abstract that they will read

\section{Motivation}

our first motivation was finding the best method for solving the cutting stock subproblem
	the works of the area focused the CSP itself, the study of the subproblem was one in its own, that no author have taken time to do
we found that a large amount of previous work was ignored because of lack of implementations
we found that a simple DP algorithm beaten the state-of-art-algorithm
we found that the algorithm was very similar to one of the first algorithms created for this problem
the motivation shifted to making a literature review
this work answers the first question and makes a critical analysis of the UKP literature

\section{Scope}

Exact methods, we know that approximations exist, but the study of how much is possible to approximate without making the CSP solution inexact

\section{Formulation and Notation}

PUT MODEL HERE

Say solution is a multiset, will be used in the dominance section below
Say that we consider items with the same weight and profit the same item

\section{Well-known properties of UKP}

In this section, we will see well-known properties of the UKP that can be exploited to speed-up computation. The main point of those properties is that they are only valid if we have available as many copies of each item type as we could need. Consequently, those properties are always valid for UKP and generally not valid for other knapsack variants. However, it's interesting to note that, if a BKP or 0-1 KP has as many or more copies of some item type than it can fit in the knapsack, then we have an unbounded quantity of that item type and the properties described below apply to these item types. If every item \(i\) of an BKP or 0-1 KP have at least \(\floor{\frac{c}{w_i}}\) copies available, then it can be solved as it was an UKP.

\subsection{Dominance}

If one item \(i\) has the same or less weight than another item \(j\), and \(i\) also has the same or more profit value than item \(j\), then it's clear that if we replace \(j\) by \(i\) in any valid solution, the solution will remain valid (the weight of the solution can only remain the same or decrease), and the profit value of the solution can only remain the same or increases. This relationship between \(i\) and \(j\) is called a dominance relation; more specifically, we can say that \(i\) dominates \(j\). It's important to note that, as UKP don't limit the quantity of each item type in a solution, we can always do these replacements (what's not true for other KP variants).

Let's suppose that \(i\) dominates \(j\); we have two possibilities: the profit value of \(i\) is greater than the profit value of \(j\), or they are the same. If it's the former, then \(j\) can't be part of an optimal solution. By definition, an optimal solution is a valid solution with the greatest profit value; if \(j\) was part of an optimal solution, replacing \(j\) by \(i\) would give an also valid solution with an even greater profit value (\emph{reductio ad absurdum}). If it's the latter (i.e. \(i\) and \(j\) have the same profit value), \(j\) can be part of an optimal solution. However, we would get an optimal solution with the same or less weight, by replacing \(j\) by \(i\) in the optimal solution.

If we are interested in obtaining one optimal solution, but not all optimal solutions, we can use dominance relations to reduce the computational effort needed to find a solution. This is done by detecting dominance relations, and removing the dominated items, thus reducing the size of the problem (e.g. numbers of items). The detection can be done in a preprocessing phase by an algorithm specific for dominance detection, or within the solving algorithm (reusing computation that it would need to done anyway). Both approachs have upsides and downsides, some of them will be discussed further in this work.

Dominance relations become more interesting after we perceive that they can be apllied to item multisets (solutions), and not only to individual items. Again, as UKP don't limit the quantity of each item type in a solution, we can treat any solution as a new item type, with weight and profit value equal to the weight and profit value of the solution.

If a solution \(s\) dominates an item \(j\), we have the same properties that if \(s\) was an item. Whenever we would use \(j\), we can use the items that comprise \(s\) in place of \(j\). In the literature, such dominance relation is called \emph{collective dominance}. Special cases of the collective dominance are the \emph{multiple and simple} dominances. Our example could be regarded as simple dominance if \(s\) was comprised of one single copy of one item (as the first example of this section); and could be regarded as multiple dominance if \(s\) was comprised of multiple copies of the same item (without any other item types). Simple dominance can be seen as a special case of multiple dominance, and some authors use `multiple dominance' to refer to both\cite{hybrid}.

If a solution \(s\) dominates a solution \(t\), and \(t\) is made only of copies of the same item, we have what's called in the literature of \emph{threshold dominance}. If \(t\) is made of \(n\) copies of item \(j\), then we know that we can diregard solutions with \(n\) or more copies of item \(j\) (as each group of \(n\) copies of \(j\) can be replaced by \(s\) without loss). Depending on the solving algorithm structure, this can mean that at some point, the item can be excluded from a global item list (reducing the further computational effort of the algorithm). Collective dominance can be seen as a special case of threshold dominance, where \(n = 1\) (i.e. solution \(t\) is a single item solution).

The simple and multiple dominances were deeply studied, and algorithms that remove all simple and multiple dominated items in \(O(n^2)\), as heuristics with less complexity that don't guarantee removing all dominated items, were proposed (see \cite{pisinger_thesis} for a good revision about this subject). In the other hand, the collective and threshold dominance seems too computationally expensive to be done outside a DP algorithm (where the subproblems optimal solutions can be reused to detect collective and threshold dominance\cite{pya}).

The EDUK and EDUK2 algorithms integrate and make use of these four dominances. In fact, threshold dominance was discovered by the EDUK creators in \cite{eduk}, and was the EDUK algorithm main feature. However, we will see that both algorithms seem to be dominated by algorithms that precede they by about forty years (as the ordered step-off from Gilmore and Gomore\cite{gg66}, and its `improved version' made by Harold Greenber\cite{hg_impv}). Algorithms that didn't directly made use of any of those dominances.

A closer look to those algorithms reveal that they indirectly make use of all four dominances. What we want to say by this is that: the algorithms will eventually stop using dominated items to generate new solutions in the course of the algorithm execution. However, this is only one way to see it. The approach used by those algorithms is focused in solutions, not individual items. We would be closer to the truth saying that it makes use of some sort of \emph{solution dominance}. Ideally, if solution \(s\) dominates solution \(t\) (where both solutions can be comprised of any items), then an optimized algorithm would not generate any solutions that are a superset of \(t\) (as the corresponded supersets of \(s\) would dominate them anyway). Such dominance would generalize any previous dominances, as the dominated items can be seen as dominated single item solutions (or multiple item solutions in the case of threshold dominance).

Those old algorithms don't apply this ideal solution dominance, but a weaker version of it. This weaker version of solution dominance don't avoid generating every solution that is a superset from a dominated solution, but can be implemented without almost any overhead. As it's very algorithm specific, it will be further discussed at the algorithms section (i.e. section \ref{algorithms}).

\subsection{Periodicity and periodicity bounds}

%The UKP has an obvious periodic property that for every knapsack capacity that is a multiple of the best item's weight, the optimal solution for that knapsack capacity is \(\frac{c}{w_b}\) copies of the best item (it's clearly the most efficient use that can be made of such capacity). The UKP has also a not-so-obvious and more interesting periodic property that states that:
The UKP has an interesting periodic property that can be stated in the following way: for every set of item types, there exists a capacity \(y\), for which every capacity \(y'\) (\(y' > y\)) will have an optimal solution that is an optimal solution for capacity \(y' - w_b\) with one more copy of the best item added. In other words, after some capacity, we can find optimal solutions simply by adding copies of the best item to optimal solutions of lower capacities (all other items aren't relevant anymore). We will call this periodic property: \emph{periodicity}. Should be clear that, if \(y\) is smaller than \(c\), discovering \(y\) can reduce the computational effort for solving UKP (this is, if the cost of computing \(y\) isn't higher than the amount of saved effort).
%The focus of this section will be this second periodic property, that we will call simply by `periodicity'.

The periodicity is a direct consequence of the threshold dominance. However, the periodicity was discovered much before the concept of threshold dominance. For instance, it was already described in the 1972 book `Integer Programing'\cite{garfinkel}. As the concept of threshold dominance was already explained in this work, the author will use it to explain periodicity.

The threshold dominance property states that, if solution \(s\) dominates solution \(t\), and \(t\) contains only \(n\) copies of the same item type \(j\), then solutions with \(n\) or more copies of \(j\) can be be replaced by solutions using \(s\) instead (without the risk of losing all optimal solutions).
The periodicity states that after some capacity \(y\) we can obtain optimal solutions only by adding copies of the best item to the optimal solutions from capacities below \(y\) (all other items aren't relevant anymore).
The link between these two properties is that \emph{the best item threshold dominate every other item}.

First, let's explain why the best item threshold dominate every other item. For any two positive integers \(a\) and \(b\) there will always exist at least one integer number that it's divisible by both \(a\) and \(b\) (e.g. \(a \times b\)). Therefore, for each non-best item \(j\), there will exist a capacity value \(y\) that it's divisible by both \(w_b\) and \(w_j\). Given \(m_j = \frac{y}{w_b}\) and \(n_j = \frac{y}{w_j}\), by the definition of best item we have that \(m_j \times w_b \leq n_j \times w_j\) and \(n_j \times p_b \geq n_j \times p_j\). In other words, a solution comprised only of \(m_j\) copies of the best item will threshold dominate a solution comprised only of \(n_j\) copies of item \(j\).

Second, let's explain how periodicity is a direct consequence of fact that the best item threshold dominate every other item. As we have seen last paragraph, for each non-best item \(j\), there will exist a positive integer \(n_j\) (in a way that solutions with \(n_j\) copies of \(j\) can be replaced by \(m_j\) copies of \(b\)). Let's say that exists a solution \(u\) comprised of \(n_j - 1\) copies of each item \(j\). If another copy of any item \(j\) is added to \(u\), the resulting solution \(u' \equiv u \cup \{i\}\) could be replaced by another solution \(v\) that contained no copies of \(j\) and \(m_j\) more copies of \(b\). Any solution that weights more than solution \(u\) has only two possibilities: it uses more copies of the best item; or it uses more than \(n_j\) copies of item \(j\), in which case, copies of \(j\) can be replaced by copies of \(b\) until the quantity of copies of \(j\) is smaller than \(n_j\). Consequently, after the knapsack capacity \(y \equuiv w_u\), every non-best item \(j\) is irrelevant.


MAKE LINK WITH KELLERER BOUND, IN OTHER WORDS, THE KELLERER BOUND IS THE SAME THING AS ABOVE



A periodicity bound \(y\) is an upper capacity bound for the existence of optimal solutions without the best item. 
A periodicity bound \(y\) is an upper capacity bound for the existence of optimal solutions without the best item. 
In another words, it's a guarantee that any optimal solution for an instance where \(c \geq y\) has at least one copy of the best item. 
The periodicity bound is specially useful because it can be applied repeatedly. 
For example, let \(c = 1000\), \(y = 800\) and \(w_b = 25\) where \(b\) is the best item; because of \(c \geq y\) we know that any optimal solution has a copy of \(b\), so we can add one \(b\) to the solution and combine with an optimal solution for \(c = 975\); but \(975\) is yet bigger than \(800\), so we can repeat the process until \(c = 775\). 
This way, for any UKP instance where \(c \leq y\) we can reduce the instance capacity by \(max(1, \lceil(c-y^{*})/w_b\rceil)\times w_b\). After solving this instance with reduced capacity we can add \(max(1, \lceil(c-y^{*})/w_b\rceil)\) copies of \(b\) to the optimal solution to obtain an optimal solution for the original instance.

There exist many proposed periodicity bounds, but some are time consuming (as \(O(n^2)\)\cite{badbound1}), others depend on specific instance characteristics (as \cite{badbound2}\cite{pya}).
We used only a UKP5-specific periodicity bound described later and the \(y^{*}\) bound described in~\cite[p. 223]{gar72}.
The \(y^*\) is \(O(1)\) on an item list ordered by non-increasing efficiency,  and it is generic, being successfuly applied on instances of most classes.
Assuming \(i\) is the best item, and \(j\) is the second most efficient item, then \mbox{\(y^{*} = p_i / (e_i - e_j)\)}.


