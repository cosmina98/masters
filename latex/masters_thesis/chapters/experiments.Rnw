<<setup,echo=false>>=
library(ggplot2)
library(statsr)
library(dplyr)
library(xtable)
library(gridExtra)
library(stringi)

csp_csv <- read.csv("../data/cutstock_knap_solvers.csv", sep = ";")
csp_csv$X <- NULL
# forgot to add sort_time to knapsack time in the chart generation codes, now
# changing it here to avoid changing many places
csp_csv <- mutate(csp_csv, hex_sum_knapsack_time = hex_sum_knapsack_time + hex_sum_sort_time)
breq_csv <- read.csv("../data/128_16_std_breqd_all.csv", sep = ";")
breq_csv $X <- NULL
fast <- read.csv("../data/pya_ukp5.csv", sep = ";")
fast$X <- NULL
slow <- read.csv("../data/mtus.csv", sep = ";")
slow$X <- NULL
mtu <- read.csv("../data/mtu_impl_desktop_uc1.csv", sep = ";")
mtu$X <- NULL
env_data <- read.csv("../data/env_influence.csv", sep = ";")
env_data$X <- NULL

compare_num_iter <- function(csv, first_m, second_m) {
  first_t <- csv %>% select(filename, algorithm, total_iter) %>%
    filter(algorithm == first_m) %>% arrange(filename)
  second_t <- csv %>% select(filename, algorithm, total_iter) %>%
    filter(algorithm == second_m) %>% arrange(filename)
  first_t$diff <- first_t$total_iter - second_t$total_iter
  first_t$norm_diff <- first_t$diff / second_t$total_iter
  first_t <- first_t %>% arrange(norm_diff)
  first_t$order <- 1:length(first_t$norm_diff)
  first_t
#  ggplot(first_t,
#         aes(x = as.numeric(filename),
#             y = norm_diff*100)) +
#    ylab(paste('Deviation of the number of iterations of',
#               first_m, "\nrelative to", second_m,
#               '(in %)')) +
#    xlab("Instance index when ordered by the value at axis y")
}

ukp_time_comp_plot <- function (data, legend) {
  data <- select(data, algorithm, filename, internal_time)
  data$algorithm <- sapply(data$algorithm, (function (f) { factor(strsplit(as.character(f), "_")[[1]][1])}))
  data$language <- data$algorithm
  data$language <- sapply(data$language, (function (f) { gsub("cpp-mtu[12]", "C++", f) }))
  data$language <- sapply(data$language, (function (f) { gsub("fmtu[12]", "Fortran", f) }))
  data$language <- factor(data$language, levels = c('C++', 'Fortran'))
  data$algorithm <- sapply(data$algorithm, (function (f) { gsub(".*1", "MTU1", f) }))
  data$algorithm <- sapply(data$algorithm, (function (f) { gsub(".*2", "MTU2", f) }))
  data$algorithm <- factor(data$algorithm, levels = c('MTU1', 'MTU2'))
  data <- data %>% group_by(filename) %>% mutate(fname_mean_time = mean(internal_time)) %>% arrange(fname_mean_time)
  data$filename <- factor(data$filename, levels = unique(data$filename))
  p <- ggplot(data, aes(x = as.numeric(filename), y = internal_time, color = language)) 
  p <- p + geom_point() + scale_y_continuous(
    trans = 'log10', limits = c(0.001, 1000),
    breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
    labels = c('0.001', '0.01', '0.1', '1', '10', '100', '1000')
  ) 
  p <- p + xlab("Instances ordered by the mean time value")
  p <- p + ylab("Time (seconds, log10 scale)")
  p <- p + theme(legend.position = legend)
}

fix_names <- function(x) {
  x <- gsub('all', 'ALL', x)
  x <- gsub('ss2', 'S.S.', x)
  x <- gsub('saw', 'SAW', x)
  x <- gsub('nsds2', 'P.P.', x)
  x <- gsub('sc', 'S.C.', x)
  x <- gsub('hi', 'W.C.D.', x)
  x
}
@

\chapter{Experiments and Analysis}
\label{sec:exp_and_res}

In this chapter, five experiments are described, and their results analyzed.
The first experiment address some doubts raised by the comments of an anonymous referee in the peer review of \cite{sea2016}. 
The doubts are related to the effects of the shared memory cache in the comparison of the algorithms' times, when more than one run is executed at the same time.
Such questions are relevant for the setup of the remaining experiments, so this is the first experiment analyzed.

The second experiment is an updated version of the comparison presented in \cite{sea2016}, that included UKP5 and EDUK2 over the dataset described in \ref{sec:pya_inst}.
The updated version includes one extra algorithm, and the data was collected executing one run at time in a computer with better settings. The dataset used is exactly the same.

The third experiment complements the second.
It presents the results of MTU1 and MTU2, with two implementations of each one of them, over ten percent of the dataset used in the second experiment.
The ten percent of the dataset is significative and already shows that both algorithms can't compete with the algorithms presented in the second experiment for the dataset used.

The fourth experiment makes use of the new dataset proposed in this work, the BREQ 128-16 Standard Benchmark.
All algorithms implemented during this work are executed over the dataset, as it's small in relation to the others.
The results confirm our hypothesis.

Finally, the fifth and last experiment examinate UKP as the pricing subproblem described in Section \ref{sec:csp_ukp_inst}.

\section{Effect of running tests in parallel}

After the publication of \cite{sea2016}, the author come to perceive that algorithm's times were affected by executing multiple runs in parallel, \emph{even if each run was being executed in a exclusive/isolated core}. We credit this effect to the fact that the L2 and L3 memory cache levels are shared between cores. Some experiments were run to discern the magnitude of this effect.

For the rest of this section, we will differ between runs that executed in parallel and runs that executed one-at-time (serially). It's important to note that we are talking about the wall-clock time of the algorithm execution (without the instance reading, or output printing) between: 1\textsuperscript{st}) a run that executes in a isolated core (with no other process in the same core), with one of the other cores running the OS, and two of the remaining cores isolated and executing one run too (i.e. parallel runs); 2\textsuperscript{nd}) a run that executes in a isolated core (with no other process in the same core), with one of the other cores running the OS, and two of the remaining cores isolated and free (i.e. serial runs).

\subsection{Setups}

The setup of this experiment is different from the remaining experiments.
The experiment is repeated in two distinct computers;
this was necessary to observe the impact of different amounts of shared cache.
Only the UKP and the PYAsUKP (EDUK2) were used;
some algorithms will be more affected other less, but if some are strongly affected this is sufficient evidence for running all algorithms serially.
Also, both algorithms were present in the original comparison \cite{sea2016}, and both algorithms are relatively fast (a practical concern). 

The experiments used the reduced PYAsUKP benchmark (see Section \ref{XXX}) and ten runs for each combination of the same algorithm, in the same computer, over the same instance, and in the same mode (parallell or serial).
For the same computer, all runs of one algorithm were executed before starting the first run of the other algorithms, what is specially important for the parallel runs, as this means they only have competed for cache with other runs of the same algorithm.
For the same computer, algorithm, and mode, the order of the instances was randomly choosen.

\subsection{Experiments}

The experiments have shown that the mean time of multiple runs of the same algorithm in the same computer vary considerably if they are executed in parallel or serially; how much they vary depends of the specific algorithm and computer.

The author do not intend to do an in-depth analysis, but only to show that the difference exists and can be significative.
The experiments described in all the other Section of this chapter were executed serially to avoid this noise.
The already mentioned referee of \cite{sea2016}, questioned if the faster memory access could have benefited UKP5 over PYAsUKP.
This experiment is also an answer to that question.
The experiment setting in \cite{sea2016} corresponds to parallel runs over the \verb+notebook+ computer.
The author believes that the point made in \cite{sea2016} still stands, as the effect found over the algorithms' times would not render a different analysis.

% WE WERE COMPARING SD OF DIFFERENT MEANS DIRECTLY, THIS WAS VERY VERY STUPID
% REMOVING THE WHOLE SECTION
%The first we can see at Figure~\ref{fig:sd_env_infl} is that the standard deviation of the parallels runs is significantly greater than the ones from the serial runs (note that the axis aren't fixed scale). The runs in parallel mode were executed in random order and each of the ten runs with the same setting, will have probably competed for cache with different runs; this explains the SD difference between parallel and serial.

%The SD of UKP5 is much smaller than the one of PYAsUKP (independent of mode or computer), but this is only a effect from the UKP5 times being smaller (SD vary with scale). The desktop setting shows considerably less SD over the serial runs (in contrast to the notebook setting); what can be explained by the fact the L1 cache (that isn't shared between cores) is bigger in the desktop than the notebook computer (bigger L2 and L3 caches also help). The desktop setting also shows considerably less SD for the parallel runs, however the instances with the biggest SD seems to be less affected by this effect than the others.

%Finally, we can see that the instance type is tied to the algorithm used and the standard deviation of their times. The author believes that the distribution of the items size (that changes how farther will be the memory positions accessed by UKP5 sequentially) and profit (that ) have a big impact on the standard deviation, as they will effect the cache misses.

\begin{figure}[h]
\caption{}
\begin{center}
<<times_env_infl,fig=true,echo=false>>=
env_file_mean_times <- env_data %>% 
  select(algorithm, computer, mode, filename, internal_time) %>% 
  group_by(algorithm, computer, mode, filename) %>% 
  summarise(mean_time = mean(internal_time))

create_facet <- function(env_file_mean_times, alg_name, com_name) {
  serial <- filter(env_file_mean_times, mode == "serial" &
                     algorithm == alg_name &
                     computer == com_name) %>% arrange(filename)
  parallel <- filter(env_file_mean_times, mode == "parallel" &
                       algorithm == alg_name &
                       computer == com_name) %>% arrange(filename)
  ratio_parallel_serial <- data.frame(filename = serial$filename,
                                      algorithm = serial$algorithm,
                                      computer = serial$computer,
                                      serial_time = serial$mean_time,
                                      parallel_time = parallel$mean_time) %>% 
    mutate(ratio = parallel_time/serial_time)
  ratio_parallel_serial
}

t1 <- create_facet(env_file_mean_times, "ukp5", "notebook")
t2 <- create_facet(env_file_mean_times, "ukp5", "desktop")
t3 <- create_facet(env_file_mean_times, "pyasukpt", "notebook")
t4 <- create_facet(env_file_mean_times, "pyasukpt", "desktop")
t <- rbind(t1, t2, t3, t4)
t$inst_class <- sapply(t$filename, (function (f) { factor(strsplit(as.character(f), "_")[[1]][1])}))
t$env <- paste(t$algorithm, t$computer, sep = '_')
t$inst_class <- fix_names(t$inst_class)
ggplot(t, aes(x = serial_time, y = ratio, color = inst_class)) +
  xlab('Mean time of the serial runs (seconds)') +
  ylab('mean parallel time / mean serial time\n(same instance, same environment)') +
  geom_point() + theme(legend.position = 'bottom') + theme(legend.title = element_blank()) + facet_wrap(~ env) +
  scale_x_continuous(trans = 'log10', breaks = c(0.1, 10, 100))
@
\end{center}
\legend{Source: the author.}
\label{fig:mean_time_env_infl}
\end{figure}

% The lack of logarithmic scale at the x axis of Figure \ref{fig:mean_time_env_infl} flatten the UKP5 data points, but gives us a clear view of how UKP5 can be benefited in an experiment that makes multiple runs in parallel and compares PYAsUKP and UKP5. The execution of runs in parallel seems to affect UKP5 far more than PYAsUKP, as it has much more points distant from \(y \approx 1\) (where serial and parallel runs have similar mean times) than PYAsUKP, and only UKP5 has a significative share of points over \(y = 2\) (where the mean times of the parallel runs are more than two times the serial mean time).

% However, all UKP5 runs use little time (clearly less than 25 seconds) where many of the PYAsUKP runs have times between 25 and 300 (or 200) seconds. Even more, the PYAsUKP runs that take more time are also the more affected by the parallel execution; the author hypothesize that they take more time because the instance is harder, and use more memory for the same reason, this way they end up using more than the L1 cache and being more strongly affected by this effect (the instance `hardness' acts as a confounding variable). The result is that more time is added to the PYAsUKP total time because of this effect than time is added to the UKP5 total time, making the comparison unfair to PYAsUKP (this is, unless we are interested on how the algorithms behave in a parallel setting).

The execution of runs in parallel seems to affect UKP5 far more than PYAsUKP, as it has much more points distant from \(y \approx 1\) (where serial and parallel runs have similar mean times) than PYAsUKP, and only UKP5 has a significative share of points over \(y = 2\) (where the mean times of the parallel runs are more than two times the serial mean time).
The author believes that the fact that UKP5 initializes two arrays with the size of the knapsack capacity (plus \(w_{max}\)), and access many diferent memory regions in sequence (at each position of those arrays it access in an arbitrary order the position up to \(w_{max}\) positions far from the current position), makes it to be more affected by the cache sharing.

However, one could point that all UKP5 runs use little time (less than 10 seconds when serially and, consequently, no more than thirty when in parallel), where many of the PYAsUKP runs have times between 10 and 100 seconds (serially), with some up to 300 seconds.
Even more, the PYAsUKP runs that take more time are also the more affected by the parallel execution.
The author hypothesize that they take more time because the instance is harder, and use more memory for the same reason, this way they end up using more than the L1 cache and being more strongly affected by this effect (the instance `hardness' acts as a confounding variable, the same effect happens to UKP5).
The result is that more time is added to the PYAsUKP total time because of this effect than time is added to the UKP5 total time (in absolute numbers), making the comparison unfair to PYAsUKP (this is, unless we are interested on how the algorithms behave in a parallel setting).

Let's examinate closely the values for the \verb+notebook+ setting, as this was the one used at the \cite{sea2016}.
We will refer to UKP5 (or PYAsUKP) Parallel (or Serial) Total time (or TT, for short) as the sum of the internal times of all the runs of that algorithm in that mode and, for now, only in the \verb+notebook+ setting.
The UKP5 Parallel TT is 2.03 times greater than UKP5 Serial TT, while PYAsUKP Parallel TT is only 1.60 times greater than the PYAsUKP Serial TT.
However, this means about 8252 extra seconds for UKP5 Parallel TT (compared to the serial time) and about 74180 extra seconds for PYAsUKP Parallel TT (compared to the serial time).
We will see that this considerable absolute difference don't end up benefiting UKP5 in the analysis, as the PYAsUKP Serial TT is about 15.33 times bigger than the UKP5 Serial TT, but the PYAsUKP Parallel TT is about 12.10 times bigger than UKP5 Parallel TT.
In fact, the parallel execution seems to benefit the PYAsUKP numbers against UKP5 while not by much.
If we focused in the desktop setting, we would see that PYAsUKP was benefited too (while by less), the PYAsUKP Parallel TT would be 15.62 times greater than the UKP5 Parallel TT; and the PYAsUKP Serial TT would be 16.50 times greater than the UKP5 Serial TT.

\begin{figure}[h]
\caption{Parallel and Serial Runs Standard Deviation}
\begin{center}
<<sd_env_infl,fig=true,echo=false>>=
serial_big_sd <- filter(env_data, mode == "serial") %>%
                 group_by(computer, algorithm, filename) %>%
                 summarise(internal_time_sd = sd(internal_time)) %>%
                 arrange(computer, algorithm, filename)
parallel_big_sd <- filter(env_data, mode == "parallel") %>%
                   group_by(computer, algorithm, filename) %>%
                   summarise(internal_time_sd = sd(internal_time)) %>%
                   arrange(computer, algorithm, filename)
com_sd <- data.frame(filename = serial_big_sd$filename,
                     serial_sd = serial_big_sd$internal_time_sd,
                     parallel_sd = parallel_big_sd$internal_time_sd,
                     algorithm = serial_big_sd$algorithm,
                     computer = serial_big_sd$computer)

com_sd$inst_class <- sapply(com_sd$filename, (function (f) { factor(strsplit(as.character(f), "_")[[1]][1])}))
com_sd$env = paste(com_sd$algorithm, com_sd$computer, sep = '_')

com_sd$inst_class <- fix_names(com_sd$inst_class)
ggplot(data = com_sd,
       aes(x = serial_sd, y = parallel_sd, color = inst_class)) +
       geom_point() + facet_wrap(~ env, scales = 'free')
@
\end{center}
\legend{Source: the author.}
\label{fig:sd_env_infl}
\end{figure}

The Figure \ref{fig:sd_env_infl} present the Standard Deviation (SD) values computed over each ten runs in the exact same conditions.
The author will not try to draw many conclusions from this data, as SD values with different means aren't directly comparable.
However, it's interesting to point that, in general, the runs in the serial mode, and/or the \verb+desktop+ setting, have a much smaller SD than the ones in parallel mode, and/or the notebook setting (either because of the serial/desktop times are smaller, or because the algorithms don't compete so much for cache).
%As the variation is very small in a serial/desktop setting, and a similar setting is used for the remaining experiments, the author found no problem in executing only one run for combination of instance and algorithm in the rest of the experiments.

The author belives this superficial analysis is sufficient to convince the reader that there's a significant difference by running the experiments serially or in parallel.
If some real-world application of UKP needs to solve multiple knapsacks in parallel, algorithms that use less memory (or with better locality of reference) can be prefered not only to avoid swapping, but because they will affect each others times less.
As this isn't the case here, and as the CSP (real world application of UKP covered in this chapter) specifically solves multiple knapsacks serially, the remaining experiments of this chapter will consist of serially executed runs, with only one run over each combination of algorithm and instance.

<<env_infl_nums,echo=false,results=hide>>=
# Used to get the values of the above paragraph. Change 'echo=false' to
# to display the values 'echo=true'.
ukp5_par <- filter(env_data, mode == 'parallel' & algorithm == 'ukp5' & computer == 'notebook')
pya_par <- filter(env_data, mode == 'parallel' & algorithm == 'pyasukpt' & computer == 'notebook')
ukp5_ser <- filter(env_data, mode == 'serial' & algorithm == 'ukp5' & computer == 'notebook')
pya_ser <- filter(env_data, mode == 'serial' & algorithm == 'pyasukpt' & computer == 'notebook')

sum(ukp5_par$internal_time) / sum(ukp5_ser$internal_time)
sum(ukp5_par$internal_time) - sum(ukp5_ser$internal_time)
sum(pya_par$internal_time) / sum(pya_ser$internal_time)
sum(pya_par$internal_time) - sum(pya_ser$internal_time)
sum(pya_ser$internal_time) / sum(ukp5_ser$internal_time)
sum(pya_par$internal_time) / sum(ukp5_par$internal_time)

ukp5_par <- filter(env_data, mode == 'parallel' & algorithm == 'ukp5' & computer == 'desktop')
pya_par <- filter(env_data, mode == 'parallel' & algorithm == 'pyasukpt' & computer == 'desktop')
ukp5_ser <- filter(env_data, mode == 'serial' & algorithm == 'ukp5' & computer == 'desktop')
pya_ser <- filter(env_data, mode == 'serial' & algorithm == 'pyasukpt' & computer == 'desktop')

sum(ukp5_par$internal_time) / sum(ukp5_ser$internal_time)
sum(ukp5_par$internal_time) - sum(ukp5_ser$internal_time)
sum(pya_par$internal_time) / sum(pya_ser$internal_time)
sum(pya_par$internal_time) - sum(pya_ser$internal_time)
sum(pya_ser$internal_time) / sum(ukp5_ser$internal_time)
sum(pya_par$internal_time) / sum(ukp5_par$internal_time)
@

\section{Setup of the remaining experiments}

The remaining experiments were run using a computer with the following characteristics: the CPU was an Intel\textsuperscript{\textregistered} Core\textsuperscript{TM} i5-4690 CPU @ 3.50GHz; there were 8GiB RAM available (DIMM DDR3 Synchronous 1600 MHz) and three levels of cache (256KiB, 1MiB and 6MiB, where the last two are shared between cores).
The operating system used was GNU/Linux 4.7.0-1-ARCH x86\_64 (i.e. Arch linux). 
Three of the four cores were isolated using the \emph{isolcpus} kernel flag. 
The \emph{taskset} utility was used to execute runs in one of the isolated cores.
% SEE ABOUT SWAP AND THE MEMORY USE SPECIALLY FOR MGREENDP1
% CHECK THE COMPILER VERSION/FLAGS for each experiment?: The UKP5 code was compiled with gcc (g++) version 5.3.0 (the \emph{-O3 -std=c++11} flags were enabled).

% Two other smaller experiments were relevant to those and are described in the next two subsections. The first shows that our implementation of MTU1 and MTU2 (written in C++) are in par with the original ones (written in Fortran); we will use our implementation in the remaining of the experiments. The second shows that there can be considerable time difference when the runs of an experiment are executed in parallel (even one per core, and with the core isolated from other processes) instead one at time. 

\section{PYAsUKP 4540 Instances Benchmark}
\label{sec:pya_exp}

In this section we present an updated version of the bechmark presented in \cite{sea2016}, which in turn was based in the benchmark presented at \cite{pya}, already described at Section \ref{pya_inst}. The comparison at \cite{sea2016} was between UKP5 and PYAsUKP, here the exact same dataset is used, and one extra algorithm is compared. This experiment is complemented by the one presented at next section, that compares two implementations of both MTU1 and MTU2 over a reduced version of the same dataset. The reduced version was used because of time restraints, executing only over 10\% of the instances was sufficient to know that the methods weren't competitive.

\begin{figure}[h]
\caption{Benchmark with fast methods (no timeout)}
\begin{center}
<<pya_fast_fig,fig=true,echo=false>>=
csv_no_na <- fast[complete.cases(fast), ]
csv_no_na$type <- sapply(csv_no_na$filename, (function (f) {
  factor(strsplit(as.character(f), "_")[[1]][1],
         levels = c('all', 'hi', 'nsds2', 'saw', 'sc', 'ss2'))
}))
csv_no_na_mean <- csv_no_na %>%
  group_by(filename) %>%
  mutate(mean_methods_time = mean(internal_time))
#csv_no_na$n <- sapply(csv_no_na$filename, (function (f) { as.numeric(gsub("n", "", strsplit(as.character(f), "-")[[1]][2]))}))
dt_copy <- csv_no_na_mean
dt_copy$type <- factor('all', levels = c('all', 'hi', 'nsds2', 'saw', 'sc', 'ss2'))
csv_no_na_order <- rbind(csv_no_na_mean, dt_copy) %>%
  arrange(mean_methods_time)
csv_no_na_order$filename <- factor(csv_no_na_order$filename,
                                   levels = unique(csv_no_na_order$filename))

csv_no_na_order$type <- fix_names(csv_no_na_order$type)
ggplot(csv_no_na_order,
       aes(x = as.numeric(filename),
           y = internal_time,
           color = algorithm)) + 
  geom_point() +
  scale_y_continuous(trans = 'log10', limits = c(0.001, 1000),
                     breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000),
                     labels = c('0.001', '0.01', '0.1', '1', '10',
                                '100', '1000')) +
#  ggtitle('Benchmark with the 4540 PYAsUKP instances') +
  ylab('Time to solve (seconds, logarithmic)') +
  xlab('Instance index when ordered by average time to solve') +
  theme(legend.position = 'bottom') + facet_wrap( ~ type, ncol = 3)
@
\end{center}
\legend{Source: the author.}
\label{fig:pya_fast}
\end{figure}

The observant reader will perceive that the subset-sum (S.S.) distribution don't show MGREENDP data points. This happens because it fails automatically over instances where the two most efficient items have the same efficiency (what always happens at subset sum instances). We could fix this problem, but this would disable MGREENDP specific periodicity check (that it's main feature) and make the algorithm even more similar to UKP5. So we opted for not running mgreendp over those instances.

Figure \ref{fig:pya_fast} show us that, except by Strong Correlated (S.C.) and S.S., the times of PYAsUKP's runs show often two main trends. The first trend cover the times of ukp5 and mgreendp (beggining at the left of the x axis), and the second trend with times significantly smaller than UKP5 and mgreendp (beggining at the middle/right of x axis), both merge in a single trend that points to the top right corner of the chart. The UKP5 and MGREENDP present similar times that form plateus. Those plateus aggregate instances that take about the same time to solve by UKP5/MGREENDP.

While the author don't examinated the PYAsUKP's internals, he belives that the second PYAsUKP trend is formed by instances where the B\&B preprocessing phase had considerable success (stopped the computation or used bounds to remove items before excuting the DP). This would also explain the lack of this trend at the S.C. and S.S. instance classes, where such bounds and tests have less effect (the S.S. instances do not have different efficiency between items and, consequently, are little affected by bounds). When PYAsUKP B\&B phase fails to yield good results, the PYAsUKP's DP phase times are greater than the UKP5 and mgreendp times.

The plateaus formed by the UKP5 and mgreendp runs show very little time variation between some groups of instances. A closer examination of the data reveals that the instance groups (plateaus) aggregate instances with the same number of items (for the same distribution) or, instances with different number of items but all of them magnitude(s) smaller (or greater) than the numbers of items of the plateau(s) above (or below). This behaviour shows that UKP5 and mgreendp1 are little affected by the specific items that make an instance, the instance size and distribution being good predictors of the time the UKP5/MGREENDP will need to solve it. 

PYAsUKP seems to have a much greater variation between the times for instances with similar number of items of the same distribuion. We can see a line with a slope of about 45 degrees close to the top right corner of the charts. This line is over a logarithmic y axis, so it's a huge variation compared to the UKP5 plateau below it that is solving the same instances (thse instances, as we have seen above, have similar number of items).

The times between UKP5 and MGREENDP are very similar, specially at datasets Without Collective Dominance (W.C.D) and Postponed Periodicity (P.P.), where it has a small advantage. This should come with no surprise as they are similar algorithms. What is surprising is its behaviour at SAW and SC instance classes, where the situation is reversed, and MGREENDP takes considerably more time than UKP5 in many cases. The behaviour at those two classes can be explained by three main factors.

The first factor is a characteristic of the optimal solutions from the saw and sc instances. The optimal solutions of these distributions are comprised mainly of (hundreds of) copies of the best item and one copy other item. This happens because, in such distributions, the smallest item is the best item (as already pointed at Sections \ref{sec:sc_inst} and \ref{sec:saw_inst}). This characteristic gives a big importance to the best item in those instances, and together with the the next two factors, explain the algorithms behavior.

The second factor is that MGREENDP solves the knapsack subproblems without the best item. The algorithm does that to allow for a mechanism that periodically checks if it can stop the computation and fill the remaining knapsack capacity with copies of the best item. Solving the DP subproblems without the best item weakens the effect of the solution dominance applied by mgreendp (also used by UKP5). Some solutions that would be never generated in UKP5 will be generated in MGREENDP, this because better solutions that used the best item don't exist to dominate those inferior solutions.

The third and last factor is the periodicity check applied by UKP5. As the mgreendp mechanism discussed above, this check stops UKP5 when it finds that the remaining capacity can simply be filled by copies of the best item. As this check don't remove the best item from the item list, it has less overhead than the mgreendp test for those instances. The UKP5 periodicity check benefited 222 of the 240 sc instances and 972 of the 1100 saw instances. % this stat is buried in the file with all ukp5 outputs, sorry about that, check for last_y_value_outer_loop in ukp5 outputs

Our conclusion about this experiment is that UKP5 and MGREENDP generally will dominate PYAsUKP at big instances of the presented distributions, however for a specific instance PYAsUKP can show times much smaller, similar or much greater than UKP5. As PYAsUKP times seems to grow more than UKP5/MGREENDP times with the instance growth, the possibily that PYAsUKP has better results than UKP5 for even bigger instances of those distributions seems improbable (while possible). About UKP5 and MGREENDP, they have similar results (as expected), where UKP5 is slight worse than mgreendp in some distributions and considerable better in others, the choice between one and another has to be made based in the distribution (specially considering that mgreendp will need workarounds to work with instances where the two most efficient items can have the same efficiency). The author would like to reminder that those distributions are artificially generated, and don't model any real-world instances (that the author has knowledge), and consequently any conclusions have limited validity.

%\begin{figure}[h]
%\caption{Benchmark with slow methods (10\% of the instances, 600s timeout)}
%\begin{center}
%<<pya_slow_fig,fig=true,echo=false>>=
%csv_no_na <- slow
%csv_no_na[is.na(slow$internal_time), ]$internal_time <- 600
%csv_no_na$type <- sapply(csv_no_na$filename, (function (f) {
%  factor(strsplit(as.character(f), "_")[[1]][1],
%         levels = c('all', 'hi', 'nsds2', 'saw', 'sc', 'ss2'))
%}))
%csv_no_na_mean <- csv_no_na %>%
%  group_by(filename) %>%
%  mutate(mean_methods_time = mean(internal_time))
%dt_copy <- csv_no_na_mean
%dt_copy$type <- factor('all', levels = c('all', 'hi', 'nsds2', 'saw', 'sc', 'ss2'))
%csv_no_na_order <- rbind(csv_no_na_mean, dt_copy) %>%
%  arrange(mean_methods_time)
%csv_no_na_order$filename <- factor(csv_no_na_order$filename,
%                                   levels = unique(csv_no_na_order$filename))
%
%ggplot(csv_no_na_order,
%       aes(x = as.numeric(filename),
%           y = internal_time,
%           color = algorithm)) + 
%  geom_point() +
%  scale_y_continuous(trans = 'log10', limits = c(0.001, 600),
%                     breaks = c(0.001, 0.01, 0.1, 1, 6, 60, 600),
%                     labels = c('0.001', '0.01', '0.1', '1', '6', '60', '600')) +
%#  ggtitle('Benchmark with 454 (10%) of the PYAsUKP instances') +
%  ylab('Time to solve (seconds, logarithmic)') +
%  xlab('Instance index when ordered by average time to solve') +
%  theme(legend.position = 'bottom') + facet_wrap( ~ type, ncol = 3)
%@
%\end{center}
%\legend{Source: the author.}
%\label{fig:pya_slow}
%\end{figure}

\subsection{Algorithms implemented but not used at this benchmark}

The PYAsUKP benchmark was proposed at paper \cite{pya}. In this paper, the authors compare MTU2 to EDUK and EDUK2 using an easier version of the PYAsUKP benchmark. The PYAsUKP benchmark was not used because MTU2 used 32 bits integers and had overflow problems when solving the instances (the easier version of the benchmark had smaller coefficients to allow MTU2 to run without problems).

As MTU2 and MTU1 are the most well-known B\&B algorithms for solving UKP, the author found interesting to test them against the PYAsUKP benchmark. The author implemented both algorithms in C++ (using templates) and adapted the original code in Fortran to work with 64 bits. The author tried to execute MTU1 and MTU2 over the PYAsUKP benchmark, but very fast become clear that both algorithms aren't competitive against the three algorithms compared in Figure \ref{fig:pya_fast}. Many runs ended by timeout (used more than 1000 seconds), and running the four versions of the algorithm over the 4540 instances would take time from more relevant experiments. The author choose to run the experiment over the reduced PYAsUKP benchmark (only one tenth of the instances). The results can be found at Section \ref{sec:mtu_exp}.

Harold Greenberg wrote a paper with two other methods for solving the UKP, \cite{on_equivalent_greenberg}, this paper was published after \cite{green_improv} (that proposed the algorithm implemented by mgreendp). The author found interesting to implement those algorithms too, as they used different approachs\footnote{Those approaches are described as ``following the corner polyhedron approach of integer programming'' and ``the approach of solving (K) as a group knapsack problem.''.}, and also because they were from the same author that already designed an efficient algorithm. However, the first algorithm proved itself very time- and memory-consuming, and the second algorithm didn't worked for all cases (could not be considered an exact method without using a backup solver to solve instances where the method failed). After some tests become clear that the intent of the paper was a theoretical exploration of new approaches, not proposing efficient algorithms, and the algorithms were dicarded from experimentation over this benchmark instances. Readers interested in those methods can check implementations of them, made available at Github\footnote{The C++ code implementing both methods can be found at \url{https://github.com/henriquebecker91/masters/blob/e2ff269998576cb69b8d6fb1de59fa5d3ce02852/codes/cpp/lib/greendp.hpp}.}.

\section{MTU1 and MTU2 implementation}
\label{sec:mtu_exp}

The specific setup of this experiment was the following: the specific commit was \verb+[master 42ecda2] "Changes on fortran MTU output format and removal of unused code."+ \footnote{Hyperlink for tree at specific commit: \url{https://github.com/henriquebecker91/masters/tree/42ecda29905c0ab56c03b7254b52bb06e67ab8d7}}.
The code was compiled using the available Makefiles, and the gcc and gcc-fortran version were the 6.1.1 (2016-06-02), the \emph{-O3} flag was used on both C++ and Fortran versions.
The Fortran version read the same instances from a simplified format (that preserved the instance's item order).
The binary \verb+ukp2sukp.out+ (codes/cpp) was used to convert one format to the other (same commit mentioned above).
%As the algorithms don't make a heavy memory use (and has good locality of reference), five isolated cores of the computer were used in parallel for running the experiment. The computer setup was: XXX.

The Fortran codes were an adaptation of the original MTU codes. The only difference from the originals was that any 32 bits integers or float variables/parameters were replaced by their 64 bits counterparts. 

The two implementations of the MTU1 algorithm have no significative differences (besides the programming language).
The only significative difference between the MTU2 implementations was the algorithm used to sort partially the items array.
The original algorithm described in \cite{mtu2} didn't specify the exact method for performing this partial sorting.
The original implementation used a complex algorithm developed by the same authors of MTU2 in \cite{partial_sort_martello} to find the k\textsuperscript{th} most efficient item in a unordered array (and then sort).
Our implementation use the \verb+std::partial_sort+ procedure of the standard C++ library \verb+algorithm+.
Our implementation also checks if the whole vector is ordered before starting to execute the partial sorts, and it sorts by non-increasing efficiency and if tied by non-decreasing weight.

\begin{figure}[h]
\caption{Comparison between MTU implementations}
\begin{center}
<<mtu_comp,fig=true,echo=false>>=
mtu$internal_time <- sapply(mtu$internal_time, function(x) if (is.na(x)) 1000 else x)

mtu1 <- filter(mtu, algorithm == "cpp-mtu1_desktop_uc1" | algorithm == "fmtu1_desktop_uc1") # only mtu1
mtu2 <- filter(mtu, algorithm == "cpp-mtu2_desktop_uc1" | algorithm == "fmtu2_desktop_uc1") # only mtu2

p1 <- ukp_time_comp_plot(mtu1, 'bottom') + ggtitle("MTU1 (Fortran vs C++)")
p2 <- ukp_time_comp_plot(mtu2, 'bottom') + ggtitle("MTU2 (Fortran vs C++)")
grid.arrange(p1, p2)
@
\end{center}
\legend{Source: the author.}
\label{fig:breq_bench}
\end{figure}

The test was made with the reduced PYAsUKP benchmark (see Section \ref{XXX}). We can see that the MTU1 implementations had a very small variation, with the C++ version being slight faster.
The MTU2 implementation had a much bigger variation. We do believe that this variation is caused by the order checking and the difference of sorting algorithms.

There's a trend between the values 180 and 230 of the x axis.
This trend is composed by the subset-sum instances.
The subset-sum instances are always naturally ordered by efficiency as their efficiency is the same for all items (the weight and profit are equal).
The instance when generated by PYAsUKP is also ordered by non-decreasing item weight, what makes it perfectly ordered for our C++ implementation.
The Fortran implementation don't seem to work well with this characteristic of the subset-sum instances.
We have chosen to use the C++ implementation in our experiments.
The reason are: the C++ implementation can read the same format used by PYAsUKP (and the other algorithms); with the exception of PYAsUKP (that uses OCaml) all other methods use C++ and the same structures; choosing the Fortran implementation would harm considerably the MTU2 times for something that seems to be a minor implementation detail.

\subsection{BREQ 128-16 Standard Benchmark Results}

We have run eight algorithms over the BREQ 128-16 Standard Benchmark (proposed at section \ref{sec:breq_inst}). The results confirm our hypothesis that this distribution would be hard for DP algorithms and easy for B\&B algorithms.

\begin{figure}[h]
\caption{Benchmark with the 128-16 Standard BREQ instances.}
\begin{center}
<<breq,fig=true,echo=false>>=
csv_no_na <- breq_csv
csv_no_na[is.na(csv_no_na$internal_time),]$internal_time <- 1000
csv_no_na$n <- sapply(csv_no_na$filename,
(function (f) {
  as.numeric(gsub("n", "", strsplit(as.character(f), "-")[[1]][2]))
}))

ggplot(csv_no_na,
       aes(x = n,#* (1 + (as.numeric(algorithm) - 1)/10),
           y = internal_time,
           color = algorithm)) + 
  geom_point() +
  scale_y_log10() +
  scale_x_continuous(trans = "log2",
                     breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16,
                                2^17, 2^18, 2^19, 2^20)) +
  ylab('Time to solve (seconds)') +
  xlab('Instance size (n value)') +
  theme(legend.position = 'bottom')
@
\end{center}
\legend{Source: the author.}
\label{fig:breq_bench}
\end{figure}

Let's examinate what Figure \ref{fig:breq_bench} tell us. The MGREENDP1 algorithm (a CA algorithm) is clearly dominated, and after the first two instance sizes, all of its runs end in timeout. So we will exclude it from the rest of the analysis.

The rest of the methods form two lines with different slopes, line with a steep slope and a line with a more gradual slope. The steep slope line show algorithms whose time grows very fast relative to the instance size growth. This group is mainly composed by the DP methods: UKP5, UKP5\_SBW (i.e. UKP5 sorted by weight), and the EDUK algorithm. The second group, that form a more gradual slope, have algorithms whose time grows much slower with the instance growth. This group is mainly composed by B\&B and hybrid methods as: MTU1, MTU2, EDUK2 and mgreendp.

Examinating only MTU1 and MTU2, we can see clearly that for small instances their times overlap, but with the instance size growth the core problem strategy of MTU2 (that tries to avoid sorting and examinating all the items) begins to pay off, making it the \emph{best algorithm} to solve BREQ instances. 

The behavior of EDUK2 shows that the default B\&B phase (executed before defaulting to EDUK) solves the BREQ instances in all cases. If it didn't, some EDUK2 points would be together with the EDUK points for the same instance size. Between the pure DP algorithms, EDUK was the one with the worst times, being clearly dominated by our two UKP5 versions. 

The UKP5 algorithm sorted the items by decreasing efficiency, and had the \(y^*\) bound and periodicity checking enabled. These two optimizations benefited none of the one hundred runs. No knapsack capacity from an instance was reduced by the use of the \(y^*\) bound; all instances had only overhead from the use of the periodicity checking. The UKP5\_SBW sorted the items by increasing weight and had these two optimizations disabled. The benchmark's instance files had the items in random order, so both algorithms used a small and similar time ordering the items\footnote{It's interesting to note that, because of the BREQ distribution and except by the profit rounding at the small items, the decreasing efficiency order is the reverse of the increasing weight order.}.

The UKP5\_SBW times had a much smaller variation than the UKP5 for the same instance size, what can be only attributed to the change in ordering (as the two previously cited optimizations had only wasted time with overhead). The decreasing efficiency ordering helped UKP5 to be faster than UKP5\_SBW in some cases, and made it slower in others, what don't give us a clear winner.

Finally, mgreendp showed an interesting behaviour. The mgreendp is a modern implementation in C++, made by the author, of an algorithm made by Harold Greenberg. The algorithm of Harold Greenberg (that wasn't named in the original the paper) was an adaptation of the \emph{ordered step-off} algorithm from Gilmore and Gomore. This algorithm periodically compute bounds (similar to the ones used by the B\&B approach) to check if it can stop the DP computation and fill any remaining capacity with copies of the best item. The author don't know if it could be called a hybrid algorithm, as the only characteristic taken from B\&B is the bound computation. The majority of the times, the bound computation allowed the algorithm to stop the computation at the beggining, having results very similar to EDUK2 (the hybrid B\&B-DP algorithm). However, six of the mgreendp executions had times in the steep slope line (the bound failed to stop the computation). Without the bound computation, mgreendp is basically the \emph{ordered step-off} from Gilmore and Gomore (whose is very similar to UKP5, as already pointed); consequently, those six outlier runs have times that would be expected from UKP5 for the respective instance size. One of the mgreendp runs in the last instance size, and one in the penultimate instance size, was ended by timeout (the bound failed to stop the computation and the DP algorithm was terminated by timeout).

While the simple, multiple and collective dominances are rare in a BREQ distributions with integer profits; the solution dominance used by UKP5 works to some extent. The UKP5 combines optimal solutions for small capacities with single items and generate solutions that, if optimal for some capacity, will be used to generate more solutions after (recursively). In a BREQ instance, solutions made of many small items rarely are optimal and, consequently, often discarded, wasting the time used to generate them. However, as a silver lining, the UKP5's solution dominance will discard those solutions as soon as possible, and will \emph{not} use them to generate any new solutions (saving some computational effort). In other words, almost all subproblem solutions are useless, and UKP5's solution dominance helps to create less of them.

%The algorithms described at \cite{TheUnboundedKnapsackProblem-T.C.HU.pdf} were not implemented and tested because of time reasons. However, we can see, by the algorithms assimptotic worst case complexity (\(O(n v_1 w_1)\) and \(O(n w_i)\)), that they 

%<<<<label=breq_table,echo=FALSE,results=tex>>=
%library(plyr)
%
%breq_csv_s <- breq_csv[complete.cases(breq_csv), ]
%breq_csv_s$n <- sapply(breq_csv_s$filename, (function (f) {
%  as.numeric(gsub("n", "", strsplit(as.character(f), "-")[[1]][2]))
%}))
%breq_csv_s <- ddply(breq_csv_s, c('algorithm', 'n'), summarise,
%  ns    = length(n),
%  avg   = mean(internal_time),
%  min   = min(internal_time),
%  max   = max(internal_time),
%  sd    = sd(internal_time)
%)
%
%xtable(breq_csv_s)
%@

\section{CSP Knapsack Subproblem Experiment}
\label{sec:csp_experiments}

% CITE GG-63 page 19, where it's said that it's relevant to always maximize the column value
% We did not divide in high demand and low demand, but this don't affect the knapsack problem itself, and we are focusing in which knapsack methods is the best for this use (p. 16)

the experiment described at this section is more complex than the others. the other experiments involved UKP instances of specific distributions saved in files, and executables that read the instances, solved them and returned the solving time. in this experiment, the instances are CSP instances, and the continuous relaxation of the those instances is solved by using the pr

Two items with the same weight never exist in a cutstock knapsack problem.
The dual values of the CSP master model can be negative or zero (non-positive). Such non-positive values can break code optimized with the assumption that all items have a positive profit. Because of this, on our implementation we have removed those items before solving the knapsack problem with a non-cplex solver (cplex have no problem with such nonpositive profits), and remaped the solution items to their old indexes before returning the solution to the master problem. 

% ALREDY EXPLAINED IN ANOTHER WAY:
%Difference of the number of rolls in the last hexdecimal gigit is normal and caused the by the fact floating point arithmetic don't have infinite precision.
%We do not checked if executing many times the same fp method over the same instance can give different iteration counts. However floating point arithmetic is deterministic, while incorrect from a infinite precision point of view. (CONFIRMED BY EXECUTING THE SAME INSTANCE 100 TIMES)
%The two int ukp5 cutstock solvers have difference in the number of iterations. Ex.: BPP\_1000\_1000\_0.2\_0.8\_9.csp.txt, ukp5\_int\_cutstock has 6908 iterations and ukp5\_int\_ns\_cutstock has 6719 iterations. The only thing that change between the two is the order of the array. The two always get an optimal solution with the smallest weight. At iteration 5879, the optimal solution given by the two methods differ, for the same knapsack, both solutions are optimal (consequently have the same profit value), and both have the same weight (that is the smallest possible for an optimal solution of that knapsack). This little change creates a cascading effect that makes one method end 189 iterations before the other. This can seem little considering the number of iterations of the biggest iteration number (189 of 6719 is about 2.81\%), but this difference happened at iteration 5879 so there were only 1029 iterations left for the run with more iterations (one code ended 18\% faster simply by an arbitrary distiction between two optimal solutions of the same weight).
%Add graphs created to the dissertation. Say that ending at 80\% iterations of the more iterations method don't mean the code was 25\% faster as the knapsack times aren't uniform. If the divergence occurs close to the end and is significant (if the difference occured at iteration x, and x is about 80\% of the iterations of the method with the greatest number of iterations, and the less iterations method ended doing 90\% of the more iterations method, those 10\% were composed entirely of instances significantly harder than the first ones (that were equal). This effect wasn't studied extensively because of time and scope reasons.
%The graph of iterations shown by the optimal solutions given by the cplex knapsack solver seems to result in a smaller number of iterations. It's expected that the UKP5 versions will vary between 0.2 and -0.2 from the mean (about 0 to 45\% relative to each other) as we have examinated this behavior before. This don't see to be a B\&B approach effect as MTU1 don't behave the same way. The author hypothesis is that the UKP5's idiosyncrasy of returning always one of the smallest optimal solutions can be unfavoreable to its relationship with the master model. The gap between an optimal solution weight and the knapsack capacity is interpreted as a roll's waste at the master problem. 

\begin{figure}[h]
\caption{CSP Knapsack Solver Total Time}
\begin{center}
<<knapsack_time,fig=true,echo=false>>=
# Note that the points at y = 600 were introduced to allow
# for the visualization of the runs that resulted in timeout.
# Also, note that after the visible mtu1 point closest to the top right
# all other points are below cplex points (representing the mtu1 timeouts).
# For the hardest problems, there's a small but clear pattern that shows
# advantage of the methods that sort the array by efficiency over the ones
# who doesn't.
csv_no_na <- csp_csv
csv_no_na[is.na(csp_csv$hex_sum_knapsack_time), ]$hex_sum_knapsack_time <- 600
#csv_no_na <- csv[!is.na(csv$hex_sum_knapsack_time), ]
csv_no_na_order <- csv_no_na %>% group_by(filename) %>% mutate(mean_methods_time = mean(hex_sum_knapsack_time)) %>% arrange(mean_methods_time)
csv_no_na_order$filename <- factor(csv_no_na_order$filename, levels = unique(csv_no_na_order$filename))
ggplot(csv_no_na_order,
       aes(x = as.numeric(filename),
           y = csv_no_na_order$hex_sum_knapsack_time,
           color = algorithm)) +
  xlab('Instance index when ordered by the\nmean time methods spent solving knapsack subproblems') +
  ylab('Knapsack\'s solver total time (seconds, logarithmic scale)') +
  geom_point() +
  scale_y_log10() +
  theme(legend.position = 'bottom')
@
\end{center}
\legend{Source: the author.}
\label{fig:csp_knapsack_time}
\end{figure}

\begin{figure}[h]
\caption{CSP Master Model Solver Total Time}
\begin{center}
<<master_time,fig=true,echo=false>>=
master_time <- csp_csv
master_time[is.na(csp_csv$hex_sum_knapsack_time), ]$hex_sum_knapsack_time <- 600
master_time[is.na(csp_csv$hex_sum_master_prob_time), ]$hex_sum_master_prob_time <- 600
#csv_no_na <- csv[!is.na(csv$hex_sum_knapsack_time), ]
master_time <- master_time %>% group_by(filename) %>% mutate(mean_methods_time = mean(hex_sum_master_prob_time)) %>% arrange(mean_methods_time)
master_time$filename <- factor(master_time$filename, levels = unique(master_time$filename))
ggplot(master_time,
       aes(x = as.numeric(filename),
           y = master_time$hex_sum_master_prob_time,
           color = algorithm)) +
  xlab('Instance index when ordered by the\nmean time methods spent solving the master model') +
  ylab('Master model solver total time (seconds, logarithmic scale)') +
  geom_point() +
  scale_y_log10() +
  theme(legend.position = 'bottom')
@
\end{center}
\legend{Source: the author.}
\label{fig:csp_master_time}
\end{figure}

\begin{figure}[h]
\caption{Percentage of time taken by solving knapsack subproblems}
\begin{center}
<<cutstock_master_knap_corr,fig=true,echo=false>>=
corr_time <- csp_csv[complete.cases(csp_csv),]
corr_time$relative_time <- corr_time$hex_sum_knapsack_time / (corr_time$hex_sum_master_prob_time + corr_time$hex_sum_knapsack_time)
corr_time <- arrange(corr_time, relative_time)
corr_time$filename <- factor(corr_time$filename, levels = unique(corr_time$filename))
ggplot(corr_time,
       aes(x = as.numeric(filename),
           y = relative_time * 100,
           color = algorithm)) +
  xlab('Instance index when ordered by the y axis value') +
  ylab('How much of the total time was\nspent solving knapsack subproblems (in %)') +
  geom_point() +
  theme(legend.position = 'bottom')
@
\end{center}
\legend{Source: the author.}
\label{fig:percentage_knap_subproblem}
\end{figure}

\subsection{The differences between the number of iterations}
\begin{figure}[h]
\caption{How many iterations each method used relative to the one with the biggest count for the same instance (in \%)}
\begin{center}
<<relative_iter_count,fig=true,echo=false>>=
csv_no_na <- csp_csv[complete.cases(csp_csv),]
csv_no_na <- select(csv_no_na, filename, algorithm, total_iter)
iter_t <- csv_no_na %>%
  group_by(filename) %>%
  mutate(max_iter_for_filename = max(total_iter)) %>%
  arrange(max_iter_for_filename)
iter_t$norm_iter <- iter_t$total_iter / iter_t$max_iter_for_filename
iter_t$filename <- factor(iter_t$filename,
                          levels = unique(iter_t$filename))
ggplot(iter_t,
       aes(x = as.numeric(filename),
           y = norm_iter * 100,
           color = algorithm)) +
  ylab("Percentage of iterations relative to the method\nwith the biggest count for the same instance") +
  xlab("Instance index when ordered by the\niteration count of the method with the biggest count") +
  geom_point() +
  theme(legend.position = 'bottom')
@
\end{center}
\legend{Source: the author.}
\label{fig:relative_iter_count}
\end{figure}

In Figure~\ref{fig:relative_iter_count}, it's possible to see that the methods had different numbers of iterations for the same instance. The author used a constant for the CPlex seeds of the master problem solver and the method that used CPlex as the knapsack solver (cplex\_cutstock). Each method is deterministic and will give the same iteration number if run many times over the same instance. However, the iteration number can be different for different methods over the same instance. This happens because the knapsack subproblems can have many optimal solutions, and different methods break this tie in different ways (or they return the first proven optimal solution found, that can be also different). The chosen optimal solution will affect the master problem, that will generate a slight different knapsack subproblem. This effect cascades and can change the profit values of all next knapsack subproblems, and the number of knapsack subproblems that are needed to solve (that's the same as the number of iterations).

The cplex\_cutstock stands out having many of the smallest number of iterations. The author can't explain what property CPlex knapsack solutions have that create such a difference. The effect don't seems to be associated with the B\&B approach, as MTU1 don't exhibit the same effect. We can only explain the differences in iteration count between the UKP5 variants (further discussed at Section \ref{sec:diff_it_count}). The UKP5 variants always return an optimal solution with the smallest weight possible, this translates to generating patterns with the greatest possible waste (the gap between optimal solution weight and the knapsack capacity equals to the waste in a CSP pattern). The author's hypothesis is that such characteristic can affect negatively the number of iterations needed to find a pattern set that can't be improved (loop end condition).

\subsection{The only outlier}

In all the experiments presented at this work, the author verified if the optimal solution value (the value of the objective function) was the same for all methods. Given the innacurate nature of floating point arithmetic, in this experiment, the optimal solution values differed between the methods. However, except by one outlier, no method had an absolute difference from the mean optimal value greater than \(2^{-20}\). In other words, for each instance, the optimal value in rolls of any method subtracted by the mean of the optimal value in rolls for all methods was smaller than \(2^{-20}\) and greater than \(-(2^{-20})\). 

The outlier was the run of ukp5\_int\_cutstock over N4C1W1\_O.csp, that resulted in 257.7500 rolls, while the other methods resulted in 257.5833 rolls for the same instance (a 0.1667 roll difference). Hoping to find the origin of the outlier, the author tracked what differed between ukp5\_int\_cutstock and ukp5\_fp\_cutstock, that are the same algorithm with the same item ordering, but using integer profits instead of the floating point ones.

The solutions given by the two methods at the third iteration differed. The solution given by the fp method isn't the optimal solution for the integer method (and vice-versa) because when the items profit were made integer (by multiplying them by \(2^{40}\) and rounding them down) a small value was lost with the rounding. For the floating point method, one solution was the optimal one, for the integer method, many solutions were optimal (including the one that was optimal for the floating point method). The tie breaker of the integer method choose a different optimal solution, and started a cascade effect (one different solution from the knapsack solver is sufficient to change all subsequent knapsacks generated by the master problem solver).

The author found that differences between knapsack solutions because of precision loss, followed by the cascade effect, are common. However, there was only one outlier, so the precision loss alone don't explain the outlier. The precision loss only explains the difference in the number of iterations between ukp5\_int\_cutstock and ukp5\_fp\_cutstock. The section \ref{XXX} has further discussion on how small changes to the algorithm changes the iteration count, and except by this outlier, this don't seem to affect the final result. The section was written based on the analysis made while trying to (unsuccessfuly) find the origin of the outlier.

\subsection{Iteration count differences between very similar methods}
\label{sec:diff_it_count}

In Figure \ref{fig:comp_num_iter}, we can see the relative difference of the number of iterations between any two versions of UKP5 that share one trait and differ in the other. The traits are the type used for the item profits (floating point or integer), and if the items were sorted by efficiency or not. It's interesting that such small variations of the same algorithm can yield significant differences to the numbers of iterations. 

\begin{figure}[h]
\caption{Relative differences between the numbers of iterations}
\begin{center}
<<comp_num_iter,fig=true,echo=false>>=
t1 <- compare_num_iter(csp_csv, 'ukp5_int_cutstock', 'ukp5_int_ns_cutstock')
  #geom_point() + ggtitle('UKP5 Integer\n(sorted by efficiency and not sorted')
t1$type <- 'Integer (sort by efficiency vs no sort)'
t2 <- compare_num_iter(csp_csv, 'ukp5_fp_cutstock', 'ukp5_fp_ns_cutstock')
t2$type <- 'Floating Point (sort by efficiency vs no sort)'
  #geom_point() + ggtitle('UKP5 Floating Point\n(sorted by efficiency and not sorted)')
t3 <- compare_num_iter(csp_csv, 'ukp5_fp_ns_cutstock', 'ukp5_int_ns_cutstock')
t3$type <- 'No Sort (FP vs Integer)'
  #geom_point() + ggtitle('UKP5 no sort\n(Floating Point vs Integer)')
t4 <- compare_num_iter(csp_csv, 'ukp5_fp_cutstock', 'ukp5_int_cutstock')
t4$type <- 'Sort by efficiency (FP vs Integer)'
  #geom_point() + ggtitle('UKP5 sorted by efficiency\n(Floating Point vs Integer)')
#grid.arrange(t1, t2, t3, t4)
t <- rbind(t1, t2, t3, t4)
ggplot(t,
       aes(x = order,
           y = norm_diff*100)) +
  ylab(paste('Deviation of the number of iterations of',
             '\none method relative to other (in %)')) +
  xlab("Instance index when ordered by the value at axis y") +
  facet_wrap(~ type) + geom_point()
@
\end{center}
\legend{Source: the author.}
\label{fig:comp_num_iter}
\end{figure}

All four charts show that some instances were solved in the same number of iterations between the two variants compared; those instances form a horizontal line at \(y = 0\). The two variants with less difference in the number of iterations are the two integer variants with different sort methods (top right chart). This was expected. If the profits are integer, the change in the items order only makes difference when a knapsack has two optimal solutions tied for the smallest optimal solution, and the difference of order makes UKP5 choose different solutions (the item order acts as a tiebreaker in this case).

When the items profit is a floating point, the order the items are added influences the result (floating point addition isn't associative). The difference is usually very small (least significant hex digit of a double's mantissa), but the magnitude of the difference is irrelevant for the algorithm, that will choose the solution with the greatest profit. As the item order changes the order of the additions inside UKP5, what solution will be considered optimal (between solutions with very similar profit values) will change.

When we compare variants of the same sort method, but with different profit types (two charts at bottom of the figure), the differences aren't caused by solutions tied for optimal with smallest weight, or by the lack of the associative property in the floating point addition, but by the loss of precision caused by the use of integer values. Solutions will be different because for the integer version, some items will have the same profit value, while for the floating profit version, one of those items will have a profit greater than the others (yet for a profit difference smaller than \(2^{-40}\)).

We can conclude that choosing one optimal solution over another (or choosing between two solutions with values very close to the optimal) can change the number of iterations considerably, without affecting the master problem optimal solution value (in a significant manner). A researcher studying UKP algorithms for this application has to consider the implications of this fact in his or her work.%It's important to note that, while all knapsacks of the same instance have the same capacity, quantity of items and weights, the change of the items profit change the distribution, and consequently, the hardness of the knapsack.

In the section \ref{XXX} (Future Works) we will see some questions raised by this conclusion. Although there was one outlier, converting profit values to integer seems to be a valid method to test classic UKP algorithms (that work only with integer profits) for solving CSP subproblems.

\subsection{Algorithms not used at this benchmark}

Some algorithms were available, but weren't used at this benchmark. Those are the two algorithms from \cite{XXX}, the algorithm from \cite{XXX} (mgreendp, from Section \ref{XXX}), the MTU2 algorithm and EDUK/EDUK2 (PYAsUKP).

The two algorithms from \cite{XXX} weren't used for the same reasons described at Section \ref{XXX}. The author tried to use mgreendp, but it fails if the two best items have the same efficiency, what happen sometimes at knapsack subproblems. The MTU2 algorithm wasn't used because it was designed for large UKP instances, and calls MTU1 over small numbers of items, what would end up with times similar to MTU1 for many instances.

This benchmark needs to call the UKP solving methods from inside the C++ code with the CPLEX master's model. The author tried to integrate EDUK and EDUK2 (that are written in OCaml) to the benchmark, using the interface between C/C++ and OCaml, but the examples provided with the PYAsUKP sources for this kind of integration weren't working. The author could have saved the knapsack instances generated when solving the CSP problem with other algorithm, and solved them using the PYAsUKP executable. This wasn't done because of the following reasons: many knapsacks are small and solved very fast, and in those cases the PYAsUKP timing report zero or innacurate values; the solutions returned by PYAsUKP would affect the next knapsacks generated, so the comparison would be unfair.

