\chapter{Prior Work}

It's important to note that the name ``unbounded knapsack problem'' is more recent than the problem itself. To the best of the author's knowledge, the term was used by the first time in \cite{mtu2} (1988\~1989). Earlier, papers simply refered to `a' or `the' knapsack problem(s), there was no worry about specifying what variant of the problem was being refered. An earlier paper from the same author tackled both UKP and BKP\cite{mtu1} and called them, respectively, the General Unconstrained Knapsack Problem (GUKP) and General Constrained Knapsack Problem (GCKP). In the same paper, the adjective unidimensional was also used to characterize both variants. More recently, the term UKP seems to be accepted as norm, also unidimensional is considered the default and the term multi-dimensional is used to differ from it. Ans researcher making a literature review about a specific variant of the knapsack problem should be aware of such caveat.

This literature review starts at 1961, when the `column generation' approach was proposed\cite{gg-61} by Gilmore and Gomory. The main utility of the `column generation' approach was to avoid the existence of an exponential number of variables when solving the tightest linear programming model of BPP and CSP. The relationship between UKP and BPP/CSP was alredy superficially described at Section \ref{XXX}, and its technical detail are described at Section \ref{XXX}. The UKP isn't solved, only it's said that ``the auxiliary problem will be of the integer programming variety, but of such a special type (the `knapsack' type) that it is solvable by several methods''\cite[2]{gg-61}. Two years later, in \cite{gg-63}, the same authors proposed an specific algorithm for UKP, and experiments solving BPP and CSP instances were executed. Some findings of this experiments will be discussed in Section \ref{XXX}.

In 1966, Gilmore and Gomory published \cite{gg-66}, that focused the one-dimensional and two-dimensional knapsack problems related to BPP and CSP. In this paper, the algorithm for UKP that was first described at \cite{gg-63} is discussed more profoundly. The author of this work rediscovered this algorithm and published a paper about it while thinking it was novel\cite{sea2016}, it apologizes to the academic and scientific community for this overlook. Further information about the algorithms of \cite{gg-66} and \cite{sea2016} in Section \ref{XXX}. An small improvement for the algorithm of \cite{gg-66} was proposed in \cite{better_step_off}. The algorithm was implemented by the author of this work and its results can be seen in Section \ref{XXX}.

The papers \cite{cabot} and \cite{turnpike} were published shortly after. Both papers are behind a paywall and the author hadn't access to them. However, the algorithm from \cite{cabot} was compared indirectly by \cite{mtu1} (that will be discussed in the sequence). The comparison found that the the algorithm from \cite{cabot} was dominated by the algorithm proposed in \cite{mtu1}. In \cite{better_step_off}, is implied that the proposed algorithm is an improvement over the algorithm from \cite{turnpike}. However, the author think that both conclusions should be taken with a grain of salt. In \cite{mtu1}, empirical evidence was presented, but using datasets that are considered small by today standards and that used an item's distribution that the author has reservations about (see Section \ref{XXX}). In \cite{better_step_off}, the claims are backed up by theoretical reasoning, but empirical evidence shown in Section \ref{XXX} revealed some unexpected behavior over some instance datasets. The author thinks that revisiting such abandoned algorithms would be interesting, but it wasn't a priority in this work.

%This generated distortions and abandoned paradigms. The whole shift from dynamic programming methods to B\&B methods can be associated with the tests being made over ever-increasing instances; randomly generated with little or some correlation between profits and weights; and the use of naive dynamic programs; the shift of PYAsUKP to revisit dynamic programming can easily be correlated with the evolution of the artificial instances generated to be difficult to solve by B\&B while small (on capacity and number of items); the DP methods began to shine again, as the instances had small dimensions and a structure that made them hard to solve by B\&B but not so much by a good (non-naive) DP algorithm. The UKP5 is only the last part of this history.

%The paper ``A Finite Renewal Algorithm for the Knapsack and Turnpike Models'' and http://pubsonline.informs.org/doi/abs/10.1287/opre.18.2.306 were behind a paywall and the paper ``A better ...'' said to have a better algorithm anyway. 

%B\&B methods appear, and begin to claim that DP isn't efficient, they don't compare with the DP methods, only between each other, using some distributions and without motivation or real world instances

% MTU1 and MTU2 papers:
In the 70's, there was a shift of attention from the DP approach to the B\&B approach.
The first algorithms using this approach seem to be the Cabot's enumeration method\cite{cabot} and the MTU1 algorithm\cite{mtu1}, both cited last paragraph.

The algorithm MTU1 was proposed at paper \cite{gen_uni_knap_prob}, with the name of KP1 at the time (we will refer to this paper as the `MTU1 paper'). % old paper of MTU1
To the author's knowledge, the MTU1 paper was the only paper to present experimental results comparing the B\&B and DP methods, before PYAsUKP paper, in 2009\cite{pya}.
Unfortunately, for today standards, the instances used for the comparison were very small (what's understandable considering the paper's publishing date). 
The numbers of items items used were 25, 50 and 100 for instance; the weights (and profits) had values between 11 and 110 (in the case of the profits 120); the knapsack capacity was chosen between \(0.2 sum_{i \in n}{w_i}\) and \(sum_{i \in n}{w_i}\) (the capacity value isn't proportionally a problem, only absolutely); the distributions used were uncorrelated and weakly correlated (\(p_i = w_i + \alpha\), where \(\alpha\) was randomly choosen from -10 and 10 following an uniform distribution).

The comparison was between KP1 (MTU1), the dynamic programming algorithm called `periodic step-off' from \cite{gg66} (that we will call G.G., for short), and two B\&B algorithms for the 0-1 KP (for which the UKP instances were tranformed in 0-1 KP instances). The best results were from MTU1, and the second best from the G.G.  algorithm. However, the instances were too small to draw strong conclusions, and the relative difference between G.G. and MTU1 average times wasn't even an order of magnitude apart. The G.G. algorithm was about four times slower than MTU1 in the instances with (\(n = 25\); about two or three times slower in the instances with \(n = 50\)); and less than two times slower in instances with \(n = 100\). This trend could indicate that for big instances, G.G. algorithm would have better times than MTU1 (G.G. algorithm would have a big constant multiplier in better average-case assimptotic complexity).

%The MTU1 paper \cite{gen_uni_knap_prob} also makes an indirect comparison with Cabot's B\&B algorithm. The MTU1 paper uses the dataset described in Cabot's paper\cite{cabot}, and arrives at the conclusion Cabot's algorithm is clearly dominated by MTU1. The instance dimensions are only a little different to the ones used in their experiment described above (number of items up to 80, only uncorrelated distribution).

The experiments described above were used by the authors on another paper\cite{large_mtu2} to state that ``The most efficient algorithms for the exact solution of UKP [...] consist of two main steps: Step 1. Sort the item types according to (5). Step 2. Find the optimal solution through branch-and-bound.''. What established B\&B as most efficient approach for the UKP. This new paper introduced MTU2 (and we will refer to it as the `MTU2 paper').

The MTU2 algorithm was designed for very large instances (up to 250,000 items). Only sorting the whole item's list was already computationally expensive, and the solutions often involved only the most efficient items. The MTU2 main feature was grouping and sorting only the \(k = max(100, n/100)\) most efficient items (called core problem), and calling MTU1 over it. If the optimal solution of the core problem was proven optimal for the original problem, the algorithm stopped. Otherwise, the tentative solution was used as bound to remove dominated items (that could be proven to not contribute to the solution), and it added the \(k\) most efficient items outside the core problem to it, restarting the process. 

The algorithms' comparison included only MTU1 and MTU2. The datasets used in the paper were large, but artificial and abundant in dominated items. A more detailed analisys of the datasets and the experiment setting is available at Section \ref{XXX}. The MTU2 was clearly the better algorithm for the chosen datasets.

MTU2 was adopted for subsequent paper as the baseline for testing new algorithms for UKP. The author credit this to many factors, such as: the code of MTU2 was freely available; the algorithm was well and thoroughly explained in Martello and Toth publications; it presented empyrical evidence of dominating other methods, and therefore, comparing with it would remove the necessity of comparing to many other algorithms; its description stated that it was designed for very large instances. However, MTU2 don't dominate completely MTU1, it simply was better for the chosen instances (that were choosen to exactly to evidence this). Instances where the MTU2 needs to repeat the process of adding items to the core problem many times can be more easily solved by MTU1 than by MTU2. Unfortunately, the works that followed choose to compare only against MTU2.

% EDUK paper, pages relevant are 8 to 14, 
% Author claim uncorrelated instances aren't realistic, the realistic instances have no simple dominance, and sometimes no simple/multiple/collective dominance. The instances can be seen as better than the older ones, but are yet artificial instances that aren't guaranteed to model any real world instances.
% MTU2 greatly exploits over the kind of dominance found in uncorrelated instances and as such is bound to be better there
% MTU2 has problems with the hard instances, and EDUK has not, making it the champion
% There's a comparison of EDUK with the naive programming algorithm, over small instances, the comparison makes sense in the context of the CSP/BPP, but the naive algorithm is orders of magnitude slower than the improved version of Gilmore and Gomory, and in the testing, their times are oly slight above EDUK, in a scale where the times are measured in centiseconds

% EDUK, and EDUK2 follow, they only test against themselves and B\&B methods using instances that were made to be hard for B\&B , do not implement the Fred Glover algorithm and don't adapt MTU2 to using 64 bits (nor test againt both MTU1 and MTU2),
% Fred Glover algorithm was not implemented by time reasons, and ould make a nice addition to any new comparison between UKP solving methods, yet, the author has no idea of their real performance, as the experiments they used to compare themselves to MTU1/2 had some flaws.

%That the whole UKP literature has many problems: David Johnson - Pitfall 4 "Start by using randomly generated instances to evaluate the behaviour of algorithms, but end up using algorithms to investigate the properties of randomly-generated instances."; David Johnson "[...] problems without applications do not have real-world instances, so the experimenter is left to invent (and justify) test data in a vacuum. And referees may well question the testing of code for algorithms that will never be used in practice." (p. 5). We are researching instances that are hard for UKP, not that this has a utility on itself.

% A LITTLE OF CSP HISTORY TOO
%\section{Cutting Stock}

% BLOCKQUOTE BELOW
% This corresponds to widely-used practices and general beliefs expressed in the literature. Usually, in the context of an LP- based branch-and-bound algorithm, where the LP relaxation is solved using column generation, cutting planes are carefully selected in order to avoid the destruction of the structure of the pricing problem. This viewpoint is shared by numerous authors. Barnhart, Hane, and Vance [BHV00], for example, mention as one of the main contributions of their branch-and-price-and-cut algorithm ``a pricing problem that does not change even as cuts are added, and similarly, a separation algorithm that does not change even as columns are added.'' (BELOV, page 3)

%About the pricing problem with cuts. ``It is much more difficult than the linear knapsack problem for column generation arising when no cuts are added in the LP.'' (BELOV, page 26)

% The paragraph below is only about 2D-2CP?
%``The general-purpose cuts considered in this work make the pricing problem extremely complicated and much implementation effort is required. The question arises, whether any other classes of cuts, which do not complicate the pricing problem, e.g., cover inequalities in 2D-2CP, can provide a comparable contribution.'' (p. 46)

% maybe this goes in another section
%	timeline: DP -> huge random instances -> B\&B is better (no empiric evidence) -> instances that are hard for B\&B (linear distribution) -> PYAsUKP is better than B\&B in instances designed to be hard to solve by it (in fact any DP non-naive DP solution would have results better than B\&B methods only, like MTU1 and MTU2). also, using MTU2 instead of MTU1 shows a lack of understanding of the methods. MTU2 was developed for very large random instances, not for relatively small and hard (distribution-wise) instances


% MAYBE ADD
% The subject that I (Henrique Becker) am studying (at the year of 2015/2016) is the exact resolution of the UKP (unbounded knapsack problem) with the objective of solving the subproblems generated by the column generation approach of solving BPP/CSP (Bin Packing Problem and Cutting Stock Problem). The paper "An Improved Knapsack Solver for Column Generation" (2013, Klaus Jansen and Stefan Kraft) seemed (by the title) to be relevant to my studies. However, the paper presents a variant of the UKP, the UKPIP (the Unbounded Knapsack Problems with Inversely Proportional Profits) that's a subproblem of the Variable-Sized Bin Packing (VBP) that's a generalization of the BPP when there's the possibility of choosing between many bin sizes (on the classical BPP there's one fixed bin size for instance). The UKIPIP is a generalization of the UKP, in the sense that it allows for choosing between many knapsack sizes, and when restricted to only one knapsack size it's equivalent to the UKP. Yet, when there's many knapsack sizes to choose from, the same solution in different knapsacks will yield a diferent profit value, as the profit value of the items is scaled by the knapsack size (the smaller the knapsack size, the bigger the profit value for the same solution); and the solution is a multiset of items and the one knapsack choosen to store these items. The paper seems to focus on approximation methods to solve the UKPIP and the other two variants of it (the bounded one, BKPIP and the 0-1 one, 0-1 KPIP). So the subject of the paper (approximation algorithms for a variant of the UKP as subproblem of a variant of the BPP) is outside our area of interest (exact algorithms for the classical UKP as a subproblem of the classical BPP).
