\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\usepackage{mathtools}
%DeclarePairedDelimiter\floor{\lfloor}{\rfloor} 

\setcounter{tocdepth}{3}
\usepackage{graphicx}

% ADD THOSE LIBS AT THE PACKAGE FOR THE JOURNAL
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{url}
\urldef{\mail}\path|{hbecker, buriol}@inf.ufrgs.br|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{UKP5: a simple imperative algorithm for the unbounded knapsack problem}

% a short form should be given in case it is too long for the running head
\titlerunning{UKP5: a simple imperative algorithm for the UKP}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Henrique Becker \and Luciana Salete Buriol}
%
\authorrunning{UKP5: a simple imperative algorithm for the UKP}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Federal University of Rio Grande do Sul,\\
Paulo Gama. 110, 90040-060, Porto Alegre, RS, Brazil\\
\mail\\
\url{http://ppgc.inf.ufrgs.br/}}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle

% NOTE: IMPORTANT: The abstract need to be between 70 and 150 words. Always check this before editing.
% Info used in the average time:
% ss=639/8
% sc=(246+584+1835+0+4197+14760+598+1037+3931+1313+8297+26161)/(5+7+20+11+49+107+6+9+21+19+54+108)
% per=(1700+20861+2768+23358)/(142+1020+159+686)
% wcd=(78+338+1308+11918)/(5+49+99+469)
% saw500=479/74
% saw600=(188+1044+6058)/(11+101+1413)
% saw=saw=((saw500*500)+(saw600*600))/1100
% avg=((ss*400)+(sc*240)+(per*800)+(wcd*2000)+(saw*1100))/(400+240+800+2000+1100)
% avg ~= 30
\begin{abstract}
In this paper we present a novel imperative algorithm for solving the unbounded knapsack problem. We call this algorithm \emph{ukp5}. This algorithm has the following advantages over eduk2 (algorithm that until now claimed to be the state of art): it's much simpler; can be easily implemented on an imperative programming language (eduk2 uses concepts of functional programming that undermine this goal); and our implementation is, in average, about thirty times faster than the only known eduk2 implementation (pyasukp), using as benchmark the family of hard instances proposed by the authors of eduk2. Our algorithm applies the concepts of sparsity, dominance, and periodicity integrated on an imperative ``dynamic programming''-like algorithm.
\keywords{unbounded knapsack problem, ukp, imperative programming, dynamic programming}
\end{abstract}

\section{Introduction}

The unbounded knapsack problem (UKP) is a variation of the well-known bounded knapsack problem (BKP), where there's an unbounded quantity of each item type available.

The following formal notation of the UKP will be used for the rest of the article: an UKP instance is composed of a capacity \(c\), and a list of \(n\) items\footnote{The order of the items doesn't change the optimal solution value. Nonetheless, the order of the items is relevant for some algorithms performance, and we will refer to items by its position in the item list. For these reasons we will refer to the input items as a list, and not as a set.}; each item can be referenced by its index in the item list \(i \in \{1\dots n\}\); each item \(i\) has a weight value, denoted by \(w_i\), and a profit value, denoted by \(p_i\); a solution is an item multiset\footnote{A multiset is a set that allows multiple copies of the same element.}; the sum of the items weight or profit of a solution \(s\) will be denoted by \(w_s\) and \(p_s\), repectively; a valid solution is solution \(s\) where \(w_s \leq c\); an optimal solution \(s*\) is a valid solution with the greatest \(p_{s*}\); we will refer to the profit value shared between all optimal solutions for a capacity \(y\) as \(opt(y)\), if we ommit the capacity then \(c\) is implied; the UKP objective is to find an optimal solution for the given UKP instance. In linear programming notation, we could write:

\begin{align}
  maximize: &\sum_{i=1}^n p_i x_i\label{eq:objfun}\\
subject~to: &\sum_{i=1}^n w_i x_i \leq c\label{eq:capcons}\\
            &x_i \in \mathbb{N}_0\label{eq:x_integer}
\end{align}

In this article, we assume that the capacity \(c\), the quantity \(n\) and the weights of the items \(w_i\) are all positive integers. The quantities of each item \(i\) in an optimal solution are denoted by \(x_i\), and are restricted to the non-negative integers\footnote{If the quantities weren't restricted to the integers the problem wouldn't be NP-Hard.}, as shown in \eqref{eq:x_integer}. The profits of the items \(p_i\) are all positive real numbers.

The efficiency of an item \(i\) is the value of \(\frac{p_i}{w_i}\), and will be denoted as \(e_i\). We use \(w_{min}\) and \(w_{max}\) to denote the smallest item weight and the biggest item weight, respectively. Also, we refer to the item with the greatest efficiency as the \emph{best item}\footnote{If the efficiencies tie the best item is the item with the lowest weight between the tied items.}, and the item with the lowest weight as the \emph{smallest item}. If two or more items have the same weight we consider only the one with the best profit (the other can be ignored without loss to the optimal solution value), if they have the same weight and profit we consider them the same item.

\subsection{Bibliographic Research, Motivation and Contribution}

The UKP it's NP-Hard, so it's always positive to have efficient algorithms for solving it. It arises in real world problems mainly as a subproblem of the Bin Packing Problems (BPP) and Cutting Stock Problem (CSP). Both BPP and CSP are similar and of importance for the industry \cite{CSURVEY} \cite{CGG} \cite{CGG2}. The currently fastest known solver for BPP/CSP\footnote{This statement is based on the results of a recent survey on BPP/CSP solvers with plenty of experimental results, see \cite{CSURVEY}.} uses a technique (introduced in \cite{CGG}) that needs to solve an instance of the UKP at each iteration. The need for efficient algorithms that solve UKP is clear.

Two techniques are often used for solving UKP: Dynamic Programming (DP) \cite{CGAR} CGAR CTCHU and Branch and Bound (B\&B) CMTU2. The DP approach have a stable pseudo-polynomial time \(O(c n)\). The B\&B approach is less stable. It can be much faster than DP on instances with some characteristics, as the rest of division between the weight of the best item and the capacity being small; or the items having a big efficiency variance. Nonetheless, B\&B has always the risk of an exponential time worst case.

The state of art solver on UKP, introduced on \cite{CPYA}, is a hybrid solver that combines both approaches. It tries solving the problem by B\&B, and if this aproach fails to solve the problem fast, then it switches to DP using some data gathered by B\&B to speed-up the process. The solver's name is pyasukp, and it's an implementation of the eduk2 algorithm. We have found that the DP used by pyasukp is very slow when compared to our algorithm. Even solving some instances in less than a centisecond using B\&B, the solver is yet about thirty times slower than our method, in average. Also, our algorithm is much less complex, and can be easily implemented on a imperative language\footnote{The eduk2 algorithm that is the base of the pyasukp solver use concepts of functional programming that make it harder to implement on imperative languages. The own authors of eduk2 admit that implemented pyasukp in OCaml because of this difficulty.}. The periodicity stop condition, and the solution dominance concept, used on ukp5 are algorithm specific. So we don't count them as individual contributions.

We don't take in account the results of the recent paper \cite{CMUL}, because its focus is parallel algorithms, what go beyond the scope of this paper. Yet, regarding to pyasukp, \cite{CMUL} states that ``The algorithm (pyasukp) significantly outperforms all existing algorithms for solving the problem.''.

% AlgorithmsForKnapsackProblemsPsinger -- p. 148, cita 53 (Martello and Toth) to explain why do not transform unbounded knapsack instances in bounded (we could simply say that the lack of restrictions gives more opportunity for optimization) AND 
% Garfinkel, PYAsUKP

\subsection{Dominance}

Dominance, in the UKP context, is a set of techniques for reducing the size of the item list without affecting \(opt\). Clearly, in this conditions, every item that isn't used in an optimal solution could be discarded, but this would need the knowledge of the solution beforehand. This way, a dominance, or a dominance relation, was born as a property that can usually be verified in polynomial time over \(n\), and allows to exclude items without affecting \(opt\). It's easy to see that any polynomial-time algorithm that can be used to reduce the input of an exponential time algorithm is promising. Instances where many items can be excluded by the two simplest dominances (simple dominance and multiple dominance) became known as ``easy'' data instances. So much research about these two dominances was done, that, at the year of 1995, after enumerating the research done until the moment on UKP, Pisinger says ``Seen in this light, perhaps too much effort has been previously been used on the solution of easy data instances.'' \cite[p. 20]{CPISINGER}.

Other two important dominance relations are collective dominance and threshold dominance \cite{CGAR} \cite{CPYA}. These two dominances are too expensive to be done at a preprocessing phase (as simple and multiple dominance). This way, they are integrated on the UKP solving method, and remove items while the algorithm executes. The collective dominance needs to known the \(opt(y)\) to exclude an item \(i\) with \(w_i = y\). The threshold dominance needs to know the \(opt(\alpha\times w_i)\) to exclude the item \(i\) from capacity \(y = \alpha\times w_i\) onwards, where \(\alpha\) is any positive integer.

The specifics of those four dominance relations are very well described at \cite{CPYA}. The ukp5 don't make use of any of those four dominances. It uses a ``solution dominance'' approach that is implicit in the algorithm and achives results similar to the four dominances. The decision to not use the known dominances is based on two main points: ukp5 was made to solve hard data instances, applying the dominance over hard data instances don't always pay off; our ``solution dominance'' already excludes items, so any benefits of the dominances would be diminushed, but the cost would be the same. For the rest of the article, when we say that an specific item is dominated, we are saying that it could be excluded without changing \(opt\).

% CITE ``'' p. 20, Pisinger

% In \cite{\cite{CPISINGER}}, a complete analysis of the simplest dominance type (simple/multiple dominace) is given. 

\subsection{Periodicity}

A periodicity bound \(y*\) is an upper capacity bound for the existence of optimal solutions without the best item. In another words, it's a guarantee that any solution for an instance where \(y* \leq c\) will have at least one copy of the best item. The periodicity bound is specially usefull because it can be applied repeatedly. For example, let \(c = 1000\), \(y* = 800\) and \(w_b = 25\) where \(b\) is the best item; because of \(y* \leq c\) we know that any optimal solution will have a copy of \(b\), so we can add one \(b\) to the solution and combine with an optimal solution for \(c = 975\); but \(975\) is yet bigger than \(800\), so we can repeat the process until \(c = 775\). This way, for any \(y* \leq c\) we can reduce the UKP instance capacity by \(max(1, \lceil(c-y*)/w_b\rceil\times w_b)\) and add to a reduced instance optimal solution \(max(1, \lceil(c-y*)/w_b\rceil)\) copies of \(b\).

There exists many proposed periodicity bounds, but some have undesired complexities over \(n\) (as \(O(n^2)\)\cite{CBADBOUND}), others depend on specific instance characteristics (as \cite{BADBOUND2}\cite{CPYA}), so we opted by using only a ukp5-specific periodicity bound described later and the \(y*\) bound described on \cite[p. 223]{CGAR} (that is \(O(1)\) on an item list ordered by non-decreasing efficiency and is generic).

\section{The proposed algorithm: UKP5}

The ukp5 is inspired by the DP algorithm described in \cite[p. 221]{CGAR}. The name ``ukp5'' comes from its five improvements over the \cite{CGAR} DP algorithm: improved solution symmetry pruning performance (caused by the reversal of the solution generation direction); removal of the \(w_i \leq y\) test (substituted by the use of a bigger array); sparsity (no concept of ``solutions with remaining space''); pruning dominated solutions (this concept will be addressed in more detail later); an algorithm-specific periodicity stop condition (idea mentionated on \cite{CGAR}, but not integrated on the algorithm). We do not intend to discuss every of those changes in detail, as this is outside the scope of this paper. We will discuss the algorithm born from all those changes, and focus on the most interesting of them.

\section{Explanation of the core algorithm ideia}

This section is better read with a copy of the Algorithm \ref{alg:ukp5} at hand. We use the \textbf{continue} keyword at line \ref{alg:continue} to mean that the loop ends its current iteration and begin the next iteration (as is common in C-like programming languages).

When the algorithm ends, the \(opt\) variable holds the optimal solution value, and \(y_{opt}\) holds the lowest weight of an optimal solution. We have two main data structures, the arrays \(g\) and \(d\). The \(g\) is a sparse array where we store solutions profit. If \(g[y] > 0\) then there exists a non-empty solution \(s\) with \(w_s = y\) and \(p_s = g[y]\). The \(d\) array stores the index of the last item used on the solution. If \(g[y] > 0 \land d[y] = i\) then the solution \(s\) with \(w_s = y\) and \(p_s = g[y]\) has an copy of item \(i\). This array makes trivial assemble the solution\footnote{Note that \(j = d[y_{opt}]\) is the index of the last item used, \(d[y_{opt} - w_j]\) is the index of the penultimate item used, and so on, until \(d[0]\).}, but its main utility is to to drastically speed-up the optimal computation.

Our first loop (lines \ref{begin_trivial_bounds} to \ref{end_trivial_bounds}) simply stores all solutions made of one item at the arrays \(g\) and \(d\). Now, for a moment, let's ignore lines \ref{if_less_than_opt_begin} to \ref{if_less_than_opt_end} and replace \(d[y]\) at line \ref{main_inner_loop_begin} by \(n\). After these changes, we could say that what our second loop (between lines \ref{main_ext_loop_begin} and \ref{main_ext_loop_end}) do is: iterate \(g\) and when it finds a stored solution (\(g[y] > 0\)) it tests \(n\) new solutions (the combinations of the current solution with every item); then it substite solutions already stored if exists a new solution with the same weight and a greater profit. This is basically what our algorithm do, but those two small code changes make two drastic performance changes.%, specially when combined. 

When we add the lines \ref{if_less_than_opt_begin} to \ref{if_less_than_opt_end} the algorithm will stop creating new solutions from dominated solutions. If a solution \(s\) with a lesser weight (\(w_s < y\)) has a bigger profit (\(p_s = opt > p_t\), where \(w_t = y \land p_t = g[y]\)), then \(s\) dominates \(t\). New solutions created from \(t\) are guaranteed to be dominated by the solutions created from \(s\)\footnote{The proof is trivial. For any item \(i\) the \(s \cap \{i\}\) solution will dominate the \(t \cap \{i\}\) solution.}. A whole set of solutions that have \(t\) as subset could be discarded.

The change from \(n\) to \(d[y]\) is inherited from the algorithm we based ukp5. It's a generic optimization trick that can be used at any problem where the solution is a set (as opposed to a sequence). If the item multiset \(\{1, 1, 2, 3\}\) is a solution, then every permutation of it is reached in a different moment, wasting processing\footnote{The algorithm would computate \(\{1, 2, 3\} \cap \{1\}\) and \(\{1, 1, 2\} \cap \{3\}\), for example.}. To avoid computing every solution in many different orders, we enforce decreasing order. Any item inserted on a solution \(s\) has to have an index that is equal to or lower than the index of the last item inserted on \(s\). 

When the two changes are combined, and the items are ordered by non-decreasing efficiency, ukp5 gets a big performance improvement. The non-dominated solutions (solutions that weren't skipped) are efficient (they have to be to not be dominated), consequently they need to be made of efficient items, that will have the lowest index values, and reduce the processing done at ukp5 inner loop greatly.

The third loop (lines \ref{get_y_opt_loop_begin} to \ref{get_y_opt_loop_end}) gets the optimal solution value and minimal weight. Any non-excluded optimal solution\footnote{If an optimal solution has an bigger weight than another it can be never generated, as consequence of the test at lines \ref{if_less_than_opt_begin} to \ref{if_less_than_opt_end}. The optimal solution with the lowest weight is guaranteed to be generated.} is guaranteed to be between \(c - w_{min} + 1\) and \(c\) (both inclusive). The proof is simple. A valid solution can't weight more than \(c\), and for any solution \(s\) that weights less than \(c - w_{min} + 1\), we can obtain a solution with a bigger profit inserting a copy of \(i\) to \(s\), where \(w_i = w_min\).

\subsection{Solution Dominance}

In this section we give a closer look at how the solution dominance speeds-up ukp5. We will use the \(min_{ix}(s)\) notation to refer to the lowest index between the items that compose the \(s\) solution\footnote{Note that, on ukp5, the lowest index of an item in a solution is also the index of the last item inserted in the solution (and consequently the value stored at \(d[y]\) where \(y\) is the solution weight).}. The \(max_{ix}(s)\) notation has an analogue meaning.

When a solution \(t\) is skipped because \(s\) dominates \(t\) (lines \ref{if_less_than_opt_begin} to \ref{if_less_than_opt_end}), some solutions \(u\), where \(t \subset u\), will not be generated. If \(s~dominates~t \land t \subset u \land max_{ix}(u - t) \leq min_{ix}(t)\) then \(u\) will not be generated by ukp5. In other words, if \(\{3, 2\}\) is dominated, then \(\{3, 2, 2\}\), \(\{3, 2, 1\}\) and any solution \(\{3, 2\} \cap u\), where \(u\) has only indexes lower than or equal to \(2\) will not be generated. Ideally, any \(u\) where \(t \subset u\) should not be generated as it will be dominated by a solution \(v\) where \(s \subset v\) anyway. It's interesting to note that this will happen eventually, as any \(t \cap {i}\) where \(i > min_{ix}(t)\) will be dominated by \(s \cap \{i\}\) (or by a solution that dominates \(s \cap \{i\}\)), and at some point no solution that is a superset of \(t\) will be generated anymore.

\subsection{Non Essential Steps}

With the purpose of making the initial explanation simpler, we have ommited some steps that are relevant to the algorithm performance, but not essential for accessing its correctness and core idea. A complete overview of the ommited steps is presented at this section.

All the items are sorted by non-decreasing efficiency\footnote{If they have the same efficiency they are sorted by increasing weight.}. The simple/multiple, collective or threshold dominances aren't used by UKP5, as this is often counterproductive for hard instances\footnote{Where the undominated-to-all-items ratio is very close to one.} and superseeded by our implicit solution dominance. The \(y*\) periodicity bound is computed\cite[p. 223]{CGAR}, and used to reduce the \(c\) value, if possible.

An ukp5-specific periodicity check is also used. This periodicity check isn't used to reduce the \(c\) capacity before starting ukp5, as \(y*\). The periodicity check is a stop condition inside ukp5 main loop (LINES). The stop condition follows: let \(y\) be the value of the variable \(y\) at line \ref{main_ext_loop_begin}; let \(y'\) be the biggest capacity where \(g[y'] \neq 0 \land d[y'] > 1\); if, at some moment, \(y > y'\), then we can stop the computation and fill the remaining capacity with copies of the first item (item of index \(1\)).%In other words, if all solutions \(s\) with \(w_s > y\) contain at least one copy of the lowest indexed item (item of index \(1\)), then the only item we can add to them is the lowest indexed item (the inner loop at \ref{main_inner_loop_begin} iterates only until \(d[y]\), that will be \(1\)).

%if for all \(y' > y\) (where \(y\) is the loop control variable defined at line \ref{main_inner_loop_begin}) any \(g[y'] \neq 0\) has \(d[y'] = 0\), then we can stop at \(y\) as we know that any remaining capacity will be filled by copies of the item of lowest index (item of index \(0\)).

This periodicity check works only if the best item is the first item. If this assumption is false, then the described condition will never happen\footnote{The only item that can dominate all other items by threshold is the best item.}, and the algorithm will iterate until \(y = c - w_{min}\) as usual.

There's and \emph{else if} construct at line \ref{if_new_lower_bound_end}. If \(g[y + w_i] = g[y] + p_i \land i < d[y + w_i]\) then \(d[y] \gets i\). This may seem unecessary, as appears to be an optimization of a very rare case, where two solutions have the same weight and profit. Nonetheless, without this test, the ukp5 was about 1800 (one thousand and eight hundreds) times slower on the subset-sum instances presented at CTABLE1.

\section{Experiments}
% Describe all instance datasets.
% Describe the environment (computer, isolated cores, disabled hyperthreading)
% Only used internal time, internal time of PYAsUKP was strange
% external time not used because HD data reading race conditions, and different reading methods
% TABLE WITH RESULTS
In this section we describe the experiments environment, instance sets and results. We compare our ukp5 implementation and the eduk2 implementation made by \cite{CPYA} (called PYAsUKP). The exact versions of the source codes used can be found at \url{https://github.com/henriquebecker91/masters/tree/v0.1}\footnote{The ukp5 implementation is at \textbf{codes/cpp/} and the two eduk2 implementations are at \textbf{codes/ocaml/}. The \emph{pyasukp\_site.tgz} is the version used to generate the instances, and was also available at \url{http://download.gna.org/pyasukp/pyasukpsrc.html} by this paper writing date. The site version had bugs on the eduk2 UKP solver, so a version without those bugs was obtained thanks to Vincent Poirriez. This version is called \emph{pyasukp\_mail.tgz} and was the one used to solve the instances (i.e. make the CTABLE1). The \emph{create\_*\_instances.sh} scripts insides \textbf{codes/sh/} were used to generate the instance datasets.}. The times reported were given by the tools themselves and are supposed to not count the instance loading time (i.e. reading the instance from disk to memory). The runs external time\footnote{Given by the \textit{time} application, available at \url{https://www.archlinux.org/packages/extra/x86_64/time/}. The bash internal command was \emph{not} used.} were also captured and no significative discrepancy was perceived. Therefore, we have chosen to use the tools reported times (as is the common practice). For all instances, the weight, profit and capacity are integral.

We use the following non-standard notations at this section: \(rand(x, y)\) means a random integer between \(x\) and \(y\) (both inclusive)\footnote{We could not determine if the methods used by pyasukp to generate random integer sets results in an uniform distribution. We advice checking the source code for more information on the exact method used for obtaining random integer sets.}; \(x\overline{n}\) means \(x\) as digits followed by the value of variable \(n\) as digits, for example: if \(n = 5000\) then \(10\overline{n} = 105000\).

\subsection{Environment}

The computer used on the experiments was an ASUS R552JK-CN159H. This model has four physical cores\footnote{Intel Core i7-4700HQ Processor, 6M Cache, up to 3.40 GHz.}. The operating system used was Linux nymeria 4.3.3-2-ARCH x86\_64 GNU/Linux (i.e. Arch linux). Three of the four cores were isolated using the \emph{isolcpus} kernel flag. The \emph{taskset} utility was used to execute ukp5 and pyasukp on the isolated cores. The computer memory was never completely used (no swapping was done). The ukp5 code was compiled with gcc (g++) version 5.3.0 and the \emph{-O3 -std=c++11} flags enabled.

\subsection{Instance Sets}

The instance sets aim to reproduce the ones described in \cite{CPYA}. The same tool was used to generate the datasets (pyasukp), and the same parameters were used, otherwise noted the contrary. In \cite{CPYA}, section 5.1.1 \emph{Known ``hard'' instances}, some sets of easy instances are used to allow comparison with MTU2 (that had integer overflow problems with harder instances). With exception of the subset-sum dataset, all datasets have a similar harder set (\cite{CPYA}, section 5.2.1 \emph{New hard UKP instances}), so we ommited the easy sets and used only the harder ones. Each instance has a random capacity value within an interval, this interval is shown at CTABLE1. The pyasukp parameters \emph{-wmin \(w_{min}\) -cap c -n \textbf{n}} were used in all instances generation. When we found a discrepancy between the formula presented in \cite{CPYA} and the pyasukp code, or generated instances, we opted for changing the formula based on the observed behaviour\footnote{As the pyasukp code can be hard to follow, we cannot guarantee that the formula presented here is a perfect match for the code; but, based by the generated instances, we believe it to be correct to a good extent.}.

\subsubsection{Subset-Sum}\label{sec:subsetsum}
Instances where \(p_i = w_i = rand(w_{min}, w_{max})\). The ones used at \cite{CPYA} were so easily solved that we had measuring problems (many instances solved on less than 0.01 seconds). Because of this, in this paper, we use a dataset similar to the one used at \cite{CPYA}, but with each parameter multiplied by ten. Therefore, we generated 10 instances for each possible combination of: \(w_{min} \in \{10^3, 5\times10^3, 10^4, 5\times10^4, 10^5\}\), \(w_{max} \in \{5\times10^5, 10^6\}\) and \(n \in \{10^3, 2\times10^3, 5\times10^3, 10^4\}\). Totalizing 400 instances. We do not discriminate each combination at CTABLE1 for brevity. The pyasukp \emph{-form ss -wmax \(w_{max}\)} parameters were used.

\subsubsection{Strong Correlation}
Instances generated using the following formula: \(w_i = w_{min} + i - 1\) and \(p_i = w_i + \alpha\), for a given \(w_{min}\) and \(\alpha\).  Note that, except by the random capacity, all instances with the same \(\alpha\), \(\mathbf{n}\) and \(w_{min}\) combination are equal. The formula don't rely on random numbers. The pyasukp \emph{-form chung -step \(\alpha\) } parameters were used.

\subsubsection{Postponed Periodicity}
This family of instances is generated by the following method: \textbf{n} distinct weights are generated with \(rand(w_{min}, w_{max})\) and then sorted by increasing order; \(p_1 = w_1 + rand(1, 500)\); and \(\forall i \in [2, n].~p_i = p_{i-1} + rand(1, 125)\). The pyasukp \emph{-form nsds2 -step 500 -wmax \(w_{max}\)} parameters were used.

\subsubsection{No Collective Dominance}
This family of instances is generated by the following method: \textbf{n} distinct weights are generated with \(rand(w_{min}, w_{max})\) and then sorted by increasing order; \(p_1 = p_{min} + rand(0, 49)\); and \(\forall i \in [2, n].~p_i = \lfloor w_i \times ((p_{i-1}/w_{i-1}) + 0.01)\rfloor + rand(1, 10)\). The given values are: \(w_{min} = p_{min} = \mathbf{n}\) and \(w_{max} = 10\overline{n}\). The pyasukp \emph{-form hi -pmin \(p_{min}\) -wmax \(w_{max}\)} parameters were used.

\subsubsection{SAW}
This family of instances is generated by the following method: generate \textbf{n} random weights between \(w_{min}\) and \(w_{max} = 1\overline{n}\), sorted by increasing order, with the following property: \(\forall i \in [2, n].~w_i~mod~w_1 > 0\) (\(w_1\) is the smallest weight); then \(p_1 = w_1 + \alpha\) where \(\alpha = rand(1,5)\), and \(\forall i \in [2, n].~p_i = rand(l_i, u_i)\) where \(l_i = max(p_{i-1}, q_i)\), \(u_i = q_i + m_i\), \(q_i = p_1 \times \lfloor w_i / w_1 \rfloor \), and \(m_i = w_i~mod~w_1\). The pyasukp \emph{-form saw -step \(\alpha\) -wmax \(w_{max}\)} parameters were used.

\subsection{Results and Analysis}

Based on CTABLE1, except by one instance set that we will talk about later, we can make two statements: 1) the average time, standard deviation, and maximal time of ukp5 are always smaller than the pyasukp ones; 2) the minimal pyasukp time is always smaller than the ukp5 one.

Let's begin with the second statement (about the minimal), as eduk2 uses a branch-and-bound (B\&B) algorithm before resorting to dynamic programming (DP), this is an expected result. Instances with big capacities and solutions that are composed by a large quantity of the best item, and a few of the non-best most efficient items, can be solved by B\&B very fast, in a way that no dynamic programming algorithm can be competitive against. Our exception dataset (Strong Correlation, \(\alpha = 5\), \(n = 10\) and \(w_{min} = 10\)) is exactly the case. As said before, the strong correlation formula don't make use of random numbers, so all twenty instances of that dataset have the same items. The only thing that changes is the capacity. All solutions of this dataset are composed by hundreds of the best item (that is also the smallest item, making the dataset even easier) and exactly one non-best item for making better use of the residual capacity (\(c~mod~w_1\)). All other datasets have instances that present the same characteristics\footnote{The number of instances where pyasukp was faster than ukp5 by formula are: Subset-sum: 264 (\~65\%); Strong correlation: 60 (25\%); Postponed periodicity: 105 (\~13\%); No collective dominance: 259 (\~13\%); SAW: 219 (\~20\%).}, and because of that, the pyasukp minimal is always very low\footnote{The observant reader will see that, on the dataset ``Postponed periodicity'', \(n = 50\), \(w_{min} = 50\), pyasukp has a anomalous minimal time. On this dataset, the solution of the easiest instance is composed only by two items, the best one, and the 5317 most efficient one.}.

% Put the importance of solving hard instances as they can have exponential time

As it's already well known, DP is not competitive against a B\&B approach for easy UKP instances. The ukp5 can't compete with pyasukp on easy datasets, as only the time for initializing an array of size \(c\) is already greater than the B\&B time\footnote{A problem that will be tackled in newer versions by using heaps or array slices.}. Nonetheless, for hard instances, B\&B is known to shows a very bad worst case (exponencial time). As eduk2 combine B\&B and DP with the intent of getting the strenghts of both, and none of its weaknesses, we found strange that this typical B\&B behavior is present at pyasukp. We executed pyasukp with the \emph{-nobb} flag, that disables the use of B\&B. The results show that any time under XXX seconds disappear, now taking at least XXX seconds, and times over this limit stay the same. Based on this evidence, we conclude that the pyasukp implementation of the eduk2 DP phase is responsible for the big maximal pyasukp times (the time seems exponential but it is instead pseudo-polynomial with a big constant). For future research, it is interesting to verify if an implementation of ukp5 that executed B\&B for a small amount of time before resorting to DP could have better results than pyasukp, without the complexity that is integrating B\&B with DP in the way is done by eduk2.

%Based on the first statement, and the discovery that the eduk2 DP phase was responsible for the 
Looking back at the first statement of this section, we can now conclude that for instances that are hard for B\&B, ukp5 clearly surpass the DP solution by a big constant factor. Even considering the instances that pyasukp solves almost instantly (because of B\&B), ukp5 is about thirty times faster than pyasukp, in average. If we disconsidered the advantage given by B\&B (giving ukp5 a B\&B phase, or removing the one used on eduk2) this gap would be even bigger.

Our ukp5 implementation consumed, in average, more memory than pyasukp\footnote{This is another problem that will be tackled in newer versions by the use of heaps or array slices.}. The complete data can be found at CTABLE2 at the Appendix. Note that the ukp5 memory use is linear on \(c + w_{max} - w_{min}\). It's easy to see the similarity between the average of instances with a similar capacity interval. The pyasukp memory use is too bound by \(c\) (or the slice size), but because the way it handles sparsity it's, in practice, much lower than this bound (even if its structures, i.e. lazy lists, have a much bigger memory overhead than arrays).

\section{Conclusions}

\section{Appendix}
\begin{centering}
\begin{algorithm}
\caption{First Phase -- Computation of $opt$ and $y_{opt}$}\label{alg:ukp5}
\begin{algorithmic}[1]
\Procedure{UKP5}{$n, c, w, p, w_{min}, w_{max}$}
  \State \(g \gets\) array of \((c - w_{min}) + w_{max} + 1\) positions each one initialized with \(0\)\label{create_g}
  \State \(d \gets\) array of \((c - w_{min}) + w_{max} + 1\) positions each one initialized with \(n\)\label{create_d}
  
  \For{\(i \gets 1, n\)}\label{begin_trivial_bounds}
    \If{\(g[w_i] < p_i\)}
      \State \(g[w_i] \gets p_i\)
      \State \(d[w_i] \gets i\)
    \EndIf
  \EndFor\label{end_trivial_bounds}

  \State \(opt \gets 0\)\label{init_opt}

  \For{\(y \gets w_{min}, c-w_{min}\)}\label{main_ext_loop_begin}
    \If{\(g[y] \leq opt\)}\label{if_less_than_opt_begin}
    	\State \textbf{continue}\label{alg:continue}
    \EndIf\label{if_less_than_opt_end}
    
    \State \(opt \gets g[y]\)\label{update_opt}
    
    \For{\(i=1,d[y]\)}\label{main_inner_loop_begin}
      \If{\(g[y + w_i] < g[y] + p_i\)}\label{if_new_lower_bound_begin}
        \State \(g[y + w_i] \gets g[y] + p_i\)
        \State \(d[y + w_i] \gets i\)
%      \ElsIf{\(g[y + w_i] = g[y] + p_i \land i < d[y + w_i]\)}
%        \State \(d[y + w_i] \gets i\)
      \EndIf\label{if_new_lower_bound_end}
    \EndFor\label{main_inner_loop_end}
  \EndFor\label{main_ext_loop_end}

  \For{\(y \gets c-w_{min}+1, c\)}\label{get_y_opt_loop_begin}
    \If{\(g[y] > opt\)}\label{last_loop_inner_if}
      \State \(opt \gets g[y]\)
      \State \(y_{opt} \gets y\)
    \EndIf
  \EndFor\label{get_y_opt_loop_end}
\EndProcedure
\end{algorithmic}
\end{algorithm}
\end{centering}



\begin{centering}
\begin{table}
\caption{Columns \textbf{n} and \(w_{min}\) values must be multiplied by \(10^3\) to obtain their true value. Let \(T\) be the set of times reported by ukp5 or eduk2, then the meaning of the columns \textbf{avg}, \textbf{sd}, \textbf{min} and \textbf{max}, is, respectively, the arithmetic mean of \(T\), the standard deviation of \(T\), the minimal value of \(T\) and the maximal value of \(T\). The time unit of the table values is seconds.}
\def\arraystretch{1.1}
\setlength\tabcolsep{4px}

\begin{tabular}{@{\extracolsep{4pt}}rrrrrrrrrrr@{}}

\hline
\multicolumn{3}{l}{Instance desc.} & \multicolumn{4}{l}{UKP5} & \multicolumn{4}{l}{PYAsUKP}\\
\cline{1-3}\cline{4-7}\cline{8-11}

\multicolumn{3}{l}{400 inst. per line} & \multicolumn{8}{l}{Subset-sum. Random \emph{c} between \([5\times10^6; 10^7]\)}\\
\cline{1-3}\cline{4-11}

& \textbf{n} & \(w_{min}\)  & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}

\multicolumn{3}{c}{See section~\ref{sec:subsetsum}} & 0.08 & 0.20 & 0.01 & 1.42 & 6.39 & 55.33 & 0.00 & 726.34\\
\hline

\multicolumn{3}{l}{20 inst. per line} & \multicolumn{8}{l}{Strong correlation. Random \emph{c} between \([20\overline{n}; 100\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
\textbf{\(\alpha\)} & \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
 5 & 5  & 10 & 0.05 & 0.00 & 0.05 & 0.05 & 2.46 & 2.81 & 0.00 & 6.13\\
   &    & 15 & 0.07 & 0.00 & 0.07 & 0.09 & 5.84 & 2.43 & 0.00 & 8.82\\
   &    & 50 & 0.20 & 0.06 & 0.08 & 0.24 & 18.35 & 12.64 & 0.00 & 50.58\\
 5 & 10 & 10 & 0.11 & 0.01 & 0.10 & 0.14 & 0.00 & 0.00 & 0.00 & 0.01\\
   &    & 50 & 0.49 & 0.03 & 0.47 & 0.60 & 41.97 & 33.97 & 0.00 & 93.18\\
   &    & 110 & 1.07 & 0.02 & 1.05 & 1.13 & 147.60 & 114.39 & 0.00 & 342.86\\
-5 & 5  & 10 & 0.06 & 0.00 & 0.06 & 0.07 & 5.98 & 4.02 & 0.00 & 11.99\\
   &    & 15 & 0.09 & 0.00 & 0.08 & 0.10 & 10.37 & 6.73 & 0.00 & 21.00\\
   &    & 50 & 0.21 & 0.05 & 0.09 & 0.24 & 39.31 & 30.16 & 0.00 & 89.44\\
-5 & 10 & 10 & 0.19 & 0.01 & 0.17 & 0.21 & 13.13 & 12.61 & 0.00 & 33.00\\
   &    & 50 & 0.54 & 0.02 & 0.52 & 0.59 & 82.97 & 71.22 & 0.00 & 206.74\\
   &    & 110& 1.08 & 0.02 & 1.07 & 1.13 & 261.61 & 246.21 & 0.00 & 721.89\\
\hline

\multicolumn{3}{l}{200 inst. per line} & \multicolumn{8}{l}{Postponed periodicity. Random \emph{c} between \([w_{max}; 2\times10^6]\)}\\
\cline{1-3}\cline{4-11}
& \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
& 20 & 20 & 1.42 & 0.31 & 0.55 & 2.77 & 17.00 & 17.05 & 0.01 & 63.96\\
& 50 & 20 & 10.20 & 1.28 & 7.91 & 14.98 & 208.61 & 210.72 & 0.03 & 828.89\\
& 20 & 50 & 1.59 & 0.32 & 0.96 & 2.99 & 27.68 & 22.79 & 0.02 & 100.96\\
& 50 & 50 & 6.86 & 1.23 & 4.46 & 11.78 & 233.58 & 187.91 & 2.65 & 682.95\\
\hline

\multicolumn{3}{l}{500 inst. per line} & \multicolumn{8}{l}{No collective dominance. Random \emph{c} between \([w_{max}; 1000\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
& \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
&  5 & n & 0.05 & 0.01 & 0.03 & 0.10 & 0.78 & 0.59 & 0.00 & 2.66\\
& 10 & n & 0.49 & 0.15 & 0.21 & 1.10 & 3.38 & 2.80 & 0.00 & 12.31\\
& 20 & n & 0.99 & 0.19 & 0.63 & 2.02 & 13.08 & 12.80 & 0.01 & 62.12\\
& 50 & n & 4.69 & 1.22 & 3.51 & 13.18 & 119.18 & 131.22 & 0.04 & 667.42\\
\hline

\multicolumn{3}{l}{\emph{qtd} inst. per line} & \multicolumn{8}{l}{SAW. Random \emph{c} between \([w_{max}; 10\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
\textbf{qtd} & \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
~200 &  10 & 10 & 0.11 & 0.01 & 0.10 & 0.16 & 1.88 & 1.24 & 0.01 & 4.73\\
~500 &  50 &  5 & 0.74 & 0.08 & 0.66 & 1.98 & 4.79 & 4.22 & 0.02 & 17.78\\
~200 &  50 & 10 & 1.01 & 0.03 & 0.97 & 1.27 & 10.44 & 9.02 & 0.03 & 38.69\\
~200 & 100 & 10 & 14.13 & 2.96 & 9.95 & 21.94 & 60.58 & 54.08 & 0.05 & 192.04\\
\hline

\end{tabular}
\end{table}
\end{centering}



\begin{table}
\caption{Columns \textbf{n} and \(w_{min}\) values must be multiplied by \(10^3\) to obtain their true value. Let \(T\) be the maximal memory used by ukp5 or eduk2 as reported by the \emph{time} tool, then the meaning of the columns \textbf{avg}, \textbf{sd}, \textbf{min} and \textbf{max}, is, respectively, the arithmetic mean of \(T\), the standard deviation of \(T\), the minimal value of \(T\) and the maximal value of \(T\). The time unit of the table values is Mb (megabytes, not mibibytes).}
\def\arraystretch{1.1}
\setlength\tabcolsep{4px}

\begin{tabular}{@{\extracolsep{4pt}}rrrrrrrrrrr@{}}

\hline
\multicolumn{3}{l}{Instance desc.} & \multicolumn{4}{l}{UKP5} & \multicolumn{4}{l}{PYAsUKP}\\
\cline{1-3}\cline{4-7}\cline{8-11}

\multicolumn{3}{l}{400 inst. per line} & \multicolumn{8}{l}{Subset-sum. Random \emph{c} between \([5\times10^6; 10^7]\)}\\
\cline{1-3}\cline{4-11}

& \textbf{n} & \(w_{min}\)  & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}

\multicolumn{3}{c}{See section~\ref{sec:subsetsum}} & 133.96 & 22.85 & 89.85 & 175.67 & 13.28 & 6.71 & 6.28 & 53.28\\

\hline

\multicolumn{3}{l}{20 inst. per line} & \multicolumn{8}{l}{Strong correlation. Random \emph{c} between \([20\overline{n}; 100\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
\textbf{\(\alpha\)} & \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
 5 & 5  & 10 & 12.82 & 3.78 & 7.30 & 18.59 & 9.01 & 3.24 & 5.81 & 12.51\\
   &    & 15 & 12.38 & 3.42 & 7.70 & 19.40 & 12.93 & 2.68 & 5.86 & 14.99\\
   &    & 50 & 13.61 & 3.30 & 7.91 & 18.26 & 20.01 & 6.66 & 7.60 & 30.62\\
 5 & 10 & 10 & 105.57 & 36.90 & 36.13 & 153.65 & 7.13 & 0.82 & 6.25 & 8.40\\
   &    & 50 & 106.10 & 39.68 & 36.54 & 159.28 & 21.05 & 10.53 & 6.28 & 33.97\\
   &    & 110 & 103.18 & 34.99 & 37.68 & 149.38 & 41.56 & 22.93 & 6.35 & 78.19\\
-5 & 5  & 10 & 13.06 & 4.27 & 6.88 & 18.99 & 12.81 & 3.67 & 5.85 & 16.23\\
   &    & 15 & 13.81 & 3.43 & 8.56 & 19.30 & 14.33 & 3.81 & 5.85 & 17.92\\
   &    & 50 & 14.61 & 3.22 & 8.33 & 18.76 & 23.17 & 10.39 & 5.86 & 42.08\\
-5 & 10 & 10 & 109.12 & 36.36 & 37.02 & 159.60 & 13.91 & 4.37 & 6.28 & 18.51\\
   &    & 50 & 115.43 & 38.81 & 36.23 & 160.05 & 26.97 & 13.64 & 6.35 & 45.19\\
   &    & 110& 97.20 & 33.97 & 39.73 & 151.94 & 49.22 & 29.02 & 7.94 & 98.78\\
\hline

\multicolumn{3}{l}{200 inst. per line} & \multicolumn{8}{l}{Postponed periodicity. Random \emph{c} between \([w_{max}; 2\times10^6]\)}\\
\cline{1-3}\cline{4-11}
& \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
%42.80 & 3.98 & 35.65 & 50.54 & 16.12 & 4.70 & 7.84 & 25.20\\
\cline{1-3}\cline{4-7}\cline{8-11}
& 20 & 20 & 42.80 & 3.98 & 35.65 & 50.54 & 16.12 & 4.70 & 7.84 & 25.20\\
& 50 & 20 & 44.76 & 4.12 & 36.82 & 51.72 & 33.60 & 14.51 & 12.12 & 65.98\\
& 20 & 50 & 42.28 & 4.53 & 34.92 & 50.24 & 18.37 & 4.88 & 8.04 & 29.83\\
& 50 & 50 & 44.34 & 4.48 & 36.67 & 51.39 & 39.05 & 13.15 & 14.45 & 63.64\\

\hline

\multicolumn{3}{l}{500 inst. per line} & \multicolumn{8}{l}{No collective dominance. Random \emph{c} between \([w_{max}; 1000\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
& \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
&  5 & n & 83.39 & 44.10 & 7.54 & 161.35 & 12.44 & 5.00 & 5.91 & 36.65\\
& 10 & n & 826.66 & 440.19 & 35.86 & 1580.66 & 23.54 & 16.45 & 7.84 & 136.56\\
& 20 & n & 810.90 & 433.39 & 37.86 & 1578.22 & 46.89 & 34.39 & 8.24 & 205.98\\
& 50 & n & 798.77 & 451.91 & 40.37 & 1582.24 & 120.17 & 87.97 & 13.19 & 569.85\\

\hline

\multicolumn{3}{l}{\emph{qtd} inst. per line} & \multicolumn{8}{l}{SAW. Random \emph{c} between \([w_{max}; 10\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
\textbf{qtd} & \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
~200 &  10 & 10 & 14.02 & 4.08 & 6.93 & 20.79 & 14.58 & 4.41 & 6.68 & 23.58\\
~500 &  50 &  5 & 15.89 & 3.92 & 8.72 & 23.16 & 17.39 & 3.55 & 11.96 & 27.54\\
~200 &  50 & 10 & 15.99 & 3.98 & 8.62 & 22.86 & 18.72 & 4.05 & 12.26 & 29.46\\
~200 & 100 & 10 & 111.93 & 42.44 & 39.58 & 179.31 & 60.98 & 33.91 & 20.85 & 140.52\\
\hline

\end{tabular}
\end{table}

\bibliography{ukp5.bib}

\end{document}
