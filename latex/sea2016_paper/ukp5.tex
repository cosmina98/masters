% Aumentar a introdução antes da notação formal do problema

\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
% ADD THOSE LIBS AT THE PACKAGE FOR THE JOURNAL
\usepackage{mathtools}
\usepackage{verbatim}

\setcounter{tocdepth}{3}
\usepackage{graphicx}

% ADD THOSE LIBS AT THE PACKAGE FOR THE JOURNAL
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{url}
\urldef{\mail}\path|{hbecker, buriol}@inf.ufrgs.br|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{UKP5: a new algorithm for the unbounded knapsack problem}

% a short form should be given in case it is too long for the running head
\titlerunning{UKP5: a new algorithm for the UKP}

\author{Henrique Becker \and Luciana Salete Buriol}
% (feature abused for this document to repeat the title also on left hand pages)
\authorrunning{UKP5: a new algorithm for the UKP}

\institute{Federal University of Rio Grande do Sul,\\
Paulo Gama. 110, 90040-060, Porto Alegre, RS, Brazil\\
\mail\\
\url{http://ppgc.inf.ufrgs.br/}}

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}
\maketitle

\begin{abstract}
In this paper we present UKP5, a novel algorithm for solving the unbounded knapsack problem. UKP5 is based on dynamic programming, but implemented in a non traditional way: instead of looking backwards for stored values of subproblems, it stores incremental lower bounds forward. UKP5 uses sparsity, periodicity, and dominance for speeding up computation. UKP5 is considerably simpler than EDUK2, the state-of-the-art algorithm for solving the problem. Moreover, it can be naturally implemented using the imperative paradigma, differently from EDUK2. We run UKP5 and EDUK2 on a benchmark of hard instances proposed by the authors of EDUK2. The benchmark is composed by 4540 instances, divided into five classes, with instances ranging from small to large inside each class. Speedups were calculated for each class, and the overall speedup was calculated as the classes speedups average. The experimental results reveal that UKP5 outperforms EDUK2, being 47 times faster on the overall average.
\keywords{unbounded knapsack problem, optimization, dynamic programming, NP-hard}
\end{abstract}

\section{Introduction}

The unbounded knapsack problem (UKP) is a variation of the well-known bounded knapsack problem (BKP), where there's an unbounded quantity of each item type available.

% \footnote{The order of the items doesn't change the optimal solution value. Nonetheless, the order of the items is relevant for some algorithms performance, and we will refer to items by its position in the item list. For these reasons we will refer to the input items as a list, and not as a set.}
The following formal notation of the UKP will be used for the rest of the article: an UKP instance is composed of a capacity \(c\), and a list of \(n\) items; each item can be referenced by its index in the item list \(i \in \{1\dots n\}\); each item \(i\) has a weight value, denoted by \(w_i\), and a profit value, denoted by \(p_i\); a solution is an item multiset\footnote{A multiset is a set that allows multiple copies of the same element.}; the sum of the items weight or profit of a solution \(s\) will be denoted by \(w_s\) and \(p_s\), repectively; a valid solution is solution \(s\) where \(w_s \leq c\); an optimal solution \(s*\) is a valid solution with the greatest profit between all valid solutions; we will refer to the profit value shared between all optimal solutions for a capacity \(y\) as \(opt(y)\), if we ommit the capacity then \(c\) is implied; the UKP objective is to find an optimal solution for the given UKP instance. In linear programming notation, we could write:

\begin{align}
  maximize: &\sum_{i=1}^n p_i x_i\label{eq:objfun}\\
subject~to: &\sum_{i=1}^n w_i x_i \leq c\label{eq:capcons}\\
            &x_i \in \mathbb{N}_0\label{eq:x_integer}
\end{align}

%\footnote{If the quantities weren't restricted to the integers the problem wouldn't be NP-Hard.}
In this article, we assume that the capacity \(c\), the quantity of items \(n\) and the weights of the items \(w_i\) are all positive integers. The quantities of each item \(i\) in an optimal solution are denoted by \(x_i\), and are restricted to the non-negative integers, as shown in \eqref{eq:x_integer}. The profits of the items \(p_i\) are all positive real numbers.

%\footnote{If the efficiencies tie the best item is the item with the lowest weight between the tied items.}
The efficiency of an item \(i\) is the value of \(\frac{p_i}{w_i}\), and will be denoted as \(e_i\). We use \(w_{min}\) and \(w_{max}\) to denote the smallest item weight and the biggest item weight, respectively. Also, we refer to the item with the lowest weight between the ones with the greatest efficiency as the \emph{best item}, and the item with the lowest weight (between all items) as the \emph{smallest item}. If two or more items have the same weight we consider only the one with the best profit (the other can be ignored without loss to the optimal solution value), if they have the same weight and profit we consider them the same item.

\subsection{Bibliographic Research, Motivation and Contribution}

The UKP it's NP-Hard, so it's always positive to have efficient algorithms for solving it. It arises in real world problems mainly as a subproblem of the Bin Packing Problems (BPP) and Cutting Stock Problem (CSP). Both BPP and CSP are similar and of importance for the industry \cite{survey2014}\cite{gg-1}\cite{gg-2}. The currently fastest known solver for BPP/CSP\cite{survey2014}\cite{belov} uses a technique (introduced in \cite{gg-1}) that needs to solve an instance of the UKP at each iteration. The need for efficient algorithms that solve UKP is clear.

Two techniques are often used for solving UKP: Dynamic Programming (DP) \cite{gar72}\cite{CEDUK}\cite{CTCHU} and Branch and Bound (B\&B) \cite{CMTU2}. The DP approach have a stable pseudo-polynomial time \(O(c n)\). The B\&B approach is less stable. It can be faster than DP on instances with some characteristics, as the rest of division between the weight of the best item and the capacity being small; or the items having a big efficiency variance. Nonetheless, B\&B has always the risk of an exponential time worst case.

The state of art solver on UKP, introduced on \cite{CPYA}, is a hybrid solver that combines both approaches. It tries solving the problem by B\&B, and if this aproach fails to solve the problem in little time, it switches to DP using some data gathered by B\&B to speed-up the process. The solver's name is PYAsUKP, and it's an implementation of the EDUK2 algorithm.%We have found that the DP used by PYAsUKP is very slow when compared to our algorithm. Even solving some instances in less than a centisecond using B\&B, the solver is yet about thirty times slower than our method, in average. Also, our algorithm is much less complex, and can be easily implemented on a imperative language. The periodicity stop condition, and the solution dominance concept, used on ukp5 are algorithm specific. So we don't count them as individual contributions.

The recent paper \cite{CMUL} proposes, implements and compares two parallelization approaches for classical DP formulations of the UKP. It doesn't compare with the state-of-the-art but, regarding to PYAsUKP, states that ``The algorithm (PYAsUKP) significantly outperforms all existing algorithms for solving the problem.''.

% AlgorithmsForKnapsackProblemsPsinger -- p. 148, cita 53 (Martello and Toth) to explain why do not transform unbounded knapsack instances in bounded (we could simply say that the lack of restrictions gives more opportunity for optimization) AND 
% Garfinkel, PYAsUKP

\subsection{Dominance}

Dominance, in the UKP context, is a set of techniques for reducing the size of the item list without affecting \(opt\). Clearly, in this conditions, every item that isn't used in an optimal solution could be discarded, but this would need the knowledge of the solution beforehand. This way, a dominance, or a dominance relation, was born as a property that can usually be verified in polynomial time over \(n\), and allows to exclude items without affecting \(opt\). It's easy to see that any polynomial-time algorithm that can be used to reduce the input of an exponential time algorithm is promising. Instances where many items can be excluded by the two simplest dominances (simple dominance and multiple dominance) became known as ``easy'' data instances. So much research about these two dominances was done, that, at the year of 1995, after enumerating the research done until the moment on UKP, Pisinger says ``Seen in this light, perhaps too much effort has been previously been used on the solution of easy data instances.'' \cite[p. 20]{CPISINGER}.

Other two important dominance relations are collective dominance and threshold dominance \cite{gar72} \cite{CPYA}. These two dominances are too expensive to be done at a preprocessing phase (as simple and multiple dominance). This way, they are integrated on the UKP solving method, and remove items while the algorithm executes. The collective dominance needs to known the \(opt(y)\) to exclude an item \(i\) with \(w_i = y\). The threshold dominance needs to know the \(opt(\alpha\times w_i)\) to exclude the item \(i\) from capacity \(y = \alpha\times w_i\) onwards, where \(\alpha\) is any positive integer.

The specifics of those four dominance relations are very well described at \cite{CPYA}. The UKP5 don't make use of any of those four dominances. It uses a ``solution dominance'' approach that is implicit in the algorithm and achives results similar to the four dominances. The decision to not use the known dominances is based on two main points: UKP5 was made to solve hard data instances, applying the dominance over hard data instances don't always pay off; our ``solution dominance'' already excludes items, so any benefits of the dominances would be diminushed, but the cost would be the same. For the rest of the article, when we say that an specific item is dominated, we are saying that it could be excluded without changing \(opt\).

% CITE ``'' p. 20, Pisinger

% In \cite{\cite{CPISINGER}}, a complete analysis of the simplest dominance type (simple/multiple dominace) is given. 

\subsection{Periodicity}

A periodicity bound \(y*\) is an upper capacity bound for the existence of optimal solutions without the best item. In another words, it's a guarantee that any solution for an instance where \(y* \leq c\) will have at least one copy of the best item. The periodicity bound is specially usefull because it can be applied repeatedly. For example, let \(c = 1000\), \(y* = 800\) and \(w_b = 25\) where \(b\) is the best item; because of \(y* \leq c\) we know that any optimal solution will have a copy of \(b\), so we can add one \(b\) to the solution and combine with an optimal solution for \(c = 975\); but \(975\) is yet bigger than \(800\), so we can repeat the process until \(c = 775\). This way, for any \(y* \leq c\) we can reduce the UKP instance capacity by \(max(1, \lceil(c-y*)/w_b\rceil)\times w_b\) and add to a reduced instance optimal solution \(max(1, \lceil(c-y*)/w_b\rceil)\) copies of \(b\).

There exists many proposed periodicity bounds, but some have undesired complexities over \(n\) (as \(O(n^2)\)\cite{CBADBOUND}), others depend on specific instance characteristics (as \cite{BADBOUND2}\cite{CPYA}), so we opted by using only a UKP5-specific periodicity bound described later and the \(y*\) bound described on \cite[p. 223]{gar72} (that is \(O(1)\) on an item list ordered by non-decreasing efficiency and is generic).

\subsection{Sparsity}

% Any weight difference between one solution and the next distinct solution can't be bigger than \(w_{min}\).
%The classic DP algorithm for UKP would compute the optimal solution value for every capacity and use them later to compute the optimal solution value for bigger capacities. This approach can become inneficient when the \(w_{min}\) becomes big. Lets suppose that we have an array of \(c\) positions, each with an optimal solution for that capacity value. As there can be many optimal solutions for the same capacity, we will use always the one with the lowest weight. If \(w_{min}\) is small, for example \(w_{min} = 1\), then we have the guarantee that every capacity has a distinct optimal solution. Otherwise, if \(w_{min}\) is big, for example \(w_{min} = 10^4\), it's possible to have ranges of hundreds or thousands of capacities with the same optimal solution. The classical DP algorithm would iterate this range and execute \(O(n)\) operations at each capacity, resulting in an optimal solution with the same profit value at each capacity. 

For some UKP instances, not every non-zero capacity value can be obtained by a linear combination of the items weight. If \(w_{min}\) is small, for example \(w_{min} = 1\), we have the guarantee that every non-zero capacity has at least one solution with weight equal to the capacity value (this is, while \(w_{min}\) isn't removed by dominance). But if \(w_{min}\) is big, for example \(w_{min} = 10^4\), there can be a large number of capacities where doesn't exist a solution with weight equal to the capacity. These capacities will have an optimal solution that don't fill the capacity completely. The UKP5 exploits sparsity in the sense that it avoids computing the optimal solution value for those unfulfilled capacities. When the algorithm ends, it searchs the range where the optimal solution value for \(c\) is guaranteed to be. The array that stores the optimal solutions value is, therefore, sparse.

\section{The proposed algorithm: UKP5}

The UKP5 is inspired by the DP algorithm described in \cite[p. 221]{gar72}. The name UKP5 comes from its five improvements over the \cite{gar72} DP algorithm: improved solution symmetry pruning performance (caused by the reversal of the solution generation direction); removal of the \(w_i \leq y\) test (substituted by the use of a bigger array); sparsity (no concept of ``solutions with remaining space''); pruning dominated solutions (this concept will be addressed in more detail later); an algorithm-specific periodicity stop condition (idea mentionated on \cite{gar72}, but not integrated on the algorithm). We do not intend to discuss every of those changes in detail, as this is outside the scope of this paper. We will discuss the algorithm born from all those changes, and focus on the most interesting of them.

\begin{algorithm}[!t]
\caption{UKP5 -- Computation of $opt$ and $y_{opt}$}\label{alg:ukp5}
\begin{algorithmic}[1]
\Procedure{UKP5}{$n, c, w, p, w_{min}, w_{max}$}
  \State \(g \gets\) array of \((c - w_{min}) + w_{max} + 1\) positions each one initialized with \(0\)\label{create_g}
  \State \(d \gets\) array of \((c - w_{min}) + w_{max} + 1\) positions each one initialized with \(n\)\label{create_d}
  
  \For{\(i \gets 1, n\)}\label{begin_trivial_bounds}
    \If{\(g[w_i] < p_i\)}
      \State \(g[w_i] \gets p_i\)
      \State \(d[w_i] \gets i\)
    \EndIf
  \EndFor\label{end_trivial_bounds}

  \State \(opt \gets 0\)\label{init_opt}

  \For{\(y \gets w_{min}, c-w_{min}\)}\label{main_ext_loop_begin}
    \If{\(g[y] \leq opt\)}\label{if_less_than_opt_begin}
    	\State \textbf{continue}\label{alg:continue}
    \EndIf\label{if_less_than_opt_end}
    
    \State \(opt \gets g[y]\)\label{update_opt}
    
    \For{\(i=1,d[y]\)}\label{main_inner_loop_begin}
      \If{\(g[y + w_i] < g[y] + p_i\)}\label{if_new_lower_bound_begin}
        \State \(g[y + w_i] \gets g[y] + p_i\)
        \State \(d[y + w_i] \gets i\)
%      \ElsIf{\(g[y + w_i] = g[y] + p_i \land i < d[y + w_i]\)}
%        \State \(d[y + w_i] \gets i\)
      \EndIf\label{if_new_lower_bound_end}
    \EndFor\label{main_inner_loop_end}
  \EndFor\label{main_ext_loop_end}

  \For{\(y \gets c-w_{min}+1, c\)}\label{get_y_opt_loop_begin}
    \If{\(g[y] > opt\)}\label{last_loop_inner_if}
      \State \(opt \gets g[y]\)
      \State \(y_{opt} \gets y\)
    \EndIf
  \EndFor\label{get_y_opt_loop_end}
\EndProcedure
\end{algorithmic}
\end{algorithm}

We use the \textbf{continue} keyword at line \ref{alg:continue} to mean that the loop ends its current iteration and begin the next iteration (as is common in C-like programming languages).

When the algorithm ends, the \(opt\) variable holds the optimal solution value, and \(y_{opt}\) holds the lowest weight of an optimal solution. We have two main data structures, the arrays \(g\) and \(d\). The \(g\) is a sparse array where we store solutions profit. If \(g[y] > 0\) then there exists a non-empty solution \(s\) with \(w_s = y\) and \(p_s = g[y]\). The \(d\) array stores the index of the last item used on the solution. If \(g[y] > 0 \land d[y] = i\) then the solution \(s\) with \(w_s = y\) and \(p_s = g[y]\) has an copy of item \(i\). This array makes trivial assemble the solution, but its main utility is to to drastically speed-up the optimal computation.
%\footnote{Note that \(j = d[y_{opt}]\) is the index of the last item used, \(d[y_{opt} - w_j]\) is the index of the penultimate item used, and so on, until \(d[0]\).}

Our first loop (lines \ref{begin_trivial_bounds} to \ref{end_trivial_bounds}) simply stores all solutions made of one item at the arrays \(g\) and \(d\). Now, for a moment, let's ignore lines \ref{if_less_than_opt_begin} to \ref{if_less_than_opt_end} and replace \(d[y]\) at line \ref{main_inner_loop_begin} by \(n\). After these changes, we could say that what our second loop (between lines \ref{main_ext_loop_begin} and \ref{main_ext_loop_end}) do is: iterate \(g\) and when it finds a stored solution (\(g[y] > 0\)) it tests \(n\) new solutions (the combinations of the current solution with every item); then it substite solutions already stored if exists a new solution with the same weight and a greater profit. This is basically what our algorithm do, but those two small code changes make two drastic performance changes.%, specially when combined. 

When we add the lines \ref{if_less_than_opt_begin} to \ref{if_less_than_opt_end} the algorithm will stop creating new solutions from dominated solutions. If a solution \(s\) with a lesser weight (\(w_s < y\)) has a bigger profit (\(p_s = opt > p_t\), where \(w_t = y \land p_t = g[y]\)), then \(s\) dominates \(t\). If solution \(s\) dominates \(t\), then for any item \(i\) the \(s \cap \{i\}\) solution will dominate the \(t \cap \{i\}\) solution. This way, new solutions created from \(t\) are guaranteed to be dominated by the solutions created from \(s\). A whole set of solutions that have \(t\) as subset can be discarded. 

The change from \(n\) to \(d[y]\) is inherited from the algorithm we based UKP5. It's a generic optimization trick that can be used at any problem where the solution is a set (as opposed to a sequence). If the item multiset \(\{1, 1, 2, 3\}\) is a solution, then every permutation of it is reached in a different moment, wasting processing. To avoid computing every solution in many different orders, we enforce decreasing order. Any item inserted on a solution \(s\) has to have an index that is equal to or lower than the index of the last item inserted on \(s\). 
%\footnote{The algorithm would computate \(\{1, 2, 3\} \cap \{1\}\) and \(\{1, 1, 2\} \cap \{3\}\), for example.}

When the two changes are combined, and the items are ordered by non-decreasing efficiency, UKP5 gets a big performance improvement. The non-dominated solutions (solutions that weren't skipped) are efficient (they have to be to not be dominated), consequently they need to be made of efficient items, that will have the lowest index values, and reduce the processing done at UKP5 inner loop greatly.

The third loop (lines \ref{get_y_opt_loop_begin} to \ref{get_y_opt_loop_end}) gets the optimal solution value and minimal weight. Any optimal solution, not excluded by our solution dominance test (\ref{if_less_than_opt_begin} to \ref{if_less_than_opt_end}), is guaranteed to be between \(c - w_{min} + 1\) and \(c\) (both inclusive). The proof is simple. A valid solution can't weight more than \(c\), and for any solution \(s\) that weights less than \(c - w_{min} + 1\), we can obtain a solution with a bigger profit inserting a copy of \(i\) to \(s\), where \(w_i = w_{min}\).

\subsection{Solution Dominance}

%\footnote{Note that, on ukp5, the lowest index of an item in a solution is also the index of the last item inserted in the solution (and consequently the value stored at \(d[y]\) where \(y\) is the solution weight).}
In this section we give a closer look at how the solution dominance speeds-up UKP5. We will use the \(min_{ix}(s)\) notation to refer to the lowest index between the items that compose the \(s\) solution. The \(max_{ix}(s)\) notation has an analogue meaning.

When a solution \(t\) is skipped because \(s\) dominates \(t\) (lines \ref{if_less_than_opt_begin} to \ref{if_less_than_opt_end}), some solutions \(u\), where \(t \subset u\), will not be generated. If \(s~dominates~t \land t \subset u \land max_{ix}(u - t) \leq min_{ix}(t)\) then \(u\) will not be generated by UKP5. In other words, if \(\{3, 2\}\) is dominated, then \(\{3, 2, 2\}\), \(\{3, 2, 1\}\) and any solution \(\{3, 2\} \cap u\), where \(u\) has only indexes lower than or equal to \(2\) will not be generated. Ideally, any \(u\) where \(t \subset u\) should not be generated as it will be dominated by a solution \(v\) where \(s \subset v\) anyway. It's interesting to note that this will happen eventually, as any \(t \cap {i}\) where \(i > min_{ix}(t)\) will be dominated by \(s \cap \{i\}\) (or by a solution that dominates \(s \cap \{i\}\)), and at some point no solution that is a superset of \(t\) will be generated anymore.

\subsection{Non Essential Steps}

With the purpose of making the initial explanation simpler, we have ommited some steps that are relevant to the algorithm performance, but not essential for accessing its correctness and core idea. A complete overview of the ommited steps is presented at this section.

% A QUESTÃO DO NÃO USO DAS DOMINANCIAS JÁ È DITA ANTES, REMOVER AQUI?
All the items are sorted by non-decreasing efficiency and, between items with the same efficiency, by increasing weight. The simple/multiple, collective or threshold dominances aren't used by UKP5, as this is often counterproductive for hard instances, where the undominated-to-all-items ratio is close to one, and superseeded by our implicit solution dominance. The \(y*\) periodicity bound is computed\cite[p. 223]{gar72}, and used to reduce the \(c\) value, if possible.

An UKP5-specific periodicity check is also used. This periodicity check isn't used to reduce the \(c\) capacity before starting UKP5, as \(y*\). The periodicity check is a stop condition inside UKP5 main loop (\ref{main_ext_loop_begin} and \ref{main_ext_loop_end}). The stop condition follows: let \(y\) be the value of the variable \(y\) at line \ref{main_ext_loop_begin}; let \(y'\) be the biggest capacity where \(g[y'] \neq 0 \land d[y'] > 1\); if, at some moment, \(y > y'\), then we can stop the computation and fill the remaining capacity with copies of the first item (item of index \(1\)).%In other words, if all solutions \(s\) with \(w_s > y\) contain at least one copy of the lowest indexed item (item of index \(1\)), then the only item we can add to them is the lowest indexed item (the inner loop at \ref{main_inner_loop_begin} iterates only until \(d[y]\), that will be \(1\)).

%if for all \(y' > y\) (where \(y\) is the loop control variable defined at line \ref{main_inner_loop_begin}) any \(g[y'] \neq 0\) has \(d[y'] = 0\), then we can stop at \(y\) as we know that any remaining capacity will be filled by copies of the item of lowest index (item of index \(0\)).

%\footnote{The only item that can dominate all other items by threshold is the best item.}
This periodicity check works only if the best item is the first item. If this assumption is false, then the described condition will never happen, and the algorithm will iterate until \(y = c - w_{min}\) as usual.

There's an \emph{else if} construct at line \ref{if_new_lower_bound_end}. If \(g[y + w_i] = g[y] + p_i \land i < d[y + w_i]\) then \(d[y] \gets i\). This may seem unecessary, as appears to be an optimization of a rare case, where two solutions have the same weight and profit. Nonetheless, without this test, the UKP5 was about 1800 (one thousand and eight hundreds) times slower on the subset-sum instances presented at \ref{tab:times}.

\section{Experiments}
% Describe all instance datasets.
% Describe the environment (computer, isolated cores, disabled hyperthreading)
% Only used internal time, internal time of PYAsUKP was strange
% external time not used because HD data reading race conditions, and different reading methods
% TABLE WITH RESULTS
In this section we describe the experiments environment, instance sets and results. We compare our UKP5 implementation and the EDUK2 implementation made by \cite{CPYA} (called PYAsUKP). The exact versions of the source codes used can be found at \url{https://github.com/henriquebecker91/masters/tree/v0.1}\footnote{The UKP5 implementation is at \textbf{codes/cpp/} and two versions of PYAsUKP are at \textbf{codes/ocaml/}. The \emph{PYAsUKP\_site.tgz} is the version used to generate the instances, and was also available at \url{http://download.gna.org/PYAsUKP/PYAsUKPsrc.html} by this paper writing date. The site version had bugs on the solver, so a version without those bugs was obtained thanks to Vincent Poirriez. This version is in \emph{PYAsUKP\_mail.tgz} and was the one used to solve the instances (i.e. make \ref{tab:times}). The \emph{create\_*\_instances.sh} scripts insides \textbf{codes/sh/} were used to generate the instance datasets.}. The times reported were given by the tools themselves and are supposed to not count the instance loading time (i.e. reading the instance from disk to memory). The runs external time\footnote{Given by the \textit{time} application, available at \url{https://www.archlinux.org/packages/extra/x86_64/time/}. The bash internal command was \emph{not} used.} were also captured and no significative discrepancy was perceived. Therefore, we have chosen to use the tools reported times (as is the common practice). For all instances, the weight, profit and capacity are integral.

We use the following non-standard notations at this section: \(rand(x, y)\) means a random integer between \(x\) and \(y\) (both inclusive); \(x\overline{n}\) means \(x\) as digits followed by the value of variable \(n\) as digits, for example: if \(n = 5000\) then \(10\overline{n} = 105000\).

We could not determine if the methods used by PYAsUKP to generate random integer sets results in an uniform distribution. We advice checking the source code for more information on the exact method used for obtaining random integer sets.

\subsection{Environment}

The computer used on the experiments was an ASUS R552JK-CN159H. This model CPU has four physical cores (Intel Core i7-4700HQ Processor, 6M Cache, 3.40 GHz). The operating system used was Linux nymeria 4.3.3-2-ARCH x86\_64 GNU/Linux (i.e. Arch linux). Three of the four cores were isolated using the \emph{isolcpus} kernel flag. The \emph{taskset} utility was used to execute UKP5 and PYAsUKP on the isolated cores. The computer memory was never completely used (no swapping was done). The UKP5 code was compiled with gcc (g++) version 5.3.0 (the \emph{-O3 -std=c++11} flags were enabled).

\subsection{Instance Sets}

The instance sets aim to reproduce the ones described in \cite{CPYA}. The same tool was used to generate the datasets (PYAsUKP), and the same parameters were used, otherwise noted the contrary. In \cite{CPYA}, section 5.1.1 \emph{Known ``hard'' instances}, some sets of easy instances are used to allow comparison with MTU2 (that had integer overflow problems with harder instances). With exception of the subset-sum dataset, all datasets have a similar harder set (\cite{CPYA}, section 5.2.1 \emph{New hard UKP instances}), so we ommited the easy sets and used only the harder ones. Each instance has a random capacity value within an interval, this interval is shown at \ref{tab:times}. The PYAsUKP parameters \emph{-wmin \(w_{min}\) -cap c -n \textbf{n}} were used in all instances generation. When we found a discrepancy between the formula presented in \cite{CPYA} and the PYAsUKP code, or generated instances, we opted for changing the formula based on the observed behaviour. As the PYAsUKP code can be hard to follow, we cannot guarantee that the formula presented here is a perfect match for the code; but, based by the generated instances, we believe it to be correct to a good extent.

\subsubsection{Subset-Sum}\label{sec:subsetsum}
Instances where \(p_i = w_i = rand(w_{min}, w_{max})\). The majority of the subset-sum instances used at \cite{CPYA} were solved on less than a centisecond in our experiments. This makes easy to have incorrect results by imprecise measuring. Because of this, in this paper, we use a dataset similar to the one used at \cite{CPYA}, but with each parameter multiplied by ten. Therefore, we generated 10 instances for each possible combination of: \(w_{min} \in \{10^3, 5\times10^3, 10^4, 5\times10^4, 10^5\}\), \(w_{max} \in \{5\times10^5, 10^6\}\) and \(n \in \{10^3, 2\times10^3, 5\times10^3, 10^4\}\). Totalizing 400 instances. We do not discriminate each combination at \ref{tab:times} for brevity. The PYAsUKP \emph{-form ss -wmax \(w_{max}\)} parameters were used.

\subsubsection{Strong Correlation}
Instances generated using the following formula: \(w_i = w_{min} + i - 1\) and \(p_i = w_i + \alpha\), for a given \(w_{min}\) and \(\alpha\).  Note that, except by the random capacity, all instances with the same \(\alpha\), \(\mathbf{n}\) and \(w_{min}\) combination are equal. The formula don't rely on random numbers. The PYAsUKP \emph{-form chung -step \(\alpha\) } parameters were used.

\subsubsection{Postponed Periodicity}
This family of instances is generated by the following method: \textbf{n} distinct weights are generated with \(rand(w_{min}, w_{max})\) and then sorted by increasing order; \(p_1 = w_1 + rand(1, 500)\); and \(\forall i \in [2, n].~p_i = p_{i-1} + rand(1, 125)\). The PYAsUKP \emph{-form nsds2 -step 500 -wmax \(w_{max}\)} parameters were used.

\subsubsection{No Collective Dominance}
This family of instances is generated by the following method: \textbf{n} distinct weights are generated with \(rand(w_{min}, w_{max})\) and then sorted by increasing order; \(p_1 = p_{min} + rand(0, 49)\); and \(\forall i \in [2, n].~p_i = \lfloor w_i \times ((p_{i-1}/w_{i-1}) + 0.01)\rfloor + rand(1, 10)\). The given values are: \(w_{min} = p_{min} = \mathbf{n}\) and \(w_{max} = 10\overline{n}\). The PYAsUKP \emph{-form hi -pmin \(p_{min}\) -wmax \(w_{max}\)} parameters were used.

\subsubsection{SAW}
This family of instances is generated by the following method: generate \textbf{n} random weights between \(w_{min}\) and \(w_{max} = 1\overline{n}\), sorted by increasing order, with the following property: \(\forall i \in [2, n].~w_i~mod~w_1 > 0\) (\(w_1\) is the smallest weight); then \(p_1 = w_1 + \alpha\) where \(\alpha = rand(1,5)\), and \(\forall i \in [2, n].~p_i = rand(l_i, u_i)\) where \(l_i = max(p_{i-1}, q_i)\), \(u_i = q_i + m_i\), \(q_i = p_1 \times \lfloor w_i / w_1 \rfloor \), and \(m_i = w_i~mod~w_1\). The PYAsUKP \emph{-form saw -step \(\alpha\) -wmax \(w_{max}\)} parameters were used.

\subsection{Results and Analysis}

The times used by UKP5 and PYAsUKP to solve the instance classes described at last secion are grouped in table \ref{tab:times}. No time limit was defined, and all optimal result values matched. In this section we expose and analyze our experimental results.

\begin{table}
\caption{Columns \textbf{n} and \(w_{min}\) values must be multiplied by \(10^3\) to obtain their true value. Let \(T\) be the set of times reported by UKP5 or EDUK2, then the meaning of the columns \textbf{avg}, \textbf{sd}, \textbf{min} and \textbf{max}, is, respectively, the arithmetic mean of \(T\), the standard deviation of \(T\), the minimal value of \(T\) and the maximal value of \(T\). The time unit of the table values is seconds.}
\label{tab:times}
\def\arraystretch{1.1}
\setlength\tabcolsep{4px}

\begin{tabular}{@{\extracolsep{4pt}}rrrrrrrrrrr@{}}

\hline
\multicolumn{3}{l}{Instance desc.} & \multicolumn{4}{l}{UKP5} & \multicolumn{4}{l}{PYAsUKP}\\
\cline{1-3}\cline{4-7}\cline{8-11}

\multicolumn{3}{l}{400 inst. per line} & \multicolumn{8}{l}{Subset-sum. Random \emph{c} between \([5\times10^6; 10^7]\)}\\
\cline{1-3}\cline{4-11}

& \textbf{n} & \(w_{min}\)  & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}

\multicolumn{3}{c}{See section~\ref{sec:subsetsum}} & 0.08 & 0.20 & 0.01 & 1.42 & 6.39 & 55.33 & 0.00 & 726.34\\
\hline

\multicolumn{3}{l}{20 inst. per line} & \multicolumn{8}{l}{Strong correlation. Random \emph{c} between \([20\overline{n}; 100\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
\textbf{\(\alpha\)} & \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
 5 & 5  & 10 & 0.05 & 0.00 & 0.05 & 0.05 & 2.46 & 2.81 & 0.00 & 6.13\\
   &    & 15 & 0.07 & 0.00 & 0.07 & 0.09 & 5.84 & 2.43 & 0.00 & 8.82\\
   &    & 50 & 0.20 & 0.06 & 0.08 & 0.24 & 18.35 & 12.64 & 0.00 & 50.58\\
 5 & 10 & 10 & 0.11 & 0.01 & 0.10 & 0.14 & 0.00 & 0.00 & 0.00 & 0.01\\
   &    & 50 & 0.49 & 0.03 & 0.47 & 0.60 & 41.97 & 33.97 & 0.00 & 93.18\\
   &    & 110 & 1.07 & 0.02 & 1.05 & 1.13 & 147.60 & 114.39 & 0.00 & 342.86\\
-5 & 5  & 10 & 0.06 & 0.00 & 0.06 & 0.07 & 5.98 & 4.02 & 0.00 & 11.99\\
   &    & 15 & 0.09 & 0.00 & 0.08 & 0.10 & 10.37 & 6.73 & 0.00 & 21.00\\
   &    & 50 & 0.21 & 0.05 & 0.09 & 0.24 & 39.31 & 30.16 & 0.00 & 89.44\\
-5 & 10 & 10 & 0.19 & 0.01 & 0.17 & 0.21 & 13.13 & 12.61 & 0.00 & 33.00\\
   &    & 50 & 0.54 & 0.02 & 0.52 & 0.59 & 82.97 & 71.22 & 0.00 & 206.74\\
   &    & 110& 1.08 & 0.02 & 1.07 & 1.13 & 261.61 & 246.21 & 0.00 & 721.89\\
\hline

\multicolumn{3}{l}{200 inst. per line} & \multicolumn{8}{l}{Postponed periodicity. Random \emph{c} between \([w_{max}; 2\times10^6]\)}\\
\cline{1-3}\cline{4-11}
& \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
& 20 & 20 & 1.42 & 0.31 & 0.55 & 2.77 & 17.00 & 17.05 & 0.01 & 63.96\\
& 50 & 20 & 10.20 & 1.28 & 7.91 & 14.98 & 208.61 & 210.72 & 0.03 & 828.89\\
& 20 & 50 & 1.59 & 0.32 & 0.96 & 2.99 & 27.68 & 22.79 & 0.02 & 100.96\\
& 50 & 50 & 6.86 & 1.23 & 4.46 & 11.78 & 233.58 & 187.91 & 2.65 & 682.95\\
\hline

\multicolumn{3}{l}{500 inst. per line} & \multicolumn{8}{l}{No collective dominance. Random \emph{c} between \([w_{max}; 1000\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
& \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
&  5 & n & 0.05 & 0.01 & 0.03 & 0.10 & 0.78 & 0.59 & 0.00 & 2.66\\
& 10 & n & 0.49 & 0.15 & 0.21 & 1.10 & 3.38 & 2.80 & 0.00 & 12.31\\
& 20 & n & 0.99 & 0.19 & 0.63 & 2.02 & 13.08 & 12.80 & 0.01 & 62.12\\
& 50 & n & 4.69 & 1.22 & 3.51 & 13.18 & 119.18 & 131.22 & 0.04 & 667.42\\
\hline

\multicolumn{3}{l}{\emph{qtd} inst. per line} & \multicolumn{8}{l}{SAW. Random \emph{c} between \([w_{max}; 10\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
\textbf{qtd} & \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
~200 &  10 & 10 & 0.11 & 0.01 & 0.10 & 0.16 & 1.88 & 1.24 & 0.01 & 4.73\\
~500 &  50 &  5 & 0.74 & 0.08 & 0.66 & 1.98 & 4.79 & 4.22 & 0.02 & 17.78\\
~200 &  50 & 10 & 1.01 & 0.03 & 0.97 & 1.27 & 10.44 & 9.02 & 0.03 & 38.69\\
~200 & 100 & 10 & 14.13 & 2.96 & 9.95 & 21.94 & 60.58 & 54.08 & 0.05 & 192.04\\
\hline

\end{tabular}
\end{table}
Based on \ref{tab:times}, except by one instance set that we will talk about later, we can make two statements: 1) the average time, standard deviation, and maximal time of UKP5 are always smaller than the PYAsUKP ones; 2) the minimal PYAsUKP time is always smaller than the UKP5 one.

Let's begin with the second statement (about the minimal), as EDUK2 uses a branch-and-bound (B\&B) algorithm before resorting to dynamic programming (DP), this is an expected result. Instances with big capacities and solutions that are composed by a large quantity of the best item, and a few of the non-best most efficient items, can be solved by B\&B fast, in a way that no dynamic programming algorithm can be competitive against. Our exception dataset (Strong Correlation, \(\alpha = 5\), \(n = 10\) and \(w_{min} = 10\)) is exactly the case. As said before, the strong correlation formula don't make use of random numbers, so all twenty instances of that dataset have the same items. The only thing that changes is the capacity. All solutions of this dataset are composed by hundreds of the best item (that is also the smallest item, making the dataset even easier) and exactly one non-best item for making better use of the residual capacity (\(c~mod~w_1\)). All other datasets have instances that present the same characteristics, and because of that, the PYAsUKP minimal is always close to zero. The number of instances where PYAsUKP was faster than UKP5 by instance class are: Subset-sum: 264 (\(\approx65\%\)); Strong correlation: 60 (\(25\%\)); Postponed periodicity: 105 (\(\approx13\%\)); No collective dominance: 259 (\(\approx13\%\)); SAW: 219 (\(\approx20\%\)).
%\footnote{The observant reader will see that, on the dataset ``Postponed periodicity'', \(n = 50\), \(w_{min} = 50\), PYAsUKP has a anomalous minimal time. On this dataset, the solution of the easiest instance is composed only by two items, the best one, and the 5317 most efficient one.}

% Put the importance of solving hard instances as they can have exponential time

As it's already well known, DP is not competitive against a B\&B approach for easy UKP instances. The UKP5 can't compete with PYAsUKP on easy datasets, as only the time for initializing an array of size \(c\) is already greater than the B\&B time. Nonetheless, for hard instances, B\&B is known to shows a bad worst case (exponencial time). As EDUK2 combine B\&B and DP with the intent of getting the strenghts of both, and none of its weaknesses, we found strange that this typical B\&B behavior is present at PYAsUKP. We executed PYAsUKP with the \emph{-nobb} flag, that disables the use of B\&B. The results show that any time under XXX seconds disappear, now taking at least XXX seconds, and times over this limit stay the same. Based on this evidence, we conclude that the PYAsUKP implementation of the EDUK2 DP phase is responsible for the big maximal PYAsUKP times (the time seems exponential but it is instead pseudo-polynomial with a big constant). For future research, it is interesting to verify if an implementation of UKP5 that executed B\&B for a small amount of time before resorting to DP could have better results than PYAsUKP, without the complexity that is integrating B\&B with DP in the way is done by EDUK2.

%Based on the first statement, and the discovery that the EDUK2 DP phase was responsible for the 
Looking back at the first statement of this section, we can now conclude that for instances that are hard for B\&B, UKP5 clearly surpass the DP solution by a big constant factor. Even considering the instances that PYAsUKP solves almost instantly (because of B\&B), UKP5 is about thirty times faster than PYAsUKP, in average. If we disconsidered the advantage given by B\&B (giving UKP5 a B\&B phase, or removing the one used on EDUK2) this gap would be even bigger.

The average UKP5 implementation memory consumption was greater than the PYAsUKP memory consumption. For each instance class, the UKP5-to-PYAsUKP memory consumption ratio was: Subset-sum: \(10.09\); Strong correlation: \(2.84\); Postponed periodicity: \(1.62\); No collective dominance: \(12.41\); SAW: \(1.31\). Note that the UKP5 memory use worst case is \(n + 2\times c\) (linear on \(n\) and \(c\)).

\section{Conclusions}

\section{Future Works}

%\section{Appendix}
\begin{comment}
\begin{table}
\caption{Columns \textbf{n} and \(w_{min}\) values must be multiplied by \(10^3\) to obtain their true value. Let \(T\) be the maximal memory used by UKP5 or EDUK2 as reported by the \emph{time} tool, then the meaning of the columns \textbf{avg}, \textbf{sd}, \textbf{min} and \textbf{max}, is, respectively, the arithmetic mean of \(T\), the standard deviation of \(T\), the minimal value of \(T\) and the maximal value of \(T\). The time unit of the table values is Mb (megabytes, not mibibytes).}
\def\arraystretch{1.1}
\setlength\tabcolsep{4px}

\begin{tabular}{@{\extracolsep{4pt}}rrrrrrrrrrr@{}}

\hline
\multicolumn{3}{l}{Instance desc.} & \multicolumn{4}{l}{UKP5} & \multicolumn{4}{l}{PYAsUKP}\\
\cline{1-3}\cline{4-7}\cline{8-11}

\multicolumn{3}{l}{400 inst. per line} & \multicolumn{8}{l}{Subset-sum. Random \emph{c} between \([5\times10^6; 10^7]\)}\\
\cline{1-3}\cline{4-11}

& \textbf{n} & \(w_{min}\)  & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}

\multicolumn{3}{c}{See section~\ref{sec:subsetsum}} & 133.96 & 22.85 & 89.85 & 175.67 & 13.28 & 6.71 & 6.28 & 53.28\\

\hline

\multicolumn{3}{l}{20 inst. per line} & \multicolumn{8}{l}{Strong correlation. Random \emph{c} between \([20\overline{n}; 100\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
\textbf{\(\alpha\)} & \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
 5 & 5  & 10 & 12.82 & 3.78 & 7.30 & 18.59 & 9.01 & 3.24 & 5.81 & 12.51\\
   &    & 15 & 12.38 & 3.42 & 7.70 & 19.40 & 12.93 & 2.68 & 5.86 & 14.99\\
   &    & 50 & 13.61 & 3.30 & 7.91 & 18.26 & 20.01 & 6.66 & 7.60 & 30.62\\
 5 & 10 & 10 & 105.57 & 36.90 & 36.13 & 153.65 & 7.13 & 0.82 & 6.25 & 8.40\\
   &    & 50 & 106.10 & 39.68 & 36.54 & 159.28 & 21.05 & 10.53 & 6.28 & 33.97\\
   &    & 110 & 103.18 & 34.99 & 37.68 & 149.38 & 41.56 & 22.93 & 6.35 & 78.19\\
-5 & 5  & 10 & 13.06 & 4.27 & 6.88 & 18.99 & 12.81 & 3.67 & 5.85 & 16.23\\
   &    & 15 & 13.81 & 3.43 & 8.56 & 19.30 & 14.33 & 3.81 & 5.85 & 17.92\\
   &    & 50 & 14.61 & 3.22 & 8.33 & 18.76 & 23.17 & 10.39 & 5.86 & 42.08\\
-5 & 10 & 10 & 109.12 & 36.36 & 37.02 & 159.60 & 13.91 & 4.37 & 6.28 & 18.51\\
   &    & 50 & 115.43 & 38.81 & 36.23 & 160.05 & 26.97 & 13.64 & 6.35 & 45.19\\
   &    & 110& 97.20 & 33.97 & 39.73 & 151.94 & 49.22 & 29.02 & 7.94 & 98.78\\
\hline

\multicolumn{3}{l}{200 inst. per line} & \multicolumn{8}{l}{Postponed periodicity. Random \emph{c} between \([w_{max}; 2\times10^6]\)}\\
\cline{1-3}\cline{4-11}
& \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
%42.80 & 3.98 & 35.65 & 50.54 & 16.12 & 4.70 & 7.84 & 25.20\\
\cline{1-3}\cline{4-7}\cline{8-11}
& 20 & 20 & 42.80 & 3.98 & 35.65 & 50.54 & 16.12 & 4.70 & 7.84 & 25.20\\
& 50 & 20 & 44.76 & 4.12 & 36.82 & 51.72 & 33.60 & 14.51 & 12.12 & 65.98\\
& 20 & 50 & 42.28 & 4.53 & 34.92 & 50.24 & 18.37 & 4.88 & 8.04 & 29.83\\
& 50 & 50 & 44.34 & 4.48 & 36.67 & 51.39 & 39.05 & 13.15 & 14.45 & 63.64\\

\hline

\multicolumn{3}{l}{500 inst. per line} & \multicolumn{8}{l}{No collective dominance. Random \emph{c} between \([w_{max}; 1000\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
& \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
&  5 & n & 83.39 & 44.10 & 7.54 & 161.35 & 12.44 & 5.00 & 5.91 & 36.65\\
& 10 & n & 826.66 & 440.19 & 35.86 & 1580.66 & 23.54 & 16.45 & 7.84 & 136.56\\
& 20 & n & 810.90 & 433.39 & 37.86 & 1578.22 & 46.89 & 34.39 & 8.24 & 205.98\\
& 50 & n & 798.77 & 451.91 & 40.37 & 1582.24 & 120.17 & 87.97 & 13.19 & 569.85\\

\hline

\multicolumn{3}{l}{\emph{qtd} inst. per line} & \multicolumn{8}{l}{SAW. Random \emph{c} between \([w_{max}; 10\overline{n}]\)}\\
\cline{1-3}\cline{4-11}
\textbf{qtd} & \textbf{n} & \(w_{min}\) & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max} & \textbf{avg} & \textbf{sd} & \textbf{min} & \textbf{max}\\
\cline{1-3}\cline{4-7}\cline{8-11}
~200 &  10 & 10 & 14.02 & 4.08 & 6.93 & 20.79 & 14.58 & 4.41 & 6.68 & 23.58\\
~500 &  50 &  5 & 15.89 & 3.92 & 8.72 & 23.16 & 17.39 & 3.55 & 11.96 & 27.54\\
~200 &  50 & 10 & 15.99 & 3.98 & 8.62 & 22.86 & 18.72 & 4.05 & 12.26 & 29.46\\
~200 & 100 & 10 & 111.93 & 42.44 & 39.58 & 179.31 & 60.98 & 33.91 & 20.85 & 140.52\\
\hline

\end{tabular}
\end{table}
\end{comment}

\bibliographystyle{splncs03.bst}
\bibliography{ukp5.bib}

\end{document}
