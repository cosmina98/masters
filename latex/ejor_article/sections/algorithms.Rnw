%\chapter{Approaches and algorithms}
%\label{sec:app_and_alg}

% TODO: remove the citations?
Two approaches are often used for solving UKP: dynamic programming (DP)~\cite{eduk},~\cite[p. 214]{garfinkel},~\cite[p. 311]{tchu} and branch and bound (B\&B)~\cite{mtu2}. 
%The DP approach has a stable pseudo-polynomial time algorithm linear on the capacity and number of items. 
%The B\&B approach can be less stable. 
%It can be faster than DP on instances with some characteristics, such as when the remainder of the division between the weight of the best item by the capacity is small; or the items have a big efficiency variance. Nonetheless, B\&B has always the risk of an exponential time worst case.

%\section{Dynamic Programming}
%\label{sec:dp_algs}

The Dynamic Programming (DP) approach is the oldest one found in the literature review (Section~\ref{sec:prior_work}).
Its worst-case time complexity is~\(O(nc)\) (pseudo-polynomial).
The DP approach can be considered stable, or predictable, compared to other approaches.
Stable in the sense that its run time variation when solving many instances with the same characteristics (i.e.~\(n\),~\(c\) and items distribution) can be lower than other approaches. 
Predictable in the sense that it is easier to predict a reasonable time interval for solving an instance based in the characteristics just mentioned, than it is with other approaches.

%The DP worst-case space complexity is~\(O(n + c)\), which can be considerably greater than other approaches that do not allocate memory linear to~\(c\).
%However, the space needed can be reduced by many optimizations.
%Some of these optimizations are: using a periodicity bound as explained in Section~\ref{sec:periodicity}; using modular arithmetic to reduce~\(c\) to~\(w_{max}\) in at least one array, see~\cite[p.~17]{gg-66}; using binary heaps instead of arrays, as the heap can use less memory than an array of~\(c\) positions if~\(w_{min}\) is sufficiently big.

%The DP approach often gives an optimal solution for each capacity smaller than~\(c\).
%However, some space optimizations can remove such feature.

\begin{comment}
\subsubsection{Weak solution dominance}
\label{sec:ukp5_sol_dom_expl}

% TODO: ADD THIS SOMEPLACE ELSE
In this section we will give a more detailed explanation of the workings of the previously cited weak solution dominance.
We use the notation~\(min_{ix}(s)\) to refer to the lowest index among the items that compose the solution~\(s\).
The notation~\(max_{ix}(s)\) has analogue meaning.

When a solution~\(t\) is pruned because~\(s\) dominates~\(t\) (lines~\ref{if_less_than_opt_begin} to~\ref{if_less_than_opt_end}), some solutions~\(u\), where~\(t \subset u\), are not generated. 
If~\(s\) dominates~\(t\), and~\(t \subset u\), and~\(max_{ix}(u - t) \leq min_{ix}(t)\), then~\(u\) is not generated by UKP5. 
For example, if~\(\{3, 2\}\) is dominated, then~\(\{3, 2, 2\}\) and~\(\{3, 2, 1\}\) will never be generated by UKP5, but~\(\{3,2,3\}\) or~\(\{3,2,5\}\) could yet be generated (note that, in reality, it is the equivalent~\([3,3,2]\) and~\([5,3,2]\) that could yet be generated).
Ideally, any~\(u\) where~\(t \subset u\) should not be generated as it will be dominated by a solution~\(u'\) where~\(s \subset u'\) anyway. 
It is interesting to note that this happens eventually, as any~\(t \cap \{i\}\) where~\(i > min_{ix}(t)\) will be dominated by~\(s \cap \{i\}\) (or by a solution that dominates~\(s \cap \{i\}\)), and at some point no solution that is a superset of~\(t\) will be generated anymore.

\subsubsection{Implementation details}
\label{sec:ukp5_periodicity}

With the purpose of making the initial explanation simpler, we have omitted some steps that are relevant to the algorithm performance, but not essential for assessing its correctness. 
A complete overview of the omitted steps is presented in this section.

\end{comment}

% TODO: it's necessary to add a implementation details section?:w
% TODO: add about the efficiency weight ordereing and breaking ties
% TODO: explain the addition below in the context of the ordered step-off
%There is an \emph{else if} test at line~\ref{if_new_lower_bound_end}. 
%If~\(g[y + w_i] = g[y] + p_i\) and \(i < d[y + w_i]\) then~\(d[y] \gets i\). 
%This may seem unnecessary, as appears to be an optimization of a rare case, where two distinct item multisets have the same weight and profit. 
%Nonetheless, without this test, UKP5 was about 1800 (one thousand and eight hundreds) times slower on some subset-sum instance datasets.

\subsection{EDUK}
\label{sec:eduk}

The EDUK (Efficient Dynamic programming for the Unbounded Knapsack problem) is a complex DP algorithm for the UKP, first mentioned in~\cite{ukp_new_results}.
However, only in~\cite{eduk} the algorithm essentials were described for the first time.
The author of this thesis, however, is partial to the algorithm description to be found in~\cite[p.~223]{book_ukp_2004}.

Before EDUK2 was proposed, EDUK was considered by some the state-of-the-art DP algorithm for the UKP.
An example is the comment in~\cite[p.~]{book_ukp_2004}: ``[...] EDUK [...] seems to be the most efficient dynamic programming based method available at the moment.''.

A version of the original code of the EDUK and EDUK2 algorithms is available here\footnote{PYAsUKP official site: \url{http://download.gna.org/pyasukp/pyasukpsrc.html}}.
Unfortunately, this version is not stable and has some bugs.
Consequently, the author of this thesis recommends the use of the version available here\footnote{The repository of this master's thesis: \url{https://github.com/henriquebecker91/masters/blob/f5bbabf47d6852816615315c8839d3f74013af5f/codes/ocaml/pyasukp_mail.tgz}.}.
We were given access to the latter version by Vincent Poirriez in January 11th, 2016.

\begin{comment}
The EDUK algorithm sorts the item list in increasing weight order, differently from the majority of the algorithms for the UKP that use the non-increasing efficiency order.

The sparse representation of the iteration domain is achieved by using lazy lists (a functional programming concept) instead of an array of size~\(c\) (or more) to store the solutions.
Consequently, the memory use is less dependent of~\(c\) and~\(w_{max}\) than other DP algorithms.
In~\cite{algo_tech_cut}, where the sparse representation idea was first presented, the solutions are represented as pairs of weight and profit value, as the solution was a pseudo-item (i.e. a set of items that can be treated as it was a single item).
For example, a solution~\(s\) consisting of the items~\(i\) and~\(j\) is represented by the following pair:~\((w_i + w_j, p_i + p_j)\).
Adding an item to a solution is equivalent to adding the weight and profit values of a pair to another.
For some instances, especially the ones with big~\(w_{min}\) and~\(c\) values, this sparse representation allows for saving time and memory.
In UKP5, for example, the algorithm allocates memory, initializes, and iterates over many capacities~\(y\) that are accessed and then skipped immediately because a solution~\(s\) with~\(w_s = y\) does not exist.
In EDUK, such skipped capacities are never explicitly enumerated to begin with, and no memory is allocated for them, or time used iterating over them. 
A similar effect could be obtained in UKP5 by using a \verb+std::map+ instead of an \verb+std::vector+ for the data structures~\(g\) and~\(d\) .
\end{comment}

\section{Branch-and-Bound}

% TODO: unify with comment in the prior work
The B\&B approach was established in the seventies as the most efficient approach for solving the UKP, what is greatly a consequence of the datasets, items distributions, and generation parameters used at the time.
The author of this thesis believes that this claim was first made in~\cite{mtu2}, and then other papers as~\cite{babayev} began repeating it.
The fact that only the code for MTU1 and MTU2 was readily available also did not help the situation, as some began to claim that MTU2 was the \emph{de facto} standard algorithm for the UKP, see~\cite{ukp_new_results}.

The time taken by a B\&B algorithm over an instance of the UKP can be hard to predict, and is not very dependent on the magnitude of~\(n\) and~\(c\), but more dependent on the item distribution.
In the worst case, a B\&B algorithm cannot eliminate a significant portion of the search space by the use of bounds and then, consequently, it needs to examine all search space.
In the case of the UKP, the search space is clearly combinatorial (all possible items combinations that fit the knapsack), so the worst-case of an B\&B approach for the UKP can be exponential.

The B\&B algorithms for the UKP often are not affected by the magnitude of~\(c\), however they can be affect by how close~\(c\) is from a capacity that can be entirely filled by copies of the best item.
The B\&B algorithms for the UKP will often solve the problem instantly if~\(c~mod~w_b\) is small, because the greedy heuristic lower bound will probably be optimal, and will exclude the remaining search space easily.

The fact that this approach is not significantly affected by huge values of~\(n\) and~\(c\), and more by the distribution used, makes it clear why it was considered the best approach in the seventies.
The datasets used back then had large~\(n\) and~\(c\) values, and items distributions that made easy to exclude large portions of the search space with the greedy lower bound solution (the uncorrelated distribution is the perfect example).

\subsection{MTU1}
\label{sec:mtu1}

The MTU1 algorithm is a B\&B algorithm for the UKP that avoids the explicit unfolding of the typical B\&B tree~\cite{mtu1}.
The implicit tree used by MTU1 is described in what follows, as this makes the algorithm easier to visualize and understand.
The MTU1 sorts the items in non-increasing efficiency order before beginning.
Such ordering is needed to assure the correctness of the bounds and, consequently, of the algorithm itself.
%In the algorithms description, it is to be assumed that the items are ordered in the mentioned order
%MTU1 begins by creating a lower bound solution using a greedy heuristic procedure.
As the nodes/solutions are visited in a systematic order, for any given node/solution, it is possible to know what part of the `search space'/tree was already visited or skipped, and what part has not yet been explored.
Consequently, the tree does not need to be enumerated explicitly, the current node/solution is sufficient to know which solutions should be tried next.

\subsection{MTU2}
\label{sec:mtu2}

The MTU2 algorithm was first proposed in~\cite{mtu2}.
The objective of MTU2 is to improve MTU1 run time when it is used in very large instances (e.g. up to 250,000 items).
MTU2 calls MTU1 internally to solve the UKP: it can be seen as a wrapper around MTU1 that avoids unnecessary computational effort.
The two main factors that motivated the creation of MTU2 were: 1) for the majority of the instances used in the period\footnote{For an example, one of the datasets of the paper that introduced MTU2 was analyzed in Section~\ref{sec:martello_uncorrelated}.}, an optimal solution is composed of the most efficient items; 2) for some of those instances, sorting the entire items list was more expensive than solving the instance with MTU1.

% TODO: ADD ONLY THE NEEDED TO GIVE THE READER THE NOTION THAT MTU2 IS AN WRAPPER OF MTU1

\section{Hybrid (DP and B\&B)}

As expected, some algorithms try to combine the best of two most popular approaches (DP and B\&B) for better results.

%\subsection{GREENDP}

The algorithm presented in~\cite{green_improv} is an improvement on the ordered step-off from~\cite{gg-66}.
It is very similar to UKP5.
The author does not know if it could be defined as a hybrid, but a good definition for it would be a `DP algorithm with bounds'.
The algorithm was not named in the paper and will be called GREENDP for the rest of the thesis.
The implementation of the GREENDP made by the author of this thesis, and used in the experiments (Section \ref{sec:exp_and_res}), will be called MGREENDP (Modernized GREENDP, in the sense that the algorithm now uses loops instead of the \verb+goto+ directive). 

The GREENDP algorithm consists in solving the UKP by using the ordered step-off algorithm, but without using the best item in the DP, and with interruptions at each~\(w_b\) capacity positions, for checking if the DP can be stopped and the remaining capacity filled with copies of the best item.
\begin{comment}
In those interruptions, two bounds are computed.
A lower bound for solutions using the best item is computed by combining the current best solution of the DP with as many copies of the best item as possible.
An upper bound for solutions not using the best item is computed by combining the current best solution of the DP with a pseudo-item that would fill the entire capacity gap and has the same efficiency as the second best item (it could also be seen as solving a continuous relaxation of the UKP without the best item, and only for the remaining capacity).
If the algorithm discovers that the lower bound with the best item is better than the upper bound without the best item, then the lower bound solution is optimal and the DP can be stopped.

This approach using bounds is fundamentally different from the periodicity check used by UKP5 (or the periodicity check used by the `terminating step-off').
For example, the use of bounds save computational time of GREENDP when it is used to solve BREQ instances, the periodicity check do not save computational time of UKP5 when it is used to solve the same instances (see experiments of the Section~\ref{sec:breq_exp}).
However this seems to have an impact on other families of instances (see experiments of the Section~\ref{sec:pya_exp}).
\end{comment}

The bound computed by GREENDP does not work if two or more items share the greatest efficiency.
Without the bound computation, the GREENDP is the same as the ordered step-off.

\subsection{EDUK2}

The EDUK2 algorithm was proposed in~\cite{pya}, and it is an hybridization of EDUK (a DP algorithm) and MTU2 (a B\&B algorithm).
\begin{comment}
The author of this thesis gives here a quick overview of the hybridization, but more details can be found in the paper above mentioned.
Just as with EDUK, the author does not claim to fully comprehend the EDUK2 internals, and only summarizes what is said in the original paper.
The author recommends reading Sections~\ref{sec:eduk} (EDUK) and~\ref{sec:mtu2} (MTU2) before the explanation below, as it is strongly based in both algorithms.
The comments made about EDUK code in its own section also apply to EDUK2.

The description of the changes in EDUK caused by the hybridization follows.
The~\(k = min(n, max(100, \frac{n}{100}))\) most efficient items are gathered in a tentative core problem.
A B\&B algorithm ``similar to the one in MTU1''\footnote{Quoted from \cite{pya}.} tries to solve the tentative core problem.
This B\&B algorithm has the possibility of choosing among three bound formulas, and stops after exploring~\(B = 10,000\) nodes (of the implicit enumeration tree).
If the B\&B algorithm returns a solution with value equal to an upper bound for the whole instance, then the DP algorithm never executes.
Otherwise, the solution given by the B\&B algorithm is used as a global lower bound in the hybridized EDUK algorithm.
The hybridized EDUK algorithm works like EDUK would do, with the addition of an extra phase between the slices.
The extra phase uses the lower bound to eliminate items \emph{and solutions for lesser capacities} from the algorithm.
This phase is very similar to a phase of MTU2: an upper bound is computed for solutions using one copy of the respective item (a solution~\(s\) can be treated as a pseudo-item (\(w_s, p_s\))).
If this upper bound is equal to or smaller than the global lower bound, then the item (or solution) is abandoned by the algorithm.
A new lower bound is computed for each solution that was not removed by the process described above.
The lower bound consists in filling the remaining capacity with a greedy algorithm.
If this new lower bound is better than the global lower bound, it replaces it. 

%SKIP DIDI
%Basically eduk, but with a B\&B pre-phase that can help with bounds computation
\end{comment}

%\section{Consistency Approach}

%The Consistency Approach (CA) consists in combining both the objective function (i.e.~\(maximize~p_i x_i\)) and the UKP only constraint (i.e.~\(w_i x_i \leq c\)) in a single Diophantine equation\footnote{``A Diophantine equation is an equation in which only integer solutions are allowed.''~\cite{diophantine}.
%In other words, an equation where the values of the variables/unknowns are restricted to the integer numbers.} (\(\alpha_i x_i = \beta\), where~\(\alpha_i\) are coefficients computed for each item, and~\(\beta\) is the analogue of an optimal solution upper bound).
%The combination procedure preserves the set of valid solutions, consequently, an optimal solution for the UKP can be sought by testing values for the equation variables/unknowns until the equation holds true.
%Such variables are often the quantity of each item in a solution (\(x_i\)) (on one side of the equation) and a tentative value derived from the optimal solution upper bound (\(\beta\)) (on the other side of the equation).

%\subsection{GREENDP1}

\subsection{Algorithms not included}

In this section, we discuss some algorithms that we are aware of the existence but are not included in our experiments.

The papers~\cite{cabot} and~\cite{turnpike} proposed algorithms for the UKP, but they are not included in our experiments.
We did not have access to these papers, as both are behind a paywall.
Moreover, both algorithms have comparison experiments described in the literature that show them to be dominated by algorithms included in our experiments\footnote{In \cite{mtu1}~it is shown that MTU1 dominates the algorithm from~\cite{cabot}, and in \cite{green_improv}~it is implied that GREENDP is an improved version of the algorithm from~\cite{turnpike}.}.

Two algorithms for the UKP are proposed in~\cite{on_equivalent_greenberg}.
The first algorithm could not be executed over instances of recent datasets without exceeding our time limit.
The second algorithm does not work for all UKP instances.
We choose to not include both algorithms in our experiments\footnote{Readers who are interested in those algorithms can access our implementations made available at \url{https://github.com/henriquebecker91/masters/blob/e2ff269998576cb69b8d6fb1de59fa5d3ce02852/codes/cpp/lib/greendp.hpp}.}.

% TODO: DECIDE IF THE ORDERED AND THE TERMINATING WILL BE SHOWN OR ONLY THE TERMINATING
% TODO: JUSTIFY WHY ONLY THE ORDERED/TERMINATING WERE SHOWN (NOT THE PERIODIC)
% ex: the terminating step-off add O(n + c) operations in exchange for the possibility of removing O(c) operations
An examination of the naïve DP algorithm for the UKP~\cite[p.~311]{tchu} and its improved version presented in~\cite[p.~221]{garfinkel} shows that both algorithms are dominated by the ordered step-off algorithm presented in \cite[p.~15]{gg-66}.
Also, the UKP5 algorithm proposed in \cite{sea2016} was found to be equivalent to the ordered step-off.
Since the experiments presented in this paper already include the ordered step-off, the other three algorithms were not included.

% TODO: consider reworking this phrase
A B\&B algorithm proposed in~\cite{gg-63} was disregarded because in~\cite{gg-66} the same authors only presented DP algorithms as efficient algorithms for solving the UKP. 
We did not obtain access to the code of the algorithm proposed in~\cite{babayev} and we had difficulties trying to implement it.

An instance of the UKP can be converted in an instance of the BKP by adding a constraint \(x_i \leq \floor{\frac{c}{w_i}}\) for each item \(i\).
Such conversion would allow us to add BKP algorithms to the comparison.
Nevertheless, we decided against adding them, since UKP algorithms are BKP algorithms fine-tuned for UKP instances and there exists evidence in the literature that BKP algorithms perform worse than UKP algorithms in UKP instances~\cite{mtu1}.

