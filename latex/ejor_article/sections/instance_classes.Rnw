<<setup,results=hid,echo=false>>=
library(ggplot2)
@

\section{The datasets}

In this section, we discuss some datasets from the literature and propose new datasets.

\subsection{Uncorrelated items distribution datasets}
\label{sec:inst_uncorrelated}

An instance with an uncorrelated items distribution is an UKP instance in which the weight and the profit of its items are not correlated.
The most common way of generating uncorrelated instances is to generate a value between~\(w_{min}\) and~\(w_{max}\) for the weight, and a value between~\(p_{min}\) and~\(p_{max}\) for the profit, for each of the~\(n\) items of the instance, using a Pseudo-Random Number Generator (PRNG) with a uniform distribution.

% TODO: maybe put the slides graph here

%\begin{figure}[h]
%\caption{An uncorrelated instance generated with~\(n = 100\),~\(w_{min} = 1\),~\(w_{min} = 1000\),~\(p_{min} = 1\), and~\(p_{max} = 1000\).}
%\begin{center}
%<<uncorrelated,fig=true,echo=false>>=
%n <- 100
%wmax <- 1000
%pmax <- 1000
%s <- 42
%set.seed(s)
%w <- sample(1:wmax, n, replace=TRUE)
%p <- sample(1:pmax, n, replace=TRUE)
%t <- data.frame(w = w, p = p)
%qplot(w, p, data = t,
%  xlab = 'weight',
%  ylab = 'profit')
%@
%\end{center}
%%\legend{Source: the author.}
%\label{fig:uncorrelated_example}
%\end{figure}

The average percentage of items that are not simple or multiple dominated in a uncorrelated instance is very small~\cite{zhu_dominated} (about 0.1\% for \(n = 10000\)), and grows smaller with the size of~\(n\).
Consequently, the times used by an algorithm to solve uncorrelated instances have little to do with the algorithm capability of solving a NP-hard problem efficiently, and much to do with the polynomial algorithms or heuristics it uses to remove simple and multiple dominated items before starting.
Because of this, we have chosen to not use uncorrelated instances in our comparison experiments.

\subsection{PYAsUKP dataset}
\label{sec:pya_inst}

This section describes the instance datasets proposed in~\cite{pya}, and reused in the comparison presented in~\cite{sea2016}.
All those datasets were artificially generated with the purpose of being ``hard to solve''.
The adjective `hard' can mean a different thing for each one of the datasets. 
Some item distributions used in these datasets were first proposed in~\cite{pya}, others were taken from the literature.

q
These datasets are similar to the ones used in~\cite{pya}.
The same tool was used to generate the datasets (PYAsUKP) and the same parameters were used, unless otherwise stated.
However, some instances make use of random seed values that were not provided, so the exact instances used in~\cite{pya} can be different.
The instances are exactly the same presented in~\cite{sea2016}.

The same code that implements EDUK and EDUK2 also implements the instance generator that generated the instances used in the experiments described in this thesis.
Some datasets generated by this tool have the item list ordered by increasing weight, what gives a small advantage to EDUK that uses this ordering.

In the subsection 5.1.1 \emph{Known ``hard'' instances} of~\cite{pya} some sets of easy instances are used to allow comparison with MTU2. 
However, MTU2 had integer overflow problems on harder instances. 
With exception of the subset-sum dataset, all datasets have a similar harder set (Subsection 5.2.1 \emph{New hard UKP instances}~\cite{pya}).
Thus, we considered in the runs only the harder ones. 

The notation~\(rand(x, y)\) means an integer between~\(x\) and~\(y\) (including both~\(x\) and~\(y\)), generated by a PRNG with an uniform distribution.
Also, when referring to the parameters for the generation of an instance,~\(w_{min}\) will be used to denote the smallest weight that can appear in an instance, but without guarantee that an item with this exact weight will exist in the generated instance.
The meaning of~\(w_{max}\) is analogue.
The syntax~\(x\overline{n}\) means~\(x\) as a string concatenated with the value of~\(n\) as a string (e.g. for~\(n = 5000\) then~\(10\overline{n} = 105000\)).

The dataset presented in this section comprises five smaller datasets.
Each of these datasets is characterized by a formula used to generate the items, and use different combinations of parameters to generate the instances.
Three parameters are present in all instance generation procedures, they are: \(n\) (number of items), \(c\) (knapsack capacity) and \(w_{min}\) (explained last paragraph).
The arguments for such parameters were given to the PYAsUKP binary by means of the following flags: \mbox{\emph{-wmin~\(w_{min}\) -cap c -n \textbf{n}}}.
In the description of each one of the five smaller datasets, we will present any other PYAsUKP flags that were also needed to generate that dataset.
%Each instance has a random capacity value within intervals shown in Table~\ref{tab:times}. 

We found some small discrepancies between the formulas presented in~\cite{pya} and the ones used in PYAsUKP code.
We opted for using the ones from PYAsUKP code, and they are presented below.

\subsubsection{Subset-sum instances}\label{sec:subsetsum}

Subset-sum instances are instances where~\(p_i = w_i = rand(w_{min}, w_{max})\).
Therefore, 10 instances were generated for each possible combination of:~\(w_{min} \in \{10^3, 5\times10^3, 10^4, 5\times10^4, 10^5\}\);~\(w_{max} \in \{5\times10^5, 10^6\}\) and~\(n \in \{10^3, 2\times10^3, 5\times10^3, 10^4\}\), totaling 400 instances.
Each instance had a random capacity in the range~\([5\times10^6; 10^7]\).

\subsubsection{Instances with strong correlation between weight and profit}
\label{sec:sc_inst}

There are many formulae for generating items distribution that could be considered strongly correlated item distributions.
However, in~\cite{pya}, the formula presented below was used, because ``\cite{chung_hard} have shown that solving this problem is difficult for B\&B.''.
In all strongly correlated instances with~\(\alpha > 0\), the smallest item is also the best item, a trait that often makes an instance easier (the best item ends up multiple dominating many items). 

Instances were generated using the following formula:~\(w_i = w_{min} + i - 1\) and~\(p_i = w_i + \alpha\), for a given~\(w_{min}\) and~\(\alpha\).
Note that, except by the random capacity, all instances with the same~\(\alpha\),~\(\mathbf{n}\), and~\(w_{min}\) combination are equal.
The formula does not rely on random numbers.
The PYAsUKP \emph{-form chung -step~\(\alpha\)} flags were used.

Twenty instances were generated with each combination of~\(n = 5\times10^3\),~\(\alpha \in \{5, -5\}\) and~\(w_{min} \in \{10^4, 1.5\times10^4, 5\times10^4\}\).
Twenty more instances were generated with each combination of~\(n = 10^4\),~\(\alpha \in \{5, -5\}\) and~\(w_{min} \in \{10^4, 5\times10^4, 11\times10^4\}\), totalling 240 instances. Each instance had a random capacity in the range~\([20\overline{n}; 100\overline{n}]\).

\subsubsection{Instances with postponed periodicity}

Many algorithms benefit from the periodicity property, explained in Section~\ref{sec:periodicity}, by computing an upper bound on capacity~\(y\).
For the instances created using the formula below and ``where~\(c < 2 \times w_{max}\) and~\(n\) is large enough, the periodicity property does not help''\cite[p.~13]{pya}.
The idea of such instances seems to be putting all algorithms on an equal footing regarding which capacity they are solving an instance. 

Two hundred instances were generated for each combination of~\(n = \{2\times10^4, 5\times10^4\}\) and~\(w_{min} = \{2\times10^4, 5\times10^4\}\).
Totalling 800 instances.
Each instance had a random capacity in the range~\([w_{max}; 2\times10^6]\).

\subsubsection{Instances without collective dominance}

Any items distribution in which the efficiency of the items increases with their weight has no collective, multiple, or simple dominated instances.
There is threshold dominance in such instances, as bigger items will probably dominate solutions composed of many copies of a smaller item.
A distribution with this characteristics was proposed ``to prevent a DP based solver to benefit from the variable reduction due to the collective dominance''~\cite[p.~13]{pya}, with the intent of making comparison fairer.

Five hundred instances were generated for each combination of~\(n = w_{min} \in \{5\times10^3, 10^4, 2\times10^4, 5\times10^4\}\), totalling 2000 instances.
Each instance had a random capacity in the range~\([w_{max}; 1000\overline{n}]\).

\subsubsection{SAW instances}
\label{sec:saw_inst}

The SAW instances were proposed in~\cite{pya}.
They include the strongly correlated instances with~\(\alpha > 0\), and as such share the same trait pointed out in Section~\ref{sec:sc_inst}.

This family of instances is generated by the following method: generate \textbf{n} random weights between~\(w_{min}\) and~\(w_{max} = 1\overline{n}\) with the following property:~\(\forall i \in [2, n].~w_i~mod~w_1 > 0\) (\(w_1\) is the smallest weight); sort the weights by increasing order; then~\(p_1 = w_1 + \alpha\) where~\(\alpha = rand(1,5)\), and~\(\forall i \in [2, n].~p_i = rand(l_i, u_i)\) where~\(l_i = max(p_{i-1}, q_i)\), \(u_i = q_i + m_i\), \(q_i = p_1 \times \lfloor w_i / w_1 \rfloor \), and~\(m_i = w_i~mod~w_1\).
The PYAsUKP \emph{-form saw -step~\(\alpha\) -wmax~\(w_{max}\)} flags were used.

Two hundred instances were generated for each combination of~\(w_{min} = 10^4\),~\(n \in \{10^4, 5\times10^4, 10^5\}\).
Additional 500 instances were generated with~\(w_{min} = 5\times10^3\) and~\(n = 5\times10^4\), totalling 1100 instances.
Each instance had a random capacity in the range~\([w_{max}; 10\overline{n}]\).

\subsubsection{PYAsUKP dataset and reduced PYAsUKP dataset}
\label{sec:pya_dat_red_pya_dat}

The dataset described in the last five subsections, totalling 4540 instances, will be referred to in the rest of this work as \emph{the PYAsUKP dataset}.
In each of these five smaller datasets, for the same combination of parameters, the number of instances generated was always perfectly divisible by ten.
The dataset composed of one tenth of the PYAsUKP dataset, following the same distribution, and totalling 454 instances, will be referred to in the rest of this work as \emph{the reduced PYAsUKP dataset}.

\subsection{CSP pricing subproblem dataset}
\label{sec:csp_ukp_inst}

The applied use of the UKP chosen by the author to be developed in this work was the pricing subproblem generated by solving the continuous relaxation of the set covering formulation for the classic Bin Packing Problem (BPP) and Cutting Stock Problem (CSP) using the column generation approach.

For readers interested in mathematical proofs, and in longer explanations of \emph{why} the method works, in contrast to \emph{how} it works, we recommend reading Section 15.2 (pages~455~to~459) of~\cite{book_ukp_2004} and/or the seminal paper by~\cite{gg-61}.

As the BPP and the CSP are very similar, and instances of one problem can be converted to instances of the other (similarly to 0-1 KP and BKP), we will explain the relationship only in terms of the CSP.
%The mathematical notation conflict with the one used with the rest of the work, however it is used only here, and self contained.

An instance of the CSP problem consists of~\(n\) distinct sheet sizes; each sheet size~\(i = \{1, \dots, n\}\) has an unidimensional size~\(w_i\) and a demand~\(d_i > 0\), that needs to be satisfied; to satisfy the demand it is necessary to cut sheets of the desired size from master rolls of size~\(c\), where~\(c\) is bigger than any sheet size.
It is assumed that there is a sufficient amount of master rolls to satisfy all demands and, as such, the instance does not define a number of master rolls available.
The objective of the problem is to find a way to fill all demands while using the smallest possible number of master rolls.
If one or more sheets are cut from a master roll, that master roll is considered used, and remaining space is considered waste.

The previously mentioned Set Covering Formulation (SCF) for BPP and CSP is a tight formulation proposed in~\cite{gg-61}.
The SCF eliminated the problems of the classic formulation that was loose and had too many symmetric solutions.
However, as a consequence, the SCF needs to compute all cutting patterns, i.e. all combinations of sheet sizes that can be cut from a single master roll.
As the cutting patterns are combinations, the amount of cutting patterns can be superexponential in the number of sheet sizes. The exact number of cutting patterns is affected by the sheet sizes. The best case happens when \(\forall i.~w_i > \frac{c}{2}\), in this case the number of cutting patterns is \(n\). The worst case happens when all \(n\) sheet sizes have almost the same size (let us call this size \(w^*\)), and \(n > k = \frac{c}{w^*}\), in this case the number of cutting patterns is given by the binomial coefficient \(\binom{n}{k}\) (that computes \(n!\), which is superexponential).

The SCF follows:

\begin{align}
  \mbox{minimize} & \sum_{j=1}^m x_j \label{eq:csp_objfun}\\
\mbox{subject~to} & \sum_{j=1}^m a_{ij} x_j \geq d_i,~~~\forall i \in \{1,...,n\},\label{eq:csp_demand}\\
           & x_j \in \mathbb{N}_0,~~~\forall j \in \{1,...,m\}.\label{eq:csp_x_integer}
\end{align}

All cutting patterns~\(j = \{1, \dots, m\}\) can be represented by a matrix~\(a_{ij}\) that stores the amount of sheets of size~\(i\) obtained when the cutting pattern~\(j\) is used.
If we know all cutting patterns, a solution for the CSP can be represented by a variable ~\(x_j\) that stores the amount of master rolls which were cut using a specific cutting pattern~\(j\).

It is important to remember that, in this work, our objective is not to solve CSP but its continuous relaxation.
%The only change this makes to the model above is that~\ref{eq:csp_x_integer} is restricted to~\(\mathbb{R}\), not~\(\mathbb{N}_0\).

The column generation approach consists in avoiding the enumeration of all~\(m\) cutting patterns.
The SCF relaxation is initialized with a small set of cutting patterns that can be computed in polynomial time and in which each sheet size appears at least in one of the patterns.
This reduced problem is called the \emph{master problem}.
It is solved by using the simplex method, as it is a linear programming problem.
A by-product of this solving process are the dual variables of the master problem model.
Those variables are used as input for a \emph{pricing subproblem}.
The solution of this pricing subproblem is the cutting pattern that, if added to the master problem, will give the greatest improvement to master problem optimal solution.

% As the master problem only have some cutting patterns its optimal solutions probably won't be the optimal solution for the original problem.
The pricing subproblem for the column generation of the BPP/CSP is the UKP.
An instance of the UKP created by the procedure described above will have the following structure:

\begin{align}
  \mbox{maximize} & \sum_{i=1}^n y_i x_i \label{eq:csp_ukp_objfun}\\
\mbox{subject~to} & \sum_{i=1}^n w_i x_i \leq c,\label{eq:csp_ukp_cap}\\
           & x_i \in \mathbb{N}_0,~~~\forall i \in \{1,...,n\}.\label{eq:csp_ukp_x_integer}
\end{align}

The formulation above is clearly equivalent to the formulation presented in Section~\ref{sec:formulation}.
The sheet sizes~\(i = \{1, \dots, n\}\) are the items~\(i = \{1, \dots, n\}\).
The size of the sheets~\(w_i\) is the weight of the items~\(w_i\).
The value of the dual variable associated to a specific sheet size~\(y_i\) is the profit value~\(p_i\).
The size of the master roll~\(c\) is the knapsack capacity~\(c\).
The new cutting pattern described by~\(x_i\) is an optimal solution for UKP~\(x_i\) (items inside the knapsack are equivalent to sheets cut from the master roll).


The solving process alternates between solving the master problem and the pricing subproblem, until all cutting patterns that could improve the solution of the master problem are generated and added to the master problem.
The profit values of the pricing subproblem (dual variables) are real numbers close to \emph{one}.
If the value of the optimal solution for the pricing subproblem is \emph{one} or less, we have a guarantee that the master problem cannot be improved by adding any new cutting patterns to it.
The computation could be stopped and the optimal solution for the master problem is the exact optimal solution for the continuous relaxation of the CSP instance.
However, floating point arithmetic is imprecise.
In the real-world, an implementation of the pricing subproblem can return a value slight above \emph{one}, when it should have returned \emph{one} or slight less than \emph{one}.
In this case, adding the newly generated cutting pattern will not improve the master problem solution, and will re-generate the same pricing subproblem with the same optimal solution value incorrectly above \emph{one} (infinite loop).
Taking this into account, a better method for stopping the computation is verifying if the current pricing subproblem is equal to the one from last iteration, or if the solution of the pricing subproblem is equal to the one from the last iteration.

%\begin{align}
%\sum_{i=1}^m w_i \alpha_i \leq c \label{eq:csp_pattern}\\
%\end{align}

The method described above can generate thousands of UKP instances for one single instance of the CSP.
For the same instance of the CSP, the number of UKP instances generated, and their exact profit values, can vary based on the choice of optimal solution made by the UKP solver (for the same pricing subproblem many cutting patterns can be optimal, but only one among them is added to the master problem).
Consequently, such dataset is hard to describe (has a large and variable number of instances with variable profit values).
The best way found by the author to ensure that the results are reproducible is making available the exact codes used in the experiment, together with the list of CSP instances from the literature used in the experiment.
The codes are deterministic, and consequently will produce the same results if executed many times over the same CSP instance.

A recent survey on BPP and CSP gathered the instances from the literature, and also proposed new ones~\cite{survey2014}.
The total number of instances in all datasets presented in the survey is 5692.
The author of this thesis chose ten percent of those instances for the experiments presented at Section \ref{sec:csp_experiments}.
This fraction of the instances was randomly selected among instances within the same dataset or, in the larger datasets, the same generation parameters.
The address of a repository containing the data and code used in the above mentioned experiments, and the instructions to compile the code, can be found in \ref{sec:csp_appendix}.

\cite{gg-61,gg-63,gg-66} present some optimizations to the master problem solver that we have not implemented.
The author of this thesis believes that these optimizations do not considerably affect the structure of the pricing subproblem.
Also, this work has no intention of providing a state-of-the-art algorithm for solving the CSP continuous relaxation, but only to study algorithms for the UKP in the context of a pricing subproblem and independently.

A list of implementation details follows: cutting patterns that are not used by the last solution for the master problem could be removed, however they can end up being valuable again in the future and re-generated by the pricing subproblem, so the author chose not to remove them; the classic polynomial method for creating the initial set of cutting patterns was used: it consists in creating~\(n\) patterns, each one with only one sheet size that is cut as many times as possible, state-of-the-art solvers often begin using cutting patterns generated by a more sophisticated heuristic; the sheet sizes can be divided in two groups, half with higher demand, and half with lower demand; and the pricing subproblem can be restricted to the high demand group in some conditions -- this also has not been done in our experiments.

\subsection{Bottom Right Ellipse Quadrant instances}
\label{sec:breq_inst}

The Bottom Right Ellipse Quadrant (BREQ) is a new items distribution proposed in this work\footnote{The author is aware of the existence of the \emph{circle instances}, which are described in \cite[p. 158]{book_ukp_2004}. In this book, the formula presented for the circle instances is \(p_i = d\sqrt{4R^2 - (w_i - 2R)^2}\) -- in which \(d\) is an arbitrary positive constant~(\(\frac{2}{3}\)) and \(R\)~is~\(w_{max}\). This formula describes the \emph{upper left} quadrant of an ellipse, which is different from the BREQ formula that describes the \emph{bottom right} quadrant of an ellipse. Consequently, circle instances and BREQ instances have completely different properties. Also, in the referred book, circle instances are solved as they were instances of the 0-1 KP (not of the UKP).}.
This instance distribution was created to illustrate that different item distributions favors different solution approaches and, therefore, the choice of instances (or specifically, their item distribution) defines what is considered the \emph{best algorithm}.

Distributions that are easy to solve by the DP approach and hard to solve by the B\&B approach are common in the recent literature.
This distribution has the opposite characteristic, it is hard to solve by DP and easy to solve by B\&B.

The name given to this distribution is derived from the fact that, when plotted on a graph, the items show the form of a quarter of ellipse (specifically, the bottom right quadrant).
All items with such distribution respect the following equation:~\(p_i = p_{max} - \floor[\big]{\sqrt{p_{max}^2 - (w_i^2 \times \frac{p_{max}}{w_{max}}^2)}}\).
In this context,~\(w_{max}\) and~\(p_{max}\) define the quadrant top right corner, i.e. the possible maximum value for an item weight and profit, an item with those exact values does not need to exist in a BREQ instance.
The rounding down in the formula can be dropped if the profit is a real number and not an integer.

%\begin{figure}[h]
%\caption{A 128-16 BREQ Standard instance with~\(n = 2048\)}
%\begin{center}
%<<breq_inst,fig=true,echo=false>>=
%library(ggplot2)

%t <- read.csv('../data/128_16_std_breqd-n2048-s0.csv', sep = ';')
%qplot(w, p, data = t,
%  xlab = 'weight',
%  ylab = 'profit')     
%@
%\end{center}
%%\legend{Source: the author.}
%\label{fig:breq_example}
%\end{figure}

A natural consequence of this distribution shape is that the item efficiency grows with the item weight.
This leads to the inexistence of simple, multiple and collective dominance\footnote{If the profit is integer, some small items can display those three dominance relations because of the profit precision loss caused by the rounding.
The author of this thesis believe that this exception is of little relevance and can be safely ignored for most purposes.
If profit is an infinite precision real, the statement has no exceptions.}.
In other words, for any solution~\(s\) composed of two or more items, and for any single item~\(i\), if~\(w_s \leq w_i\) then~\(p_s < p_i\).

On the other hand, threshold dominance is very common in this instance type.
Except for the best item, any item of any instance (of any distribution, not only BREQ instances) will always be threshold dominated at some capacity.
In many UKP instances, however, the knapsack capacity is smaller than those threshold values and therefore the threshold dominance is not applied or relevant.
In BREQ instances, also as a consequence of the efficiency growth, an optimal solution will never include the item~\(i\) two or more times if there is an item~\(j\) such as that~\(\sqrt{2} \times w_i \leq w_j \leq 2 \times w_i\).
% Proof that this interval is tight: http://www.wolframalpha.com/input/?i=2*(100+-+sqrt(100%5E2+-+w%5E2+*+16%5E2))+%3C%3D+100+-+sqrt(100%5E2+-+((sqrt(2)*w)%5E2+*+16%5E2))

The solutions of BREQ instances will often contain the maximum number of copies of the largest item (that is also the most profitable, and the most efficient) allowed by the instance capacity.
Any gap left will probably be filled with the heaviest item that fits the gap. The process should be repeated until no item fits the gap left (or there is no gap).
The classical greedy heuristic procedure that follows those steps would probably yield an optimal solution.
However, this is not always the case\footnote{A counter-example follows: consider an instance with~\(n = 4\),~\(c = 512\),~\(w_1 = 384\),~\(p_1 = 2774\),~\(w_2 = 383\),~\(p_2 = 2756\),~\(w_3 = 129\),~\(p_3 = 265\),~\(w_4 = 32\) and~\(p_4 = 17\); the optimal solution does not use the best item (\(w_1, p_1\)); the best solution when using the best item has a profit value of~\(2842 = 2774 + 4\times17\) (weight~\(512 = 384 + 4\times32\)) while the best solution when using the second most efficient item has the optimal profit value of~\(3021 = 2756 + 265\) (weight~\(512 = 383 + 129\)).
In this case, between two solutions with the same weight, the one with the best item is not the best one.
The weight and profit values of this example follow a BREQ distribution with~\(w_{max} = 512\) and~\(p_{max} = 8192\).}.

The reasons that make BREQ instances favor B\&B over DP can be understood by examining the two approaches behaviour.
The B\&B approach begins by creating a solution using some sort of greedy heuristic similar to the one described in the last paragraph.
This solution will be close to optimal, if not optimal, and provides a good lower bound.
With this lower bound, the search space will be greatly reduced, making the algorithm end almost instantly.
On the other side, the DP approach is based on solving subproblems.
Subproblems which will be solved for increasing capacity values yielding optimal solutions to be reused (combined with other items).
However, solutions with small weight and/or solutions composed of items with small weights are often less efficient than solutions composed of items with a greater weight and, consequently, solutions for lesser capacities are unlikely to be reused.

The objective is not to explore this distribution and its behaviour with different parameters, we only intend to show that it favors the B\&B approach over the DP.
The author of this thesis proposes a subset of the BREQ instances which the only parameters are~\(n\) and the seed for the PRNG, with the other instance parameters being computed over the value of~\(n\).
This instance distribution will be referred to as the BREQ 128-16 Standard: a BREQ distribution in which~\(c = 128 \times n\),~\(p_{min} = w_{min} = 1\),~\(w_{max} = c\) and~\(p_{max} = 16 \times w_{max}\).
The PRNG seed is used to create~\(n\) unique random weight values between~\(w_{min}\) and~\(w_{max}\) (the random number distribution is uniform); the profit values for each weight are computed using the first formula presented at this section (and the~\(w_{max}\) and~\(p_{max}\) values for that~\(n\)).

The reasoning for the choices made when defining the BREQ 128-16 Standard follows.
There was no reason to restrict the~\(w_{min}\) to~\(w_{max}\) interval to be smaller than~\(c\) (there are~\(c\) distinct valid weight values).
The constant 128 used to compute the capacity~\(c\) for a~\(n\) value was chosen as the first power of two higher than a hundred.
Consequently, less than 1\% of all possible items will appear in an instance, making instances generated with different seeds significantly different.
The~\(p_{min}\) to~\(p_{max}\) value interval was chosen to be sixteen times bigger than the~\(w_{min}\) to~\(w_{max}\) interval to alleviate the effects of rounding the profit value to an integer value (it would not need to be done if the profit was a floating point number with reasonable precision).
The efficiency of the items in a BREQ distribution will vary between zero and~\(\frac{p_{max}}{w_{max}}\), so if~\(p_{max} = w_{min}\) the efficiency would vary between zero and one, giving more relevance to the rounding.
Finally, for the biggest~\(n\) value that we were interested in testing (\(2^{20}\)), the highest possible value of an item profit is~\(2^{31} = 2^{20}\times128\times16\), what keeps the values smaller than 32 bits.

The BREQ 128-16 Standard allows us to create a simple benchmark dataset, in which we only need to worry about varying~\(n\) and the seed.
We propose a benchmark with a hundred instances, with all combinations of~\(n = 2^{11} = 2048,~2^{12},~...,~2^{20} \approx 10^6\) (ten~\(n\) values), and ten different seed values.
We will refer to it as the BREQ 128-16 Standard Benchmark (or BREQ 128-16 SB).

\section{Other distributions}

While the artificial item distributions presented in~\cite{pya} are used in the experiments of this work, some other artificial distributions are ignored.
The reasons for not using the uncorrelated distribution were extensively discussed in this chapter.
However, no reason was presented for not using the \emph{weakly correlated} distribution, presented in~\cite{mtu1},~\cite{mtu2},~\cite{babayev} and~\cite{eduk}, or the \emph{realistic random} distribution, presented in~\cite{eduk}.

The main reason for using the artificial distributions described in~\cite{pya} was that questions about the performance of the algorithms in the most recent benchmark dataset for the UKP would most certainly arise. Moreover, these experiments answer these questions preemptively.
Not using the dataset would raise unfounded suspicion about this choice.
The BREQ instances are a special case, as their purpose is exactly showing that it is easy to design a items distribution that favors one approach over another. 

