\documentclass{elsarticle}
%\usepackage{helvet}
\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{url}
%\usepackage{longtable}
%\usepackage{subfigure}
%\usepackage{mathptmx}      % use Times fonts if available on your TeX system
\usepackage{amssymb}
%\usepackage{pdf14}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{graphicx}
%\usepackage{tipa}
%\usepackage{mdwlist}
%\usepackage{booktabs}
%\usepackage{url,color}
\usepackage{xcolor}
\usepackage{hyphenat}
%\usepackage{rotating}
%\usepackage{latexsym}
%\usepackage{tabularx}
%\usepackage{algorithmic}
\usepackage[hmargin=3cm,vmargin=3cm]{geometry}
%\renewcommand{\familydefault}{\sfdefault}

\setlength\parindent{0pt}

\begin{document}
\pagestyle{empty}

\vspace{2cm}

\begin{flushright}
   \begin{minipage}{7cm}
      Henrique Becker \\
      Instituto de Informática - UFRGS \\
      Av. Bento Gonçalves, 9500. \\
      91501-970 Porto Alegre - RS - Brazil \\
      E-mail: hbecker@inf.ufrgs.br \\
   \end{minipage}
\end{flushright}

\begin{flushleft}
November, 24$^{\text{th}}$ 2016.

\vspace{1.5cm}

Dear Professor José Fernando Oliveira, \\
Co-ordinating Editor of the European Journal of Operational Research
\end{flushleft}

\medskip
First of all, we would like to thank all reviewers for their comments on our paper ``An empirical analysis of exact algorithms for the unbounded knapsack problem''.
We are thankful for both the praise and the constructive criticism, and if we focus on the criticism in the next pages is only to take less of your time.
We would also like to thank the editor for giving the opportunity of sending a revised version of the paper for possible publication in the European Journal of Operational Research.
We have addressed all questions and issues raised by the reviewers and the editor, which are discussed in the report enclosed below.
For the convenience of the reviewers, their questions and requests are quoted, numbered and italicized, and excerpts from the revised paper which address the request are colored in blue and quoted.

\medskip

\begin{flushleft}
Yours Sincerely,\\
Henrique Becker (on behalf of all authors)
\end{flushleft}

\newpage

\section{Editor}

\textbf{Request \#1:} ``\textit{The most critical comments made me doubt whether I should reconsider your paper, so I need convincing arguments from you and a thorough revision to counterbalance the critical comments.}'':

\textbf{Our answer:} We are grateful for the opportunity to improve our paper and send it to be reconsidered for publication in EJOR. We believe to have provided a thorough revision. For the convincing arguments, we would like to point to our answer to the \#1 request of the \#3 referee.

\textbf{Request \#2:} ``\textit{Please, noticed that the paper `BPP and CSP: Mathematical models and exact algorithms' is now published in EJOR and that the reference needs to be updated.}'':

\textbf{Our answer:} Thanks for the catch. The reference is now updated.

\section{Anonymous Referee \#1}

\textbf{General commentary:} ``{\itshape
The manuscript deals with the Unbounded Knapsack Problem (UKP), a knapsack problem in which no limitation is imposed on the item availability. After a brief reminder on dominance and periodicity, the authors compare various algorithms proposed in the literature on existing data sets to outline the best method.

Overall, I think the manuscript is very interesting and useful for the reader who wants to solve the UKP. I believe it deserves to be published after some minor changes (with one request of additional experimental result)
}''

\textbf{Request \#1:} ``\textit{Page 1, line 49. The authors start their introduction by defining the UKP with respect to the 0-1 knapsack problem. In my opinion, the UKP is as famous as the 0-1 knapsack problem, so the reader who already knows about knapsack problems does not need this definition, and the reader who does not will not be helped. I would suggest to add first a small sentence explaining in plain words what is the UKP, and then detail its relations with the other knapsack problems.}'':

\textbf{Our answer:} Good point. The improved paragraph follows: ``\textcolor{blue}{The objective of this work is to provide an extensive comparison of the exact algorithms for solving the Unbounded Knapsack Problem (UKP). Given the weight capacity of a knapsack and a collection of items (each with a weight and a profit value), the UKP consists in choosing how many copies of each item will be packed in the knapsack to maximize the profit carried by it while respecting its weight capacity. The UKP is similar to the Bounded Knapsack Problem (BKP) and the 0-1 Knapsack Problem (0-1 KP). The only difference between the UKP and the BKP (or the 0-1 KP) is that the UKP has an unlimited quantity of each item available. The UKP is a weakly NP-Hard problem, as are the BKP and the 0-1 KP.}''.
\medskip

\textbf{Request \#2:} ``\textit{Page 2, line 18. The best lower bound for the bin packing problem (BPP) is indeed the continuous relaxation of the set covering formulation, and it has an exponential number of variables. There also exists pseudo polynomial formulations (arc-flow for example) with pseudo polynomial number of variables for which it has recently be proved that the continuous relaxation is as good as the set covering one.}'':

\textbf{Our answer:} Thanks for the pointer. We added this information in the end of the paragraph to provide the reader a better contextualization. The excerpt follows: ``\textcolor{blue}{
However, recently \cite{eq_lb_delorme} proved that a pseudo-polynomial formulation (dynamic programming-flow formulation) is equivalent to the set covering formulation and, therefore, provides the same lower bounds for the problem.
}''.
\medskip

\textbf{Request \#3:} ``\textit{Page 3, line 41-42-43. If two items have the same profit to weight ratio and the same weight, then the two items are the same right?}'':

\textbf{Our answer:} Correct. Some instance generators do not guarantee unique items, and in our experiments, we found that is faster to let the algorithms keep only the first of the duplicates by their own internal mechanisms for simple dominance (for an example, the OSO deals with equal items by overwriting the cell with the same weight in \(O(1)\), in a step it would execute anyway) than to add a generic removal phase for all algorithms.
The passage alludes to that.
We refer to the `item list' of an instance and not the `item set' because of that.
We changed the text to avoid confusion: ``\textcolor{blue}{
If more than an item has both previously stated characteristics (i.e., they are equal), then the first item with both characteristics in the items list is the best item.
Such distinction is made because the instance generators do not guarantee unique items, and is often faster to let the algorithms themselves deal with the replicas than running a preprocessing phase.
}''
\medskip

\textbf{Request \#4:} ``\textit{Page 4, line 30-45. The interest of the figure is to help the reader to understand. I believe the authors can add a short sentence after each kind of domination to refer to the figure. (5,5) simple dominate (6,1). (5,5) multiple dominate (12, 9) with alpha = 2 \dots}'':

\textbf{Our answer:} Good idea. The four phrases added in the end of each paragraph explaining a dominance relation were: ``\textcolor{blue}{For an example, \((5, 5)\) simple dominates \((6, 1)\), as shown in Figure 1.}'', ``\textcolor{blue}{For an example, \(\alpha = 2\) copies of \((5, 5)\) multiple dominates \((12, 9)\).}'', ``\textcolor{blue}{For an example, solution \(\{(5, 5), (5, 5), (3, 2)\}\) collective dominates \((14, 11)\).}'', and ``\textcolor{blue}{For an example, \((5, 5)\) threshold dominates \(beta = 2\) copies of \((3, 2)\)}.''.
\medskip

\textbf{Request \#5:} ``\textit{Page 12, line16. The BPPLIB website indicates `If you need to refer to material taken from this library, please cite M. Delorme, M. Iori, and S. Martello. BPPLIB: A library for bin packing and cutting stock problems. Optimization Letters, 12(2):235-250, 2018.'}'':

\textbf{Our answer:} My fault. Fixed.
\medskip

\textbf{Request \#6:} ``\textit{Overall, I believe the results are very interesting and well detailed, but it would be better for the reader to have an additional unique table that summarizes the results obtained by all algorithms on all datasets, maybe with just the time and the number of instances solved to give a general overview.}'':

\textbf{Our answer:} Thank you for the suggestion. The table summarized well the overall results. It can be found on page XXX of the revised paper.
\medskip

\textbf{Request \#7:} ``\textit{Page 23, line 22. `Algorithm', the `m' was forgotten.}'':

\textbf{Our answer:} Fixed.
\medskip

\textbf{Request \#8:} ``\textit{The authors tried most of the DP and B\&B algorithms that were proposed in the literature but they did not try model (1)-(3) with an ILP solver.} [...] \textit{It would be a valuable contribution of the manuscript to test this basic ILP and show empirically what can be earned by using one of the DP or B\&B of the authors' repository instead.}'':

\textbf{Our answer:} The authors added a paragraph describing CPLEX and Gurobi in the final of the methods section and created a subsection in the experiments for testing them against the reduced PYAsUKP dataset (in the same fashion as the section comparing the MTU implementations). Also, the sections for other datasets were updated to reflect the addition of the CPLEX results. Adding these results modified many parts of the paper to be quoted here.
\medskip

\section{Anonymous Referee \#2}

\textbf{General commentary:} ``{\itshape
This paper addresses the unbounded knapsack problem, in which the objective is to select a set of items (duplicates are allowed) to maximize the total profit. Each item has a given profit and a given weight and the weight of the selected items cannot exceed a given capacity.

The paper succinctly reviews properties of the problem and the most relevant methods for that problem from the literature (also proposing a variant in an existent one), justifying their choice of methods. Results on extensive computational experiments are reported. It is worth noting that the authors made available online all the codes used, allowing the reproducibility of the experiments.

The unbounded knapsack problem is relevant problem (at least, as a subproblem, as in a set of instances tested in the paper) and the comparison between dynamic programming based and branch-and-bound is potentially useful for related problems. The paper is well written and rigorous. I have a few minor comments that I would like the authors to consider.
}''

\textbf{Request \#1:} ``\textit{A more general description of the methods in a couple of paragraphs would widen the potential readers of the paper. In particular the general dynamic programming approach (mentioning the states, stages, recursive function) and the general branch-and-bound approach (mentioning the branching scheme, bounds used commonly, ...).}'':

\textbf{Our answer:} The description of MTU1 was expanded to encompass what was asked (as MTU1 and MTU2 are the only two B\&B algorithms for the UKP considered, and MTU2 uses MTU1 inside itself). A short description of the generic recursive function, states and stages of the DP for UKP was also added. The rewritten MTU1 follows: ``\textcolor{blue}{
MTU1 is a B\&B algorithm for the UKP~\citep{mtu1}. %with worst-case space complexity \(O(n)\)\cite{mtu1}.
In the enumeration tree of MTU1, each node at depth~\(0 \leq d < n\) has~\((c'/w_{d+1}) + 1\) childs (where \(c'\) is the remaining capacity at the current node), with each of those childs representing a valid amount of copies of the item~\(d+1\) being packed in the knapsack (including zero copies).
In practice, by virtue of being depth-first, MTU1 has no explicit tree but instead make changes directly over the current solution, which represents the current branch from the root to a leaf.
Items are added to and removed from the solution to simulate the tree traversal.
The exploration order favors the childs representing higher amounts before lower amounts.
As the items are sorted by efficiency, this means the first node after root will represent the maximum amount of copies of the best item, and the path to the first leaf will be the solution given by the classic greedy heuristic which packs the most efficient item that yet fits the knapsack until there is no item that fits the knapsack. So, the first solution found is a common lower bound for the UKP.
The upper bound computed by MTU1 in each node is the continuous relaxation of the problem with the already set variables fixed, which is solved by multiplying the efficiency of the most efficient item not yet fixed by the remaining capacity.
}''.
The other paragraph consists in: ``\textcolor{blue}{
A description of the recursion, states, and stages in which the DPs for the UKP are based follow.
Given \(opt(y)\) denotes the optimal solution value for capacity~\(y\), the recursion for the UKP can be written as \(opt(y) = max \{0, p_1 + opt(y - w_1), p_2 + opt(y - w_2), \dots, p_n + opt(y - w_n)\}\) (where \(\forall~y < 0.~opt(y) = -\infty\)).
The UKP has a single state, that is the remaining knapsack capacity~\(y\).
Different from 0-1 KP and BKP, the UKP has no need to take in account which items were already used at each decision as there is an unlimited amount of copies available for each item.
This difference allows UKP to need only \(O(c)\) memory, instead of \(O(nc)\) as in 0-1 KP and BKP.
For each capacity~\(y = c - w_s\) (where \(s\) is a valid solution), there is a decision point, consequently, the number of stages is not exact but can go up to~\(c\) (as in the case of \(w_{min} = 1\)).
}''.
\medskip

\textbf{Request \#2:} ``\textit{A related issue is the explanation of algorithm 1 for which a text description should be given providing an overview of the algorithm.}'':

\textbf{Our answer:} A single-paragraph overview was provided. It follows: ``\textcolor{blue}{The ordered step-off (OSO) is a dynamic programming algorithm for the UKP~\citep{gg-66}.
The OSO is described in Algorithm 1, a complementary overview follows.
The arrays \(g\) and \(d\) store, respectively, the profit value of generated solutions and the index of the most efficient item present in such solutions, together they can be seen as a solution pool indexed by solution weight.
The gist of the algorithm is: the solution pool is initialized with all single-item solutions; the solution pool is iterated by weight order; for each solution, the solution pool is expanded with new solutions, each new solution is a copy of the current solution plus an extra item; this process enumerates all undominated solutions and it returns the optimal profit value.
The algorithm skips the creation of new solutions from old solutions already known to be dominated (lines~11~to~13), discards some dominated solutions created (e.g., if two or more solutions share the same weight, the algorithm keeps only one of these which is tied for highest profit), and avoids considering symmetric solutions (by restricting the loops up to \(d[y]\), the algorithm only considers the permutation of the solution in which the items are added in order of efficiency).
When the algorithm finishes executing, \(opt\) contains the optimal profit value and for every \(g[y] > 0\) there is a solution with: weight~\(y\), profit value \(g[y]\), and the index of the last (and most efficient) item added~\(d[y]\).}''
\medskip

\textbf{Request \#3:} ``\textit{A (an informal) definition of what are subset and strongly correlated instances (page 6) should be given.}'':

\textbf{Our answer:} Done. The supplemented description and its immediate context follow: ``\textcolor{blue}{[...] This change of tiebreaker reduced the run times of the algorithm over subset-sum and strongly correlated instances by orders of magnitude. Subset-sum instances are UKP instances with items respecting \(\forall i.~p_i = w_i\), while strongly correlated instances have items respecting \(\forall i.~p_i = w_i + \alpha\) (where \(\alpha\) is a small positive integer value which is the same for the whole instance). Therefore, the performance gain can be explained by the fact that, in subset-sum instances, all solutions with the same weight have the same profit and, in strongly correlated instances, all solutions with the same weight and the same number of items have the same profit.}''
\medskip

\textbf{Request \#4:} ``\textit{It is not clear to me that the TSO outperforms the other methods (in particular EDUK2) in instances `realistic random' as implicit in the first sentence of section 6 and in the general conclusions.}'':

\textbf{Our answer:} The first sentence of section 6 (General Discussion and Conclusions) introduce `mean time' as our performance criteria of choice. In Section 5.2 (Results on the Realistic Random Dataset), the Figure 5 indeed does not make the mean time clear, for this reason, in the text, it is mentioned that ``\textcolor{blue}{Despite EDUK2 solving some instances orders of magnitude faster than the other algorithms (especially in the larger instance sizes), the mean run time of EDUK2 (8.51 seconds) is higher than TSO mean run time (5.36 seconds).}'', this is echoed by Table A.4 (TODO: CHECK NUMBER) (in which TSO mean time is lower than EDUK2 lower time in the larger instances of the realistic random subset and, consequently, in the dataset as a whole).
\medskip

\section{Anonymous Referee \#3}

\textbf{General commentary:} ``{\itshape
The submitted paper gives a comprehensive computational investigation on the classical unbounded knapsack problem (UKP), i.e. a knapsack problem with unlimited copies available for each item.

The paper starts with a short review of the well-known concepts of dominance and periodicity. This is followed by the only algorithmic contribution: An ancient dynamic programming approach due to Gilmore and Gomory (1966) is excavated and improved by a more effective tie-breaking rule, which is invoked whenever a previously detected solution with (profit, weight) is recomputed. This tie-breaking rule gives preference to solutions with a more efficient item recently added. It turns out that this simple step yields a considerable computational improvement for test instances with a subset-sum type or highly correlated structure. However, this small observation is the only original contribution.

The remainder of the paper (i.e. most of it) contains a detailed, thoughtful and highly professional empirical investigation of exact solutions methods for UKP. In Section 3 it is discussed that the well-known periodicity brings only very limited computational gains. Moreover, a side-remark in an earlier paper (Poirriez et al., 2009) is refuted by a counterexample.

The authors spend a lot of effort to argue how they selected the algorithms used in their computational experiments and why they discarded others. This is an important issue which often remains insufficiently treated in other papers. Then they describe the details of the implementations, compile an impressively long list of benchmark instances from all available sources in the literature, and describe the computational environment. The design and reporting of the experiments are done in a very careful, professional and satisfactory way! The results are discussed in a thoughtful way and interpretations offered, wherever possible. This is much better than just throwing tables on the reader, as it is sometimes done in the literature. As another contribution, the authors generate a new class of test instances (BREQ) which hardly exhibit dominance and thus are difficult for dynamic programming based approaches, but easier for Branch \& Bound algorithms.

The main message resulting from the extensive experiments seems to be that the ``winner'' of the computational experiments depends heavily on the type of data generation. And surprisingly, the slightly improved, ancient algorithm from 1966 gives better performance than the ``state-of-the-art'' algorithm EDUK2 (Poirriez et al., 2009) for most data sets. There remains some caveat to this statement, since EDUK2 was implemented in OCaml while all other algorithms were implemented in C++. It is not clear to me, if this might have a relevant impact.

Summarizing, the authors present a carefully prepared computational study showing the surprising performance of an ancient algorithm which gains a lot from a small improvement step. They bring to attention several general aspects of computational experiments.
}''

\textbf{Request \#1:} ``\textit{This work might well be published somewhere, but I believe that it is not strong enough for a publication in EJOR, which strives to be the flagship of the OR-community. The study does not bring sufficiently new contributions for the special topic of UKP or general outcomes of wider interest. Therefore, I suggest submitting the paper to a less highly ranked journal than EJOR.}'':

\textbf{Our answer:} 
The authors believe the scientific value of a paper comes from their original contributions to knowledge.
However, we do not believe that only new algorithms or improvements to existing algorithms are to be considered as original contributions to knowledge.

Our paper cover the UKP which is a classical problem and common subproblem.
While widely studied, the UKP lacked a comprehensive empiric analysis and contextualization which helped the OR practitioner to chose the algorithm with the best practical performance.
A context in which algorithms with the same worst-case complexity can have radically different performances in practice, and theoretical improvements as the ones presented by GFDP can, in fact, result in worse performance for the literature daatasets.

In the beginning of our work we proposed a new algorithmn, that we published in~\cite{sea2016}.
While writting the extended version of that paper, aiming a journal publication of the work, we deepened the literature review and realized that that an algorithm of 1966 was very similar than ours.
That algorithm, 52 years old, maybe was forgotten because: a) its pseudocode is not structured (go to statement) making it difficult to understand or compare to more recent algorithms; b) it does not mention modern dominance relations, which are the source of the efficiency of the latest algorithms for solving UKP; c) it does not emphasize how much its performance is superior to the naïve DP algorithm.
We are very happy, and also impressed, with our finding.
We believe that this contribution by itself is of a huge impact, specially when we consider a classic problem as the unbounded knapsack problem, that is being approached for several research groups, along decades. 
Moreover, we decided to include more algorithms, instances and tests to show this clearly, to all this information be self contained in one single paper, and this resulted in this paper we are proposing.

Finally, the paper brings other minor contributions that, while alone cannot justify a paper, together are considerable:
a) the concept of \emph{solution dominance} which generalizes all previously proposed dominances; 
b) the concept of \emph{partial solution dominance} as a competitive alternative to the state-of-the-art application of simple, multiple, collective, and threshold dominances;
c) a discussion on how tigther bounds for periodicity will not help to improve the performance of state-of-the-art algorithms;
d) a new item distribution, rich only in threshould dominance, and how it favors one solving approach in relation to other (i.e., B\&B over DP);
e) a study of the item distribution evolution in CSP/BPP pricing problems and some of its implications (how it biases against some UKP-solving approachs, how it is affected by authors trying to create `hard' CSP/BPP instances);
f) finally and thanks to referee \#2, that commercial solvers as CPLEX are not a good solution to solving CSP/BPP pricing problems (in comparison to both DP and specialized B\&B) but are better than specialized B\&B for some instances of the literature.
\medskip

\textbf{Request \#2:} ``\textit{The following reference considers an algorithm by Landa (2004) which might provide interesting results: Hu T.C., Landa L., Shing MT. (2009) The Unbounded Knapsack Problem. In: Cook W., Lovász L., Vygen J. (eds) Research Trends in Combinatorial Optimization. Springer. doi.org/10.1007/978-3-540-76796-1\_10}'':

\textbf{Our answer:}
We would like to thank the referee for bringing an overlooked survey and technical report to our knowledge.
The main point of our paper is to provide a comprehensive empirical analysis of exact UKP algorithms.
Unfortunately, as our time and the length of the paper are limited, we need to determine boundaries for the paper scope.
Landa presents three algorithms (Sage-1D, Sage-2D and Sage-3D) for the UKP.
Only Sage-3D can be fairly considered as an exact algorithm for the UKP as it gives an optimal solution independently of the knapsack size (denoted by~\(c\) in our paper).
The other two algorithms do not return an optimal solution for some range of capacities.
We use~\(w_b\) and~\(p_b\) to denote, respectively, the weight and the profit of the best item.
The amount of items is denoted by~\(n\).
Sage-3D needs~\(O(n w_b p_b)\) memory space and executes at least~\(n w_b p_b\) instructions (if anything else, at least to initialize memory).
Such amount of memory is prohibitively high for many instances.
The benchmark datasets include classes of instances in which: the profit values are at least an order of magnitude greater than the weight values; the best item is always the one with highest weight; the highest weight among the items is close to~\(c\); any combination of the previous characteristics.
For some instances, a memory cost of~\(nc\) would already be prohibitively high, and~\(n w_b p_b\) can be close to~\(n c^2\) for such instances.
As~\(n w_b p_b\) is also a minimum amount of instructions executed, the algorithm cannot be competitive.
Empirically, the DP algorithms compared in our paper execute far less than~\(nc\) steps (about a small constant number of steps for each~\(c\)) and, in fact, solve UKP faster than running a naive~\(n^2\) steps algorithm for removing simple and multiple dominated items from the item list.
The algorithm holds a considerable theoretical value, but is not adequate to the empirical performance comparison we propose in our paper.
Consequently, we did not implement and run Landa's Sage-3D to add it to the comparison, but we added a mention to it in the section "4.1.1 Algorithms deliberately ignored".
The mention follows: ``\textcolor{blue}{The Sage-3D algorithm from \cite{landa_sage} cited in \cite{ukp_hu_landa_shing_survey} needs~\(O(n w_b p_b)\) memory and time, which is prohibitive for many instances considered and, therefore, was also not included.
The algorithm holds a considerable theoretical value, and its complexity is justified by the fact Sage-3D does not solve the UKP for a specific knapsack capacity, but instead builds a data structure which allows querying the solution for a specific capacity in~\(O(log(p_b))\).}''.
\medskip

\textbf{Request \#3:} ``\textit{p 6, Alg 1: lines 6, 7: The arrays g and d should be properly defined.}'':

\textbf{Our answer:} We changed the referred lines to ``\textcolor{blue}{\(g \leftarrow\) array of profit values with size \(c + 1\), initialized with zeroes}'' and ``\textcolor{blue}{\(d \leftarrow\) array of item indexes with size \(c + 1\), values uninintialized}''.
Also, the request \#2 of referee \#2 asked for a description of the algorithm.
Such description includes the following: ``\textcolor{blue}{The arrays \(g\) and \(d\) store, respectively, the profit value of generated solutions and the index of the most efficient item present in such solutions, together they can be seen as a solution pool indexed by solution weight.} [...] \textcolor{blue}{When the algorithm finishes executing, \(opt\) contains the optimal profit value and for every \(g[y] > 0\) there is a solution with: weight~\(y\), profit value \(g[y]\), and the index of the last (and most efficient) item added~\(d[y]\).}''.
\medskip

\textbf{Request \#4:} ``\textit{p 6,7: The discussion after Algorithm 1 should be improved. Moreover, there is some notational error since \(t \cap \{i\}\)  is either \(\{i\}\) or the empty set.}'':

\textbf{Our answer:} There was a notational error, all \verb+\cap+ (\(\cap\)) in that paragraph should have been \verb+\cup+ (\(\cup\)), thanks for the catch. The discussion after Algorithm~1 was changed in the following ways: the proposal of solution dominance now appears after the algorithm-specific paragraphs, immediately before the proposal of partial solution dominance; the formal description of the partial solution dominance now counts with an introductory paragraph before it and an exemplifying paragraph after it; a paragraph detailing a possible weakpoint of the (R-)OSO mechanism of partial solution dominance is also provided. The affect paragraphs are too many to be copied there.
\medskip

\textbf{Request \#5:} ``\textit{p 9, Sec. 4.1.1.: `The authors believe that [...]' This not an acceptable argument. Find a better line of reasoning.}'':

\textbf{Our answer:} This is a valid criticism.
We now share in the paper the reasons why the algorithms were not considered in the experiments (the reasons why we did believe that).
The changed paragraph follows: ``\textcolor{blue}{The naïve DP algorithm for the UKP~\cite[p.~311]{tchu}, an improved version of it presented in~\cite[p.~221]{garfinkel} and the OSO~\cite[p.~15]{gg-66} are all \(O(nc)\) DP algorithms similar to each other. % These three DP algorithms are \(O(nc)\).
However, OSO does not need to execute \(n\) operations for each distinct \(c\) value and, in practice, will iterate only a small fraction of \(n\) (or even an empty list) for most \(c\) values of most instances.
The other two algorithms \emph{always} execute \(nc\) operations regardless of any instance properties.
Preliminary tests confirmed that OSO dominated the other two algorithms and, consequently, both were not included in our experiments.}''.
\medskip

\textbf{Request \#6:} ``\textit{p.11, l1: `The authors selected on-tenth' How? Randomly?}'':

\textbf{Our answer: } We changed the text and added a footnote to clarify: ``\textcolor{blue}{The authors selected the first one-tenth of the instances for each parameter combination\footnote{\textcolor{blue}{The entire PYAsUKP dataset is available at \url{https://drive.google.com/open?id=0B30vAxj_5eaFSUNHQk53NmFXbkE}. The instances with the same parameter combination are numbered.}} (in total 454) and will refer to it as the \emph{reduced PYAsUKP dataset}.}''.
\medskip

\textbf{Request \#7:} ``\textit{p.11, Sec 4.2.3: Can you state more theoretical properties or arguments about the BREQ instances?}'':

\textbf{Our answer:}
Yes.
This section was, in fact, longer in the first draft of the paper, and we shortened it during the writing process.
We now added back the paragraph below:
``\textcolor{blue}{The optimal solution of BREQ instances is often in the first fraction of the search space examined by B\&B algorithms. Moreover, the lower bounds from good solutions allow B\&B methods to skip a large fraction of the search space and promptly prove optimality. In BREQ instances, the presence of simple, multiple and collective dominance is minimal\footnote{\textcolor{blue}{
If the BREQ formula did not include the rounding, the profit of the item would be a strictly monotonically increasing function of the items weight.
Any item distribution with this property cannot present simple, multiple, or collective dominance.
}}, but threshold dominance is very common: an optimal solution will never include the item~\(i\) two or more times if there is an item~\(j\) such as that~\(\sqrt{2} \times w_i \leq w_j \leq 2 \times w_i\).
Such characteristics lead to optimal solutions comprised of the largest weight items, which do not reuse optimal solutions for lower capacities.
This means that solving the UKP for lower capacities as DP algorithms do is mostly a wasted effort.}''.
\medskip

\textbf{Request \#8:} ``\textit{Most Figures contain dense clouds of overlapping symbols and it is very hard to recognize meaningful evidence (see e.g. Figures 4, 5, 7).}'':

\textbf{Our answer:} The purpose of most figures presented is to stress the difference (or lack of thereof) in the behavior and performance of distinct algorithms. The y axis of the figures is logarithmic time and, consequently, similar times are clumped very closely together. If a figure presents clearly separate clouds, each one of a single symbol, this means the algorithms have a clearly distinct behavior/performance. If different symbols overlap, the methods have similar performance/behavior. We do believe the figures present a better `big picture'/`whole story' notion than tables, and because of that we use them in the article body, but we also present tables for each dataset in the appendix. To reduce the visual pollution and simplify analysis we removed some algorithms from two comparisons and pointed in the text that they had behavior and performance similar to one of the yet presented algorithms. In the Figure~6 of the revised paper (Figure~3 of the first version) which presents results over the realistic random dataset, MTU1 and MTU2 are replaced by CPLEX, and among OSO, TSO, and GFDP only GFDP was kept. In Figure~7 of the revised paper (Figure~4 of the first version) which presents results over the BREQ dataset, TSO and MTU are removed because, respectively, OSO and MTU2 are already presented (CPLEX, however, was added in conformity with request \#8 of referee \#1).
\medskip

\textbf{Request \#9:} ``\textit{p.23, `Also, the trend observed could indicate that for instances with a greater n value, OSO/TSO algorithm would have lower times than MTU1, as their relative difference was diminishing.' This is a very risky, unfounded statement, the two might just as well converge to each other.}'':

\textbf{Our answer:} Noted. The phrase was changed to acknowledge this possibility too. The purpose of the statement (that was to point how the story told by the old data was compatible with our new finding) was also made clearer. The reworked paragraphs follow: ``\textcolor{blue}{Such instances are now too small to consider, and the relative difference between TSO and MTU1 mean times was less than one order of magnitude apart and diminishing. This trend hinted the possibility of the times taken by OSO/TSO and MTU1 converging (or even OSO/TSO taking less time than MTU1) for larger instances (e.g., OSO/TSO algorithm could have a costly initialization process but a better average-case complexity).}''.
\medskip

\textbf{Request \#10:} ``\textit{References: Poirriez et al, 1998: give more details.}'':

\textbf{Our answer:} We are not sure why, but seems as we copied an incomplete reference. Now it is fixed. Thanks for the pointer.

\section{Anonymous Referee \#4}

\textbf{General commentary:} ``\textit{
Work of real interest, specially the introduction of the solution dominance and the design of an algorithm using it.
}''

\textbf{Request \#1:} ``\textit{pg 8 ln 17 UPK \(\rightarrow\) UKP} [...] \textit{Two small typos I have noticed: pg 23 ln 22 algorith \(\rightarrow\) algorithm}'':

\textbf{Our answer:} Typos fixed.

\section{References}
\bibliography{biblio}
%\bibliographystyle{elsarticle-harv}
\bibliographystyle{model5-names}
\biboptions{authoryear}

\end{document}

