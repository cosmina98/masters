<<rr_breq_shared_exp>>=
my_relevel <- function(f) {
  factor(f, levels = c('cpp-mtu1', 'cpp-mtu2', 'eduk', 'eduk2', 'ordered_step_off', 'terminating_step_off', 'mgreendp'))
}

# used by rr and breq
gen_graph_all_alg_discrete_x <- function(csv) {
  alg_labels <- c(
    'cpp-mtu1'             = 'MTU1 (C++)',
    'cpp-mtu2'             = 'MTU2 (C++)',
    'eduk'                 = 'EDUK',
    'eduk2'                = 'EDUK2',
    'ordered_step_off'     = 'Ordered Step-Off',
    'terminating_step_off' = 'Terminating Step-Off',
    'mgreendp'             = 'MGREENDP'
  )
  # I thought of putting one of the legends in an unused area of the plot and
  # another below the plot, but separating legends with ggplot2 needs a hack:
  # https://stackoverflow.com/questions/13143894/how-do-i-position-two-legends-independently-in-ggplot
  # which is too much effort for a little gain.
  p <- ggplot(csv,
              aes(x = n,# * (0.75 + 0.50*(as.numeric(algorithm) - 1)/6),
                  y = internal_time,
                  color = algorithm,
                  shape = algorithm)) + 
    #geom_jitter() +
    #stat_bin2d(binwidth = 0.25, geom="text", aes(label=..count..), position = position_dodge(0.7)) +
    #geom_point() + geom_count() +
    stat_bin_2d(geom = "point",
                aes(size=..count.., alpha = 0.80),
		position = position_dodge(0.75)) +
    scale_alpha(limits = c(0, 1), guide = FALSE) +
    # remove default fill stat_bin_2d legend
    scale_fill_gradient(guide = FALSE) +
    scale_size_area(breaks = c(2, 5, 8),
                    guide = guide_legend(title = "# of points",
                                         title.position = 'top',
                                         direction = 'horizontal',
                                         title.hjust = 0.5)) +
    scale_y_continuous(trans = "log10",
                       breaks = c(10^-3, 10^-2, 10^-1, 1, 10, 60, 600, 1800),
                       labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) +
    scale_colour_manual(name = "Algorithm",
                       labels = alg_labels,
                       values = c('cpp-mtu1' = 'black', 'cpp-mtu2' = 'darkgrey', 'eduk' = 'black', 'eduk2' = 'darkgrey', 'ordered_step_off' = 'black', 'terminating_step_off' = 'darkgrey', 'mgreendp' = 'black')) +
    scale_shape_manual(name = "Algorithm",
                       labels = alg_labels,
                       values = c('cpp-mtu1' = 16, 'cpp-mtu2' = 16, 'eduk' = 17, 'eduk2' = 17, 'ordered_step_off' = 15, 'terminating_step_off' = 15, 'mgreendp' = 4)) +
    ylab('Time to solve (seconds, log10 scale)') +
    xlab('Instance size (number of items, log2 scale)') +
    theme(legend.position = 'bottom')
  #ggsave('realistic_random.pdf', width = 0.80*210, height = 0.5*297, unit = 'mm')
  return(p);
}
@

<<rr_shared>>=
rr_get_n <- function(fname) {
  as.numeric(gsub(".*_n([1-9][0-9]+)-.*", "\\1", fname))
}

rr_csv <- read.csv("../data/realistic_random.csv", sep = ";")
# the duration of runs killed by timeout will be the timeout
rr_csv$internal_time[is.na(rr_csv$internal_time)] <- 1800
rr_csv$n <- rr_get_n(rr_csv$filename)
rr_csv$algorithm <- my_relevel(rr_csv$algorithm)
@

<<breq_shared>>=
breq_get_n <- function(fname) {
  as.numeric(gsub(".*-n([1-9][0-9]+)-.*", "\\1", fname))
}

breq_csv <- read.csv("../data/128_16_std_breqd.csv", sep = ";")
# the duration of runs killed by timeout will be the timeout
breq_csv$internal_time[is.na(breq_csv$internal_time)] <- 1800
breq_csv$n <- breq_get_n(breq_csv$filename)
breq_csv$algorithm <- my_relevel(breq_csv$algorithm)
@

<<mtu_shared>>=
mtu_csv <- read.csv("../data/mtus_pya.csv", sep = ";")
mtu_csv$internal_time[is.na(mtu_csv$internal_time)] <- 1800
@

<<fast_shared>>=
fast_csv <- read.csv("../data/pya_ukp5.csv", sep = ";")
fast_csv$X <- NULL # necessary for complete.cases to work
@

<<env_shared>>=
env_data <- read.csv("../data/env_influence.csv", sep = ";")
env_data$X <- NULL
@

<<to_rework_shared>>=
fix_names <- function(x) {
  x <- gsub('all', 'ALL', x)
  x <- gsub('ss2', 'SS', x)
  x <- gsub('saw', 'SAW', x)
  x <- gsub('nsds2', 'PP', x)
  x <- gsub('sc', 'SC', x)
  x <- gsub('hi', 'WCD', x)
  x
}
@

<<csp_shared>>=
csp_csv <- read.csv("../data/csp_6195.csv", sep = ";")
csp_csv$X <- NULL
csp_csv$dataset <- gsub('(.*)/.*', '\\1', csp_csv$filename)
csp_csv$dataset <- gsub('Randomly_Generated', 'Random', csp_csv$dataset)
csp_csv$dataset <- gsub('ANI_AI', 'ANI AI', csp_csv$dataset)
csp_csv$dataset <- factor(csp_csv$dataset)
# forgot to add sort_time to knapsack time in the chart generation codes, now
# changing it here to avoid changing many places
csp_csv$pricing_total_time <- csp_csv$hex_sum_knapsack_time + csp_csv$hex_sum_sort_time
@

\section{Results and Preliminary Analyses}
\label{sec:exp_and_res}

The experiments are split into five subsections.
Each section address one dataset and the results of running some selected algorithms over them.
Due to the diversity of experiments, the results of each individual experiment is presented with its immediate analysis.
The big picture painted by all the experiments is discussed in \autoref{sec:discussion}.

\subsection{PYAsUKP dataset}
\label{sec:pya_exp}

The experiment presented in this section is an updated version of the experiment first presented in~\cite{sea2016}.
In this version GREENDP is considered, UKP5 is replaced by the Terminating Step-Off (TSO), and all runs were executed serially.
The exact same 4540 instances were used.

\begin{figure}[H]
\caption{Benchmark using the PYAsUKP dataset and the Terminating Step-off, GREENDP and EDUK2 algorithms. No timeout. The instance classes acronyms stand for: Postponed Periodicity (PP); Strongly Correlated (SC); Subset-Sum (SS); and Without Collective Dominance (WCD). SAW is not an acronym. EDUK was not included because EDUK2 superseeds EDUK. The ordered step-off run times were ommited because they were too similar to the terminating step-off run times. The GREENDP run times over subset-sum instances are not displayed because GREENDP fails if the two most efficient items have the same efficiency, which always happens with subset-sum instances.}
\begin{center}
<<pya_fast_fig>>=
fast_plot <- function(t) {
  inst_types <- c('all', 'hi', 'nsds2', 'saw', 'sc', 'ss2')

  data <- t[complete.cases(t), ]
  data$type <- sapply(data$filename, (function (f) {strsplit(as.character(f), "_")[[1]][1] }))
  data$type <- factor(data$type, levels = inst_types)

  data <- filter(data, algorithm != 'ordered_step_off')
  data <- group_by(data, filename) %>%
    mutate(mean_methods_time = mean(internal_time))

  data_copy <- data
  data$type <- factor('all', levels = inst_types)

  data <- rbind(data, data_copy) %>% arrange(mean_methods_time)
  data$filename <- factor(data$filename,
        levels = unique(data$filename))

  #data$algorithm <- gsub('ordered_step_off', 'O.S.', data$algorithm)
  data$algorithm <- gsub('terminating_step_off', 'Terminating Step-Off', data$algorithm)
  data$algorithm <- gsub('pyasukp', 'EDUK2', data$algorithm)
  data$algorithm <- gsub('mgreendp', 'MGREENDP', data$algorithm)
  data$type <- fix_names(data$type)
  shape_conv <- c('Terminating Step-Off' = 15, 'EDUK2' = 16, 'MGREENDP' = 17)

  ggplot(data,
         aes(x = as.numeric(filename),
       y = internal_time,
       color = algorithm,
       shape = algorithm)) + 
    geom_point() +
    scale_y_continuous(trans = 'log10',
           breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
           labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
  #  ggtitle('Benchmark with the 4540 PYAsUKP instances') +
    ylab('Time to solve (seconds, log10 scale)') +
    xlab('Instance index when sorted by average time to solve\n(average between the algorithms)') +
    scale_colour_grey() +
    scale_shape_manual(name = "algorithm", values = shape_conv) + 
    theme(legend.position = 'bottom') + facet_wrap( ~ type, ncol = 3)
}

fast_plot(fast_csv)
@
\end{center}
%\legend{Source: the author.}
\label{fig:pya_fast}
\end{figure}

Figure~\ref{fig:pya_fast} shows that, except for Strong Correlated (S.C.) and S.S., the run times of the EDKU2 runs often show two main trends.
The first trend starts at the left of the x axis and covers the terminating step-off and MGREENDP (i.e. no run of the terminating step-off or MGREENDP has a run time above this trend).
The second trend begins at middle/right of the x axis and is composed of run times significantly smaller than UKP5 and MGREENDP.
Both trends merge in a single trend that points to the top right corner of the chart.
UKP5 and MGREENDP present similar run times that form plateaus.
Those plateaus aggregate instances that take about the same time to solve by UKP5/MGREENDP.

The authors believe that the second EDUK2 trend is formed by instances where the B\&B preprocessing phase had considerable success (stopped the computation or used bounds to remove items before executing the DP).
This would also explain the lack of this trend at the S.C. and S.S. instance classes, where such bounds and tests have less effect (the S.S. instances do not have different efficiency between items and, consequently, are little affected by bounds).
When EDUK2 B\&B phase do not solve the instance or help to greatly reduce the amount of states, the run times of the EDUK2 DP phase are greater than the UKP5 and MGREENDP times.

The plateaus formed by the UKP5 and MGREENDP runs show very little variation of the run times among some groups of instances.
A closer examination of the data reveals that the instance groups (plateaus) aggregate instances with the same number of items (for the same distribution), or instances with different number of items that are a magnitude smaller (or greater) than the numbers of items of the plateau(s) above (or below).
This behaviour shows that UKP5 and MGREENDP1 are little affected by the specific items that constitute an instance, and that the instance size and distribution are good predictors of the UKP5/MGREENDP run time.

EDUK2 seems to have a much greater variation between the run times for instances with similar number of items of the same distribution.
We can see a line with a slope of about 45 degrees close to the top right corner of the charts.
This line is over a logarithmic y axis, so it is a huge variation compared to the UKP5 plateau below it, which is solving the same instances (these instances, as we have seen above, have similar number of items).

The run times of TSO and MGREENDP are similar, except for the SAW instances, in which MGREENDP performed considerably worse than TSO.
The authors believe that this is due to: SAW CONTINUE HERE
The first factor is a characteristic of the optimal solutions from the SAW and S.C. instances.
The optimal solutions of these distributions are composed of about \(\frac{c}{w_b}\) copies of the best item and a single copy of another item.
This happens because, in such distributions, the smallest item is the best item (as already pointed out in Sections~\ref{sec:sc_inst} and~\ref{sec:saw_inst}).
This characteristic gives a big importance to the best item in those instances, and together with the next two factors, it explains the algorithms behavior.

Solving the DP subproblems without the best item weakens the effect of the solution dominance applied by MGREENDP (also used by UKP5).
Some solutions that would be never be generated in UKP5 will be generated in MGREENDP, since better solutions (using the best item) do not exist to dominate those inferior solutions.

In summary, for the instances of the PYAsUKP dataset, UKP5 and MGREENDP often need less time than EDUK2 to solve an instance.
PYAsUKP solves some instances in less time than UKP5 and MGREENDP, but it takes much more time than UKP5/MGREENDP to solve the greatest instances.
For some distributions, UKP5 takes slight more time than MGREENDP; for other distributions, MGREENDP takes considerably more time than UKP5.
It should also be noted that MGREENDP needs a workaround to work with distributions that allow the highest efficiency to be shared by many items.
%Consequenly, the author of this thesis believes that the choice between using UKP5 or MGREENDP should be based on the distribution that needs to be solved.

\subsection{MTU1 and MTU2 (C++ and Fortran)}
\label{sec:mtu_exp}

The MTU1 and MTU2 implementations in C++ and Fortran were all executed over the reduced PYAsUKP benchmark.

\begin{figure}[H]
\caption{Comparison between MTU1 and MTU2, C++ and Fortran implementations.}
\begin{center}
<<mtu_comp, fig.height=5.5>>=
ukp_time_comp_plot <- function (data) {
  # select columns necessary to the plot, and divide information
  # given in algorithm column in two columns: language and algorithm
  data <- select(data, algorithm, filename, internal_time)
  data$language <- gsub("cpp-mtu[12]", "C++", data$algorithm)
  data$language <- gsub("fmtu[12]", "Fortran", data$language)
  data$language <- factor(data$language, levels = c('C++', 'Fortran'))
  data$algorithm <- gsub(".*1", "MTU1", data$algorithm)
  data$algorithm <- gsub(".*2", "MTU2", data$algorithm)
  data$algorithm <- factor(data$algorithm, levels = c('MTU1', 'MTU2'))

  # compute the mean of the internal_time (for the same filename but
  # with distinct algorithms), and sort the rows by this mean
  data <- group_by(data, filename) %>%
          mutate(fname_mean_time = mean(internal_time)) %>%
	  arrange(fname_mean_time)
  # make filename a factor so as.numeric of a filename will give its position
  # in the ordering described above
  data$filename <- factor(data$filename, levels = unique(data$filename))
  # plot the graph, no change in data
  p <- ggplot(data,
              aes(x = as.numeric(filename),
	          y = internal_time,
		  color = language)) +
    geom_point(shape = 3) +
    scale_shape_identity() +
    scale_colour_manual(values = c('C++' = 'black', 'Fortran' = 'darkgrey')) +
    scale_y_continuous(trans = 'log10',
                       breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
                       labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
    ylab("Time to solve\n(seconds, log10 scale)") +
    xlab("Instances sorted by mean time to solve\n(mean between the four implementations) ") +
    theme(legend.position = 'bottom') +
    facet_wrap(~ algorithm, ncol = 1)
    

  return(p)
}

ukp_time_comp_plot(mtu_csv)
@
\end{center}
%\legend{Source: the author.}
\label{fig:mtu}
\end{figure}

The MTU1 implementations had a very small variation, with the C++ version being slightly faster.
The MTU2 implementation had a bigger variation.
The authors believe that this variation was caused by the difference of sorting algorithms and items ordering.

There is a trend between the values 180 and 230 of the x axis (MTU2 C++).
This trend consists of subset-sum instances.
The subset-sum instances are always naturally sorted by efficiency as all items share the same efficiency.
The instances generated by PYAsUKP are also sorted by non-decreasing item weight, what makes them perfectly sorted to the C++ implementation.
The Fortran implementation does not seem to work well with this characteristic of the subset-sum instances.
Only the C++ implementations are used in the rest of the experiments.
The rationale follows: the C++ implementation can read the same format used by PYAsUKP (and the other algorithms); with the exception of PYAsUKP (which uses OCaml) all other methods use C++ and the same data structures; choosing the Fortran77 implementation would considerably harm MTU2 run times for something that seems to be a minor implementation detail.

\subsection{Realistic Random Dataset}
\label{sec:rr_exp}

The results of the experiments over the realistic random dataset are summarized in \autoref{fig:realistic_random}.
The solving time of both step-off algorithms and MGREENDP become almost identical as the size of the input grow. % TODO: consider removing the three and presenting only one of them
EDUK solving times are similar to the three above cited algorithms but, as the instance size grow, its times shift for worse. 
EDUK2 has many solving times that are similar to the EDUK ones but, for some instances in each size group, its B\&B phase solves the instance or helps to considerably reduce the instance size; in the three largest instance size groups, EDUK2 has the smallest solving times for that instances.
The pure B\&B algorithms (MTU1 and MTU2) show more variation than any other algorithms.
MTU1 and MTU2 have the smallest solving times for the smallest instances but, as the instance size (and \(w_{min}\)) grows, also grows the number of instances that need orders of magnitude more time to solve than the other algorithms.

\begin{figure}[h]
\caption{Solving times for 7 selected algorithms over each one of the 80 instances from the realistic random dataset. The time limit was of half an hour (1800 seconds), runs that exceeded the time limit are ploted as taking exactly the time limit. Shape and color are used to distinguish between algorithms. The shape size is used to indicate the amount of overlapping points. The horizontal position of the points was adjusted for better visualization.}
\begin{center}
<<realistic_random_figure, fig.height = 5>>=
gen_graph_all_alg_discrete_x(rr_csv) +
scale_x_continuous(trans = "log2",
                   breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16, 2^17))
@
\end{center}
%\legend{Source: the author.}
\label{fig:realistic_random}
\end{figure}

% TODO: check if this will added or not
\begin{comment}
<<realistic_random_table, results='asis'>>=
rr_t <- select(rr_csv, algorithm, n, internal_time)
rr_t <- group_by(rr_t, algorithm, n) %>% summarise(time = mean(internal_time))
rr_t <- dcast(rr_t, algorithm ~ n, value.var = "time")

pretiffy_alg_names_short <- function(names) {
  recode(names,
  'cpp-mtu1'             = 'MTU1 (C++)',
  'cpp-mtu2'             = 'MTU2 (C++)',
  'eduk'                 = 'EDUK',
  'eduk2'                = 'EDUK2',
  'ordered_step_off'     = 'Ord. Step-Off',
  'terminating_step_off' = 'Term. Step-Off',
  'mgreendp'             = 'MGREENDP')
}

rr_t$algorithm <- pretiffy_alg_names_short(rr_t$algorithm)
rr_xt <- xtable(rr_t)
caption(rr_xt) <- paste0("Mean time to solve the 80 instances from the ",
  "realistic random dataset with each of seven selected algorithms. ",
  "Runs that exceeded the 1800 seconds timeout counted as taking ",
  "1800 seconds (only affecs MTU1 and MTU2). Each column display the mean ",
  "time to solve the ten instances with the same \\(n\\) value.")

print(rr_xt, 
  include.rownames = F,
  scalebox = 0.91
)
@
\end{comment}

\subsection{BREQ 128-16 Standard Benchmark}
\label{sec:breq_exp}

BREQ 128-16 Standard Benchmark
The results confirm the hypothesis that this distribution would be hard to solve by DP algorithms and easy to solve by B\&B algorithms.

\begin{figure}[h]
\caption{Benchmark with the 128-16 Standard BREQ instances.}
\begin{center}
<<breq_figure, fig.height=5>>=
gen_graph_all_alg_discrete_x(breq_csv) +
scale_x_continuous(trans = "log2",
                   breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16,
                              2^17, 2^18, 2^19, 2^20))
@
\end{center}
%\legend{Source: the author.}
\label{fig:breq}
\end{figure}

% TODO: check if BREQ table will be kept and then remove this permanently
%<<breq_table, results='asis'>>=
%breq_t <- select(breq_csv, algorithm, n, internal_time)
%breq_t <- group_by(breq_t, algorithm, n) %>% summarise(time = mean(internal_time))
%breq_t <- dcast(breq_t, algorithm ~ n, value.var = "time")

%xtable(breq_t)
%@

Let us examine Figure~\ref{fig:breq}.
The algorithms form two lines with different slopes, one line with a steep slope and a line with a more gradual slope.
The steep slope line shows algorithms whose time grows very fast relative to the instance size growth.
This group is mainly composed by the DP algorithms: UKP5, UKP5\_SBW (i.e. UKP5 Sorted By Weight), and the EDUK algorithm.
The second group, which forms a more gradual slope, has algorithms whose time grows much slower with the instance growth.
This group is mainly composed by B\&B and hybrid methods, as: MTU1, MTU2, EDUK2 and MGREENDP.

Examining only MTU1 and MTU2, we can clearly see that for small instances their times overlap, but with the instance size growth the core problem strategy of MTU2 (that tries to avoid sorting and examining all the items) begins to pay off, making it the \emph{best algorithm} to solve BREQ instances. 

The behavior of EDUK2 shows that the default B\&B phase (executed before defaulting to EDUK) solves the BREQ instances in all cases.
If it did not, some EDUK2 points would be together with the EDUK points for the same instance size.
Among the pure DP algorithms, EDUK was the one with the worst run times, being clearly dominated by our two UKP5 versions. 

The UKP5 algorithm sorted the items by non-increasing efficiency, and had the~\(y^*\) bound and periodicity checking enabled.
These last two optimizations benefited none of the one hundred runs.
No knapsack capacity from an instance was reduced by the use of the~\(y^*\) bound; all instances had only overhead from the use of the periodicity checking.
The UKP5\_SBW sorted the items by increasing weight and had these two optimizations disabled.
The benchmark instance files had the items in random order, so both algorithms used a small and similar time ordering the items\footnote{It is interesting to note that, except for the small items that have profit rounding problems, in BREQ instances, the increasing efficiency order is the increasing weight order.}.

The MGREENDP is a modern implementation in C++, made by the author, of an algorithm made by Harold Greenberg.
The algorithm of Harold Greenberg (that was not named in the original paper) was an adaptation of the \emph{ordered step-off} algorithm from Gilmore and Gomory.
This algorithm periodically computes bounds (similar to the ones used by the B\&B approach) to check if it can stop the DP computation and fill any remaining capacity with copies of the best item.
In the majority of the runs, the bound computation allowed the algorithm to stop the computation at the beginning, having results very similar to EDUK2 (the hybrid B\&B-DP algorithm).
However, six of the MGREENDP executions had times in the steep slope line (the bound failed to stop the computation).
Without the bound computation, MGREENDP is basically the \emph{ordered step-off} from Gilmore and Gomory (which is very similar to UKP5, as already pointed out); consequently, those six outlier runs have times that would be expected from UKP5 for the respective instance size.

% TODO: try to rephrase
A run from the greatest instance size and one from the second greatest instance size were both ended by timeout.
The bound failed to stop the computation and the DP algorithm was terminated by timeout.

While the simple, multiple and collective dominances are rare in a BREQ distribution with integer profits, the solution dominance used by UKP5 works to some extent.
The UKP5 combines optimal solutions for small capacities with single items and generate solutions that, if optimal for some capacity, will be used to generate more solutions after (recursively).
In a BREQ instance, solutions composed of many small items rarely are optimal and, consequently, often discarded, wasting the time used to generate them.
The weak solution dominance used by UKP5 does not completely avoid this problem, but helps to generate less of the useless subproblem solutions.
%However, as a silver lining, the UKP5's solution dominance will discard those solutions as soon as possible, and will \emph{not} use them to generate any new solutions (saving some computational effort).
%In other words, almost all subproblem solutions are useless, and UKP5's solution dominance helps to generate less of them.

%The algorithms described at~\cite{TheUnboundedKnapsackProblem-T.C.HU.pdf} were not implemented and tested because of time reasons. However, we can see, by the algorithms assimptotic worst case complexity (\(O(n v_1 w_1)\) and~\(O(n w_i)\)), that they 

\subsection{Solving pricing subproblems from BPP/CSP}
\label{sec:csp_experiments}

All other experiments consisted of instances of the UKP with a specific distributions saved in files, and executables that read those instances, solved them and returned the solving time.
In this experiment, the instances are not UKP instances but BPP/CSP instances.
The times presented are the sum of the times used to solve all the pricing subproblems (and/or master problems) generated while solving the continuous relaxation of those BPP/CSP instances.

In a pricing subproblem, the profit of the items is a real number.
Adapting MTU1 for using floating point profit values proved to be difficult, as the bound computation procedure is based on the assumption that both weight and profit values were integer.
The solution found was to multiply the items profit values by a multiplicative factor, round them down and treat them as integer profit values.
The multiplicative factor chosen was~\(2^{40}\).

To measure the impact of the conversion described above, the author used two versions of the UKP5, one using floating point profit values, and the other using integer profit values and the same conversion described above.
The items from a pricing subproblem are always naturally sorted by increasing weight.
The author executed the two UKP5 variants with sorting enabled and disabled, to verify if the sorting cost would pay off.
Consequently, four versions of UKP5 were used in the tests, for all combinations of profit type (the original floating point, or the converted integer), and sorting (sorting by non-increasing efficiency, or not sorting, which is the same that having the items sorted by increasing weight).
The variants of each one of the four versions of the UKP5 described above to solve pricing subproblems will be referred to as: 


The profit values of pricing subproblems can be non-positive, which breaks the assumptions of some algorithms.
Items with non-positive profits are removed from the item list before passing it to the UKP solving algorithm.

The timeout used in those experiments was of 1800 seconds.
To simplify visualization, in the instances that CPLEX and MTU1 ended in timeout, these two methods are plotted as having used exactly 1800 seconds.

using CPLEX to solve the pricing subproblems is not really competitive;
for the majority of the CSP instances in the benchmark, solving their continuous relaxation with a non-CPLEX method takes less than one second.

The times are expected to be small because: many old datasets from the literature were included; if each relaxation takes more than couple of seconds to solve, then solving the original BPP/CSP instance with B\&B can become impracticable.

\begin{figure}[H]
\caption{Total time used solving pricing subproblems (UKP) in the continuous relaxation of the BPP/CSP.}
\begin{center}
<<knapsack_time, fig.height=5>>=
# Note that the points at y = 600 were introduced to allow
# for the visualization of the runs that resulted in timeout.
# Also, note that after the visible mtu1 point closest to the top right
# all other points are below cplex points (representing the mtu1 timeouts).
# For the hardest problems, there is a small but clear pattern that shows
# advantage of the methods that sort the array by efficiency over the ones
# who does not.
csp_fig <- csp_csv %>%
  filter(
    algorithm == 'cplex_cutstock'  |
    algorithm == 'mtu1_cutstock'   |
    algorithm == 'ordso_int_ns')
csp_fig[is.na(csp_fig$hex_sum_knapsack_time), ]$hex_sum_knapsack_time <- 1800
csp_fig <- csp_fig %>%
  group_by(filename) %>%
  mutate(mean_methods_time = mean(hex_sum_knapsack_time)) %>%
  arrange(mean_methods_time)
csp_fig$filename <- factor(
  csp_fig$filename,
  levels = unique(csp_fig$filename))

ggplot(csp_fig,
       aes(x = as.numeric(filename),
           y = hex_sum_knapsack_time,
           color = algorithm)) +
  xlab('Instance index when sorted by the\nmean time methods spent solving pricing subproblems') +
  ylab('Total time (seconds, log10 scale)') +
  geom_point(shape = 3) +
  scale_shape_identity() +
  scale_colour_manual(name = "Algorithm", labels = c(
    'cplex_cutstock' = 'CPLEX',
    'mtu1_cutstock' = 'MTU1',
    'ordso_int_ns' = 'Ord. Step-Off (no sort, integer)'
  ), values = c(
    'cplex_cutstock' = 'black',
    'mtu1_cutstock' = 'darkgrey',
    'ordso_int_ns' = 'lightgrey'
  )) +
  scale_y_continuous(trans = 'log10',
    breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
    labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
  theme(legend.position = 'bottom')
@
\end{center}
%\legend{Source: the author.}
\label{fig:csp_knapsack_time}
\end{figure}

There is a cluster of points where~\(5000 < x < 5500\) and~\(y < 0.1\).
This cluster highlights the instances whose pricing subproblems were solved orders of magnitude faster by the step-off method in comparison with the CPLEX or MTU1.

The authors will focus on the runs that spent more than a second solving pricing subproblems.%, in Figure~\ref{fig:csp_knapsack_time}, some patterns emerge.
First, it becomes clear that the B\&B approach is not viable for larger/harder instances, as its exponential worst-case times make the problem untractable.

Second, there seems to be an advantage in not sorting the items, and this difference is not caused by the time taken by the sorting procedure.
In the runs that lasted more than a second, the time used to sort the items was between 3\% and 0.5\% of the time used to solve the pricing subproblems.
Such small difference does not explain the gap displayed in the graph.
When the items were kept sorted by increasing weight, UKP5 was up to two times faster than when the items were sorted by non-increasing efficiency.
Keeping the items sorted by increasing weight seems to be the best choice for such instances.
Third, it seems that executing all computation using integers is slight faster than using floating point profit values, but the difference is barely noticeable.

When CPLEX is used to solve the pricing subproblems, almost all total time is spent solving the pricing subproblems.
However, the time taken by non-CPLEX methods to solve the pricing subproblem remained mostly below 25\% of the total time (or even less).

In the case of CUTSTOCK\_UKP5\_*, the most time-consuming instances spent no more than 40\% of the time solving the pricing subproblems (often considerably less).
This is not to say that, in such time-consuming instances, the pricing subproblems were not harder.
POINT TO ANI AI
% In Figure~\ref{fig:percentage_knap_subproblem}, where~\(x > 550\), we can see that there is a rise of the relative time taken to solve the pricing subproblem instances with UKP5.
However, the reason for the growth in the total time spent to solve the most time-consuming instances was that many cutting patterns had to be added to the master problem before it could not be improved anymore; what results in more iterations and, consequently, more instances of the pricing subproblem and master problem solved.

except by ANI AI, which had many unfinished runs, the times used by CPLEX doesn't seem to depend on the method being used to solve subproblems

%\subsection{The differences in the number of pricing subproblems solved}

%pricing subproblems can have many optimal solutions, and different methods break this tie in different ways.
%The choice of optimal solution will affect the master problem, which will generate a slightly different pricing subproblem.
%This effect cascades and can change the profit values of all next pricing subproblems, and the number of pricing subproblems that are needed to solve (which is the same as the number of iterations, and the number of master problems solved).

%CPLEX\_CUTSTOCK stands out by requiring many of the smallest number of iterations.
%The author cannot explain what property the pricing subproblem solutions returned by CPLEX have that creates such a difference.
%the table masks a little this effect because it could be attributed to the instances to the highest count of iterations suffering timeout (and reducing the mean)

\subsection{The only outlier}

In every experiment presented in this thesis, the author verified if the optimal solution value (the value of the objective function) was the same for all methods.
Given the innacurate nature of floating point arithmetic, in this experiment, the optimal solution values differed from method to method.

The author found that differences among knapsack solutions because of precision loss, followed by the cascade effect, are common.

\subsection{Similar methods generate different amounts of pricing subproblems}
\label{sec:diff_it_count}

The two traits are: the type used for the profit values (floating point or integer); and if the items were sorted by efficiency or not.

Although there was one outlier, converting profit values to integer seems to be a valid method to test classic UKP algorithms (that work only with integer profits) for solving pricing subproblems (where the profit values are floating point values). 

\subsection{previously in conclusions}

% TODO: update example
For a single example, we will focus on instance 201\_2500\_DI\_11.txt, for which MTU1\_CUTSTOCK ended in timeout.
The pricing problems generated when solving such instance have \(n < 200\) and \(c = 2472\).
Before timeout, MTU1 solved about 700 of those pricing problems in much less than a millisecond each.
However, there was also a few pricing problems that took ten seconds or more to solve; run times like: 10, 12, 13, 13, 26, 32, 49, 54, and 351 seconds.
All instances shared the same \(n\), \(c\) and the same items weights, the only difference between them is the profit values.
Such behaviour corroborates with what was said about B\&B algorithms being strongly affected by the items distribution, and less by \(n\) and \(c\).

It is worth mentioning that almost all instances that ended in timeout are artificial instances created to be hard to solve.
Such instances were proposed in \cite{survey2014}. % TODO: chech which dataset this was and spell it out
The authors can not state that B\&B algorithms are not viable for solving pricing problems, only that there is evidence that the B\&B worst-case can arise in such circumstances.
%Unfortunately, the author of this thesis did not have the time to gather CSP problems from industrial sources, and the experimentation had to stop at the literature instances.

\subsection{Algorithms not used in this experiment}
\label{sec:csp_alg_not_used}

The MGREENDP, MTU2 (C++ or Fortran) and EDUK/EDUK2 (PYAsUKP) implementations were not used in this experiment.
MGREENDP fails if the two most efficient items share the same efficiency, what often happens for at least one pricing subproblem of a CSP instance.
In instances with a small number of items, MTU2 behaves almost the same way that MTU1.
Finally, the authors' did not succeed in integrating the PYAsUKP code (written in OCaml) with the C++/CPLEX code used in this experiment.
%The code of this experiment needs to call the UKP solving methods from inside the C++ code that also solves the CPLEX master model.
%The author tried to integrate EDUK and EDUK2 (which are written in OCaml) to the experiment, using the interface between C/C++ and OCaml, but the examples provided together with the PYAsUKP sources for this kind of integration were not working.

