<<rr_breq_shared_exp>>=
my_relevel <- function(f) {
  factor(f, levels = c('cpp-mtu1', 'cpp-mtu2', 'eduk', 'eduk2', 'ordered_step_off', 'terminating_step_off', 'mgreendp'))
}

# used by rr and breq
gen_graph_all_alg_discrete_x <- function(csv) {
  alg_labels <- c(
    'cpp-mtu1'             = 'MTU1 (C++)',
    'cpp-mtu2'             = 'MTU2 (C++)',
    'eduk'                 = 'EDUK',
    'eduk2'                = 'EDUK2',
    'ordered_step_off'     = 'Ordered Step-Off',
    'terminating_step_off' = 'Terminating Step-Off',
    'mgreendp'             = 'MGREENDP'
  )
  # I thought of putting one of the legends in an unused area of the plot and
  # another below the plot, but separating legends with ggplot2 needs a hack:
  # https://stackoverflow.com/questions/13143894/how-do-i-position-two-legends-independently-in-ggplot
  # which is too much effort for a little gain.
  p <- ggplot(csv,
              aes(x = n,# * (0.75 + 0.50*(as.numeric(algorithm) - 1)/6),
                  y = internal_time,
                  color = algorithm,
                  shape = algorithm)) + 
    #geom_jitter() +
    #stat_bin2d(binwidth = 0.25, geom="text", aes(label=..count..), position = position_dodge(0.7)) +
    #geom_point() + geom_count() +
    stat_bin_2d(geom = "point",
                aes(size=..count.., alpha = 0.80),
		position = position_dodge(0.75)) +
    scale_alpha(limits = c(0, 1), guide = FALSE) +
    # remove default fill stat_bin_2d legend
    scale_fill_gradient(guide = FALSE) +
    scale_size_area(breaks = c(2, 5, 8),
                    guide = guide_legend(title = "# of points",
                                         title.position = 'top',
                                         direction = 'horizontal',
                                         title.hjust = 0.5)) +
    scale_y_continuous(trans = "log10",
                       breaks = c(10^-3, 10^-2, 10^-1, 1, 10, 60, 600, 1800),
                       labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) +
    scale_colour_manual(name = "Algorithm",
                       labels = alg_labels,
                       values = c('cpp-mtu1' = 'black', 'cpp-mtu2' = 'darkgrey', 'eduk' = 'black', 'eduk2' = 'darkgrey', 'ordered_step_off' = 'black', 'terminating_step_off' = 'darkgrey', 'mgreendp' = 'black')) +
    scale_shape_manual(name = "Algorithm",
                       labels = alg_labels,
                       values = c('cpp-mtu1' = 16, 'cpp-mtu2' = 16, 'eduk' = 17, 'eduk2' = 17, 'ordered_step_off' = 15, 'terminating_step_off' = 15, 'mgreendp' = 4)) +
    ylab('Time to solve (seconds, log10 scale)') +
    xlab('Instance size (number of items, log2 scale)') +
    theme(legend.position = 'bottom')
  #ggsave('realistic_random.pdf', width = 0.80*210, height = 0.5*297, unit = 'mm')
  return(p);
}
@

<<rr_shared>>=
rr_get_n <- function(fname) {
  as.numeric(gsub(".*_n([1-9][0-9]+)-.*", "\\1", fname))
}

rr_csv <- read.csv("../data/realistic_random.csv", sep = ";")
# the duration of runs killed by timeout will be the timeout
rr_csv$internal_time[is.na(rr_csv$internal_time)] <- 1800
rr_csv$n <- rr_get_n(rr_csv$filename)
rr_csv$algorithm <- my_relevel(rr_csv$algorithm)
@

<<breq_shared>>=
breq_get_n <- function(fname) {
  as.numeric(gsub(".*-n([1-9][0-9]+)-.*", "\\1", fname))
}

breq_csv <- read.csv("../data/128_16_std_breqd.csv", sep = ";")
# the duration of runs killed by timeout will be the timeout
breq_csv$internal_time[is.na(breq_csv$internal_time)] <- 1800
breq_csv$n <- breq_get_n(breq_csv$filename)
breq_csv$algorithm <- my_relevel(breq_csv$algorithm)
@

<<mtu_shared>>=
mtu_csv <- read.csv("../data/mtus_pya.csv", sep = ";")
mtu_csv$internal_time[is.na(mtu_csv$internal_time)] <- 1800
@

<<fast_shared>>=
fast_csv <- read.csv("../data/pya_ukp5.csv", sep = ";")
fast_csv$X <- NULL # necessary for complete.cases to work
@

<<env_shared>>=
env_data <- read.csv("../data/env_influence.csv", sep = ";")
env_data$X <- NULL
@

<<to_rework_shared>>=
fix_names <- function(x) {
  x <- gsub('all', 'ALL', x)
  x <- gsub('ss2', 'SS', x)
  x <- gsub('saw', 'SAW', x)
  x <- gsub('nsds2', 'PP', x)
  x <- gsub('sc', 'SC', x)
  x <- gsub('hi', 'WCD', x)
  x
}
@

<<csp_shared>>=
csp_csv <- read.csv("../data/csp_6195.csv", sep = ";")
csp_csv$X <- NULL
csp_csv$dataset <- gsub('(.*)/.*', '\\1', csp_csv$filename)
csp_csv$dataset <- gsub('Randomly_Generated', 'Random', csp_csv$dataset)
csp_csv$dataset <- gsub('ANI_AI', 'ANI AI', csp_csv$dataset)
csp_csv$dataset <- factor(csp_csv$dataset)
# forgot to add sort_time to knapsack time in the chart generation codes, now
# changing it here to avoid changing many places
csp_csv$pricing_total_time <- csp_csv$hex_sum_knapsack_time + csp_csv$hex_sum_sort_time
@

\section{Results and Preliminary Analyses}
\label{sec:exp_and_res}

The experiments are split into five subsections.
Each section address one dataset and the results of running some selected algorithms over them.
Due to the diversity of experiments, the results of each individual experiment is presented with its immediate analysis.
%The big picture painted by all the experiments is discussed in \autoref{sec:discussion}.

\subsection{PYAsUKP dataset}
\label{sec:pya_exp}

The experiment presented in this section is an updated version of the experiment first presented in~\cite{sea2016}.
In this version GREENDP is considered, UKP5 is replaced by the Terminating Step-Off (TSO), and all runs were executed serially.
The exact same 4540 instances were used.

\begin{figure}[H]
\caption{Benchmark using the PYAsUKP dataset and the Terminating Step-off, GREENDP and EDUK2 algorithms. There was no time limit. The instance classes acronyms stand for: Postponed Periodicity (PP); Strongly Correlated (SC); Subset-Sum (SS); and Without Collective Dominance (WCD). SAW is not an acronym. EDUK was not included because EDUK2 superseeds EDUK. The ordered step-off run times were ommited because they were too similar to the terminating step-off run times. The GREENDP run times over subset-sum instances are not displayed because GREENDP fails if the two most efficient items have the same efficiency, which always happens with subset-sum instances.}
\begin{center}
<<pya_fast_fig>>=
fast_plot <- function(t) {
  inst_types <- c('all', 'hi', 'nsds2', 'saw', 'sc', 'ss2')

  data <- t[complete.cases(t), ]
  data$type <- sapply(data$filename, (function (f) {strsplit(as.character(f), "_")[[1]][1] }))
  data$type <- factor(data$type, levels = inst_types)

  data <- filter(data, algorithm != 'ordered_step_off')
  #data <- group_by(data, filename) %>%
  #  mutate(mean_methods_time = mean(internal_time))
  data <- group_by(data, filename) %>%
    mutate(eduk2_time = internal_time[algorithm == 'pyasukp'])

  data_copy <- data
  data$type <- factor('all', levels = inst_types)

  data <- rbind(data, data_copy) %>% arrange(eduk2_time) #arrange(mean_methods_time)
  data$filename <- factor(data$filename,
        levels = unique(data$filename))

  #data$algorithm <- gsub('ordered_step_off', 'O.S.', data$algorithm)
  data$algorithm <- gsub('terminating_step_off', 'Terminating Step-Off', data$algorithm)
  data$algorithm <- gsub('pyasukp', 'EDUK2', data$algorithm)
  data$algorithm <- gsub('mgreendp', 'MGREENDP', data$algorithm)
  data$type <- fix_names(data$type)
  shape_conv <- c('Terminating Step-Off' = 15, 'EDUK2' = 16, 'MGREENDP' = 17)

  ggplot(data,
         aes(x = as.numeric(filename),
       y = internal_time,
       color = algorithm,
       shape = algorithm)) + 
    geom_point() +
    scale_y_continuous(trans = 'log10',
           breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
           labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
  #  ggtitle('Benchmark with the 4540 PYAsUKP instances') +
    ylab('Time to solve (seconds, log10 scale)') +
    xlab('Instance index when sorted by EDUK2 time to solve') +
    scale_colour_grey() +
    scale_shape_manual(name = "algorithm", values = shape_conv) + 
    theme(legend.position = 'bottom') + facet_wrap( ~ type, ncol = 3)
}

fast_plot(fast_csv)
@
\end{center}
%\legend{Source: the author.}
\label{fig:pya_fast}
\end{figure}

In \autoref{fig:pya_fast}, the instances (x axis) are sorted by the time EDUK2 spent to solve them.
EDUK2 B\&B phase allows EDUK2 to solve some instances of every magnitude faster than TSO/GREENDP.
This behaviour shows that EDUK2 times for a specific instance can not be predicted based on the number of items and distribution of the instance, as some instances between all instances that share these characteristics will be remarkably faster to solve by B\&B due to the specific items that constitute the instance.
Nonetheless, when the B\&B phase has little effect, the EDUK2 DP phase (basically EDUK) spend considerably more time than TSO/MGREENDP to solve the instance.

TSO and MGREENDP run times form plateaus.
For each class of instances, the plateaus aggregate runs over instances with number of items of similar magnitude.
This behaviour shows that the specific items that constitute an instance affect TSO and MGREENDP less than the number of items and distribution, both which are good predictors of the TSO and MGREENDP run time.

The run times of TSO and MGREENDP are similar, except for the SAW instances, in which MGREENDP performed considerably worse than TSO.
GREENDP DP phase does not generate solutions including the best item.
The use of the best item allows the generation of more efficient solutions, which dominate less efficient solutions, and reduce the total number of solutions generated and, consequently, the computational effort spent (i.e. the use of the best item would increase the positive effects of weak solution dominance).
The authors believe that the exclusion of the best item from the DP phase is the reason that GREENDP had run times higher than TSO over SAW instances.

When EDUK2 solves an instance in less time than TSO, the difference is often less than a second, and up to ten seconds.
When EDUK2 solves an instance in more time than TSO, the difference is often more than five seconds and up to six minutes.
The EDUK2 mean time to solve a PYAsUKP instance is \(23\) seconds and TSO mean time to solve a PYAsUKP instance is \(1.5\) seconds.

\subsection{MTU1 and MTU2 (C++ and Fortran)}
\label{sec:mtu_exp}

The MTU1 and MTU2 implementations in C++ and Fortran were all executed over the reduced PYAsUKP benchmark.

\begin{figure}[H]
\caption{Benchmark using the reduced PYAsUKP dataset and the MTU1 and MTU2, C++ and Fortran implementations. The time limit was 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit.}
\begin{center}
<<mtu_comp, fig.height=5.5>>=
ukp_time_comp_plot <- function (data) {
  # select columns necessary to the plot, and divide information
  # given in algorithm column in two columns: language and algorithm
  data <- select(data, algorithm, filename, internal_time)
  data$language <- gsub("cpp-mtu[12]", "C++", data$algorithm)
  data$language <- gsub("fmtu[12]", "Fortran", data$language)
  data$language <- factor(data$language, levels = c('C++', 'Fortran'))
  data$algorithm <- gsub(".*1", "MTU1", data$algorithm)
  data$algorithm <- gsub(".*2", "MTU2", data$algorithm)
  data$algorithm <- factor(data$algorithm, levels = c('MTU1', 'MTU2'))

  # compute the mean of the internal_time (for the same filename but
  # with distinct algorithms), and sort the rows by this mean
  data <- group_by(data, filename) %>%
          mutate(fname_mean_time = mean(internal_time)) %>%
	  arrange(fname_mean_time)
  # make filename a factor so as.numeric of a filename will give its position
  # in the ordering described above
  data$filename <- factor(data$filename, levels = unique(data$filename))
  # plot the graph, no change in data
  p <- ggplot(data,
              aes(x = as.numeric(filename),
	          y = internal_time,
		  color = language)) +
    geom_point(shape = 3) +
    scale_shape_identity() +
    scale_colour_manual(values = c('C++' = 'black', 'Fortran' = 'darkgrey')) +
    scale_y_continuous(trans = 'log10',
                       breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
                       labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
    ylab("Time to solve\n(seconds, log10 scale)") +
    xlab("Instances sorted by mean time to solve\n(mean between the four implementations) ") +
    theme(legend.position = 'bottom') +
    facet_wrap(~ algorithm, ncol = 1)
    

  return(p)
}

ukp_time_comp_plot(mtu_csv)
@
\end{center}
%\legend{Source: the author.}
\label{fig:mtu}
\end{figure}

In \autoref{fig:mtu}, both MTU1 implementations show run times in the same order of magnitude.
However, considering only the instances that both MTU1 implementations solved before timeout, the mean run time of Fortran MTU1 was 59 seconds and the mean run time of C++ MTU1 was 30 seconds.
The analysis of the individual run times shows that for many instances Fortran MTU1 spent about the double of the time spent by C++ MTU1 to solve the same instance.

For many instances, the run times of the MTU2 implementations differed in more than one order of magnitude.
The authors believe that this divergence was caused by the difference of sorting algorithms and items ordering (C++ MTU2 order items by increasing weight if they share the same efficiency).
The subset-sum instances were the ones that exhibited the greatest difference, C++ MTU2 solved all 40 subset-sum instances with a mean time of \(0.04\) seconds while Fortran MTU2 solved only 8 instances with a mean time of \(155\) seconds.
The range \([190,221]\) of the x axis of \autoref{fig:mtu} is composed of subset-sum instances.
Diregarding the subset-sum instances, the mean run time of Fortran MTU2 was 56 seconds and the mean run time of C++ MTU2 was 40.5 seconds.
Only the C++ implementations are used in the rest of the experiments.

<<range_190_121_stat, results = 'hide'>>=
mtu_csv_for_range <- group_by(mtu_csv, filename) %>%
  mutate(fname_mean_time = mean(internal_time)) %>%
  arrange(fname_mean_time)
unique(mtu_csv_for_range$filename)
@

%The subset-sum instances are always naturally sorted by efficiency, as all items share the same efficiency.
%The instances generated by PYAsUKP are also sorted by non-decreasing item weight, what makes them perfectly sorted to C++ MTU2.
%The Fortran MTU2 does not seem to work well with this item ordering of the subset-sum instances.

\subsection{Realistic Random Dataset}
\label{sec:rr_exp}

The results of the experiments over the realistic random dataset are summarized in \autoref{fig:realistic_random}.
The run times of both step-off algorithms and MGREENDP become almost identical as the size of the input grow. % TODO: consider removing the three and presenting only one of them
EDUK run times are similar to the three above cited algorithms but, as the instance size grow, its times shift for worse. 
EDUK2 has many run times that are similar to the EDUK ones but, for some instances in each size group, its B\&B phase solves the instance or helps to considerably reduce the instance size.
MTU1 and MTU2 have the smallest run times for the smallest instances but, as the instance size and \(w_{min}\) grow, also grow the number of instances in which MTU1 and MTU2 spend orders of magnitude more time to solve than the other algorithms.
%The pure B\&B algorithms (MTU1 and MTU2) show more variation than any other algorithms.

\begin{figure}[h]
\caption{Run times of all selected algorithms over the instances of the realistic random dataset. The time limit was 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit. Shape and color are used to distinguish between algorithms. The shape size is used to indicate the amount of overlapping points. The horizontal position of the points was adjusted for better visualization.}
\begin{center}
<<realistic_random_figure, fig.height = 5>>=
gen_graph_all_alg_discrete_x(rr_csv) +
scale_x_continuous(trans = "log2",
                   breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16, 2^17))
@
\end{center}
%\legend{Source: the author.}
\label{fig:realistic_random}
\end{figure}

% TODO: check if this will added or not
\begin{comment}
<<realistic_random_table, results='asis'>>=
rr_t <- select(rr_csv, algorithm, n, internal_time)
rr_t <- group_by(rr_t, algorithm, n) %>% summarise(time = mean(internal_time))
rr_t <- dcast(rr_t, algorithm ~ n, value.var = "time")

pretiffy_alg_names_short <- function(names) {
  recode(names,
  'cpp-mtu1'             = 'MTU1 (C++)',
  'cpp-mtu2'             = 'MTU2 (C++)',
  'eduk'                 = 'EDUK',
  'eduk2'                = 'EDUK2',
  'ordered_step_off'     = 'Ord. Step-Off',
  'terminating_step_off' = 'Term. Step-Off',
  'mgreendp'             = 'MGREENDP')
}

rr_t$algorithm <- pretiffy_alg_names_short(rr_t$algorithm)
rr_xt <- xtable(rr_t)
caption(rr_xt) <- paste0("Mean time to solve the 80 instances from the ",
  "realistic random dataset with each of seven selected algorithms. ",
  "Runs that exceeded the 1800 seconds timeout counted as taking ",
  "1800 seconds (only affecs MTU1 and MTU2). Each column display the mean ",
  "time to solve the ten instances with the same \\(n\\) value.")

print(rr_xt, 
  include.rownames = F,
  scalebox = 0.91
)
@
\end{comment}

Despite EDUK2 solving some instances orders of magnitude faster than the other algorithms (specially in the larger instance sizes), the mean run time of EDUK2 (8.51 seconds) is higher than the TSO mean run time (5.36 seconds).
As already observed in \autoref{sec:pya_exp}, EDUK2 often have one or more run times that are one order of magnitude higher than the highest run time of TSO, what considerably increases its mean time.

<<rr_means, results = 'hide' >>=
rr_csv2 <- read.csv("../data/realistic_random.csv", sep = ";")
rr_dcast <- dcast(rr_csv2, filename ~ algorithm, value.var = 'internal_time')
summary(rr_dcast)
summary(filter(rr_dcast, grepl('n131072', filename)))
@

\subsection{BREQ 128-16 Standard Benchmark}
\label{sec:breq_exp}

The results of the experiments over the BREQ 128-16 Standard Benchmark are summarized in \autoref{fig:breq}.
The run times outline two groups with distinct time growth, a fast-growth group (which hit the time limit) and a slow-growth group (which always take less than a second).
The fast-growth group is mainly composed by the DP algorithms: TSO, OSO, and EDUK.
The slow-grouth group is mainly composed by the B\&B and hybrid algorithms: MTU1, MTU2, and EDUK2.
GREENDP is the only algorithm with run times in both groups.

\begin{figure}[h]
\caption{Run times of all selected algorithms over the instances of the BREQ 128-16 Standard Benchmark. The time limit was 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit. Shape and color are used to distinguish between algorithms. The shape size is used to indicate the amount of overlapping points. The horizontal position of the points was adjusted for better visualization.}
\begin{center}
<<breq_figure, fig.height=5>>=
gen_graph_all_alg_discrete_x(breq_csv) +
scale_x_continuous(trans = "log2",
                   breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16,
                              2^17, 2^18, 2^19, 2^20))
@
\end{center}
%\legend{Source: the author.}
\label{fig:breq}
\end{figure}

% TODO: check if BREQ table will be kept and then remove this permanently
%<<breq_table, results='asis'>>=
%breq_t <- select(breq_csv, algorithm, n, internal_time)
%breq_t <- group_by(breq_t, algorithm, n) %>% summarise(time = mean(internal_time))
%breq_t <- dcast(breq_t, algorithm ~ n, value.var = "time")

%xtable(breq_t)
%@

EDUK2 run times are in the slow-growth group due to its B\&B phase.
EDUK2 without its B\&B phase is basically EDUK, which is in the fast-growth group.
MTU2 core strategy improve MTU1 run times over BREQ instances.
TSO strategy yields no significant gains over OSO in BREQ instances.
GREENDP is an OSO variant which periodically compute bounds (similar to the ones used by the B\&B approach) to verify if the DP can be stopped and any remaining capacity be filled with copies of the best item.
If the bounds stop the DP early then GREENDP run time falls in the slow-growth group, otherwise the run time is similar to the OSO/TSO run time for the same instance.
The results confirm the hypothesis that this distribution would be hard to solve by DP algorithms and easy to solve by B\&B algorithms.

\subsection{Solving pricing subproblems from BPP/CSP}
\label{sec:csp_experiments}

The previous experiments included datasets of UKP instances and binaries that read and solved a single instance and then returned the solving time.
In this experiment, the instances are BPP/CSP instances.
Also, the times presented are the sum of all time spent solving the multiple pricing subproblems generated by the continuous relaxation of the Set Covering Formulation (with column generation) of a single BPP/CSP instance.

% The TSO, GREENDP, MTU2 (C++ or Fortran), and EDUK/EDUK2 (PYAsUKP) implementations were not used in this experiment.
Only the ordered step-off and MTU1 (C++) were used in this experiment.
GREENDP fails if the two most efficient items share the same efficiency, what often happens for at least one pricing subproblem of a CSP instance.
In instances with a small number of items, MTU2 behaves almost the same way that MTU1, the same applies for the terminating step-off and the ordered step-off.
The authors did not succeed in integrating the PYAsUKP code (written in OCaml) with the C++/CPLEX code needed by this experiment.

%CPLEX was also used to solve the pricing problems, adding one more algorithm to the comparison.

In a pricing subproblem, the profit of the items is a real number.
Adapting MTU1 for using floating point profit values proved to be difficult, as the bound computation procedure is based on the assumption that both weight and profit values were integer.
The solution found was to multiply the items profit values by a multiplicative factor, round them down and treat them as integer profit values.
The multiplicative factor chosen was~\(2^{40}\).
In a pricing subproblem, the profit of the items can also be non-positive, which breaks the assumptions of some algorithms.
Items with non-positive profits are removed from the item list before passing it to the UKP solving algorithm.

%ITS IMPORTANT TO NOTE THAT SUCH INSTANCES ARE DIFFUCULT FOR THE MUCH HARDER PROBLEM OF SOLVING THE NOT RELAXATION EXCATLY
%The sum of the times solving pricing problems for an instance is often less than a second as solving the continuous relaxation of the BPP/CSP is much easier than solving the BPP/CSP exactly ().
Solving the continuous relaxation of the BPP/CSP is much easier than solving the BPP/CSP, due to this, the time spent solving pricing subproblems in instances designed for the latter problem often amount less than a second.
%The majority of the times presented is below a second because: many old datasets from the literature were included; to get the exact solution of a BPP/CSP instance with a B\&B algorithm it is necessary to solve multiple continuous relaxations, if each relaxation takes a significant time to solve, then solving exactly the BPP/CSP instance with B\&B becomes impracticable.

\begin{figure}
\caption{Time spent solving all pricing subproblems of each instance in the CSP pricing subproblem dataset, with two selected algorithms. % three selected algorithms.
The time limit was 30 minutes (the time limit considered the run time, not only the time solving pricing subproblems). If a run is terminated by timeout, the time spent solving pricing subproblems is displayed as exactly the time limit.}
\begin{center}
<<knapsack_time_shared>>=
# Note that the points at y = 1800 were introduced to allow
# for the visualization of the runs that resulted in timeout.
# Also, note that after the visible mtu1 point closest to the top right
# all other points are below cplex points (representing the mtu1 timeouts).
# For the hardest problems, there is a small but clear pattern that shows
# advantage of the methods that sort the array by efficiency over the ones
# who does not.

plot_csp <- function (csp_csv) {
  csp_fig <- csp_csv %>%
    filter(
      #algorithm == 'cplex_cutstock'  |
      algorithm == 'mtu1_cutstock'   |
      algorithm == 'ordso_int_ns')
  csp_fig[is.na(csp_fig$hex_sum_knapsack_time), ]$hex_sum_knapsack_time <- 1800
  
  csp_fig <- csp_fig %>%
    group_by(filename) %>%
    mutate(mean_methods_time = mean(hex_sum_knapsack_time)) %>%
    arrange(mean_methods_time)
    #mutate(mean_methods_time = hex_sum_knapsack_time[algorithm == 'mtu1_cutstock']) %>%
    #arrange(mean_methods_time)
  csp_fig$filename <- factor(
    csp_fig$filename,
    levels = unique(csp_fig$filename))
  
  ggplot(csp_fig,
         aes(x = as.numeric(filename),
             y = hex_sum_knapsack_time,
             color = algorithm)) +
    #xlab('Instance index when sorted by the mean time\nthe three algorithms spent solving pricing subproblems') +
    geom_point(shape = 3) +
    scale_shape_identity() +
    scale_colour_manual(name = "Algorithm", labels = c(
      'mtu1_cutstock' = 'MTU1',
      'ordso_int_ns' = 'Ord. Step-Off (no sort, integer)'
      # 'cplex_cutstock' = 'CPLEX'
    ), values = c(
      'mtu1_cutstock' = 'darkgrey',
      'ordso_int_ns' = 'black'
      # 'cplex_cutstock' = 'lightgrey'
    )) +
    coord_cartesian(ylim = c(0.001, 1800)) +
    scale_y_continuous(trans = 'log10',
      breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
      labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800'))#,
      #limits = c(0.0005, 1800))
}
@

%<<knapsack_time1, fig.height = 2 >>=
%csp_csv$dataset <- 'All datasets'
%plot_csp(csp_csv) +
%theme(legend.position = 'none') +
%xlab('') +
%ylab('')
%@

<<knapsack_time2, fig.height = 10 >>=
csp_plot_labels <- csp_csv %>%
  select(filename, dataset, algorithm, hex_sum_knapsack_time) %>%
  dcast(filename + dataset ~ algorithm, value.var = 'hex_sum_knapsack_time') %>%
  select(dataset, ordso_int_ns, mtu1_cutstock) %>%
  group_by(dataset) %>%
  summarise(
    n = length(dataset),
    no_na_ordso_int_ns = sum(!is.na(ordso_int_ns)),
    mean_ordso_int_ns = mean(ordso_int_ns, na.rm = T),
    no_na_mtu1_cutstock = sum(!is.na(mtu1_cutstock)),
    mean_mtu1_cutstock = mean(mtu1_cutstock, na.rm = T)
  )# %>%
## DAMNED BUGGED MUATATE CANT WORK WITH FUNCTIONS REFERING TWO DIFFERENT
## COLUMNS??
#  mutate(label = paste0(
#  	# CHANGE TO ONLY COUNT N NUMBER IN THE INTEGERS
#      "     n: ", sprintf("%*i", ceiling(log10(n + 1)), n),
#    "\nOSO  n: ", sprintf("%*i", ceiling(log10(n + 1)), no_na_ordso_int_ns),
#    "\nMTU1 n: ", sprintf("%*i", ceiling(log10(n + 1)), no_na_mtu1_cutstock),
#    "\nOSO  mean: ", sprintf("%f", max(mean_mtu1_cutstock, mean_ordso_int_ns)),# mean_ordso_int_ns),
#    "\nMTU1 mean: ", sprintf("%i", mw1(mean_ordso_int_ns))#, mean_mtu1_cutstock)
#  ))

# number of digits
nd <- function (i) { ceiling(log10(i + 1)) }
nd2 <- function (a, b) { max(nd(floor(a)), nd(floor(b))) }
# data frame to row list
df2rw <- function (df) { setNames(split(df, seq(nrow(df))), rownames(df)) }
lcsp_plot_labels <- df2rw(csp_plot_labels)
csp_plot_labels$label <- sapply(lcsp_plot_labels, function (t) paste0(
  # CHANGE TO ONLY COUNT N NUMBER IN THE INTEGERS
  "     n: ", sprintf("%*i", nd(t[['n']]), t[['n']]),
  "\nOSO  n: ", sprintf("%*i", nd(t[['n']]), t[['no_na_ordso_int_ns']]),
  "\nMTU1 n: ", sprintf("%*i", nd(t[['n']]), t[['no_na_mtu1_cutstock']]),
  "\nOSO  mean: ", sprintf("%*.2f", 3 + nd2(t[['mean_mtu1_cutstock']], t[['mean_ordso_int_ns']]), t[['mean_ordso_int_ns']]),
  "\nMTU1 mean: ", sprintf("%*.2f", 3 + nd2(t[['mean_mtu1_cutstock']], t[['mean_ordso_int_ns']]), t[['mean_mtu1_cutstock']])))

plot_csp(csp_csv) + facet_wrap( ~ dataset, ncol = 2) +
theme(legend.position = 'bottom') +
xlab('Instance index when sorted by the mean time\nthe two algorithms spent solving pricing subproblems') +
ylab('Total time (seconds, log10 scale)') +
geom_text(data = csp_plot_labels, aes(x = 125, y = 100, label = label, hjust = 0, family = 'mono', lineheight = 0.75), inherit.aes = F)
@
\end{center}
%\legend{Source: the author.}
\label{fig:csp_knapsack_time}
\end{figure}

In \autoref{fig:csp_knapsack_time}, shows that the OSO 

%As can be seen in \autoref{fig:csp_knapsack_time}, for the dataset used, CPLEX always takes more time than MTU1 or the OSO to solve pricing problems, and often one or two orders of magnirude more time.
%There is a cluster of points where~\(5000 < x < 5500\) and~\(y < 0.01\).
%This cluster highlights the instances whose pricing subproblems were solved orders of magnitude faster by OSO method in comparison with the CPLEX or MTU1.

%The authors will focus on the runs that spent more than a second solving pricing subproblems.%, in Figure~\ref{fig:csp_knapsack_time}, some patterns emerge.
%First, it becomes clear that the B\&B approach is not viable for larger/harder instances, as its exponential worst-case times make the problem untractable.

%When CPLEX is used to solve the pricing subproblems, almost all total time is spent solving the pricing subproblems.
%However, the time taken by non-CPLEX methods to solve the pricing subproblem remained mostly below 25\% of the total time (or even less).

%In the case of CUTSTOCK\_UKP5\_*, the most time-consuming instances spent no more than 40\% of the time solving the pricing subproblems (often considerably less).
%This is not to say that, in such time-consuming instances, the pricing subproblems were not harder.
% In Figure~\ref{fig:percentage_knap_subproblem}, where~\(x > 550\), we can see that there is a rise of the relative time taken to solve the pricing subproblem instances with UKP5.
%However, the reason for the growth in the total time spent to solve the most time-consuming instances was that many cutting patterns had to be added to the master problem before it could not be improved anymore; what results in more iterations and, consequently, more instances of the pricing subproblem and master problem solved.
%except by ANI AI, which had many unfinished runs, the times used by CPLEX doesn't seem to depend on the method being used to solve subproblems

%\subsection{The differences in the number of pricing subproblems solved}

%pricing subproblems can have many optimal solutions, and different methods break this tie in different ways.
%The choice of optimal solution will affect the master problem, which will generate a slightly different pricing subproblem.
%This effect cascades and can change the profit values of all next pricing subproblems, and the number of pricing subproblems that are needed to solve (which is the same as the number of iterations, and the number of master problems solved).

%CPLEX\_CUTSTOCK stands out by requiring many of the smallest number of iterations.
%The author cannot explain what property the pricing subproblem solutions returned by CPLEX have that creates such a difference.
%the table masks a little this effect because it could be attributed to the instances to the highest count of iterations suffering timeout (and reducing the mean)

\subsection{The only outlier}

To measure the impact of the conversion described above, the author used two versions of the UKP5, one using floating point profit values, and the other using integer profit values and the same conversion described above.
The items from a pricing subproblem are always naturally sorted by increasing weight.
The author executed the two UKP5 variants with sorting enabled and disabled, to verify if the sorting cost would pay off.
Consequently, four versions of UKP5 were used in the tests, for all combinations of profit type (the original floating point, or the converted integer), and sorting (sorting by non-increasing efficiency, or not sorting, which is the same that having the items sorted by increasing weight).
The variants of each one of the four versions of the UKP5 described above to solve pricing subproblems will be referred to as: 

The two traits are: the type used for the profit values (floating point or integer); and if the items were sorted by efficiency or not.

Consequently, four versions of UKP5 were used in the tests, for all combinations of profit type (the original floating point, or the converted integer), and sorting (sorting by non-increasing efficiency, or not sorting, which is the same that having the items sorted by increasing weight).
The author executed the two UKP5 variants with sorting enabled and disabled, to verify if the sorting cost would pay off.
Second, there seems to be an advantage in not sorting the items, and this difference is not caused by the time taken by the sorting procedure.
THE INSTANCES ARE NATURALLY ORDERED BY INCREASING WEIGHT

In every experiment presented in this thesis, the author verified if the optimal solution value (the value of the objective function) was the same for all methods.
Given the innacurate nature of floating point arithmetic, in this experiment, the optimal solution values differed from method to method.

The author found that differences among knapsack solutions because of precision loss, followed by the cascade effect, are common.

\subsection{previously in conclusions}

% TODO: update example
For a single example, we will focus on instance 201\_2500\_DI\_11.txt, for which MTU1\_CUTSTOCK ended in timeout.
The pricing problems generated when solving such instance have \(n < 200\) and \(c = 2472\).
Before timeout, MTU1 solved about 700 of those pricing problems in much less than a millisecond each.
However, there was also a few pricing problems that took ten seconds or more to solve; run times like: 10, 12, 13, 13, 26, 32, 49, 54, and 351 seconds.
All instances shared the same \(n\), \(c\) and the same items weights, the only difference between them is the profit values.
Such behaviour corroborates with what was said about B\&B algorithms being strongly affected by the items distribution, and less by \(n\) and \(c\).

It is worth mentioning that almost all instances that ended in timeout are artificial instances created to be hard to solve.
Such instances were proposed in \cite{survey2014}. % TODO: chech which dataset this was and spell it out
The authors can not state that B\&B algorithms are not viable for solving pricing problems, only that there is evidence that the B\&B worst-case can arise in such circumstances.
%Unfortunately, the author of this thesis did not have the time to gather CSP problems from industrial sources, and the experimentation had to stop at the literature instances.

