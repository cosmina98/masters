<<graphs_shared>>=
# number of digits
nd <- function (i) { max(ceiling(log10(i + 1)), na.rm = T) }
# data frame to row list
df2rw <- function (df) { setNames(split(df, seq(nrow(df))), rownames(df)) }
@

<<rr_breq_shared_exp>>=
my_relevel <- function(f) {
  factor(f, levels = c('cpp-mtu1', 'cpp-mtu2', 'eduk', 'eduk2', 'ordered_step_off', 'terminating_step_off', 'mgreendp'))
}

# used by rr and breq
gen_graph_all_alg_discrete_x <- function(csv) {
  alg_labels <- c(
    'cpp-mtu1'             = 'MTU1 (C++)',
    'cpp-mtu2'             = 'MTU2 (C++)',
    'eduk'                 = 'EDUK',
    'eduk2'                = 'EDUK2',
    'ordered_step_off'     = 'OSO',
    'terminating_step_off' = 'TSO',
    'mgreendp'             = 'GREENDP'
  )
  # I thought of putting one of the legends in an unused area of the plot and
  # another below the plot, but separating legends with ggplot2 needs a hack:
  # https://stackoverflow.com/questions/13143894/how-do-i-position-two-legends-independently-in-ggplot
  # which is too much effort for a little gain.
  p <- ggplot(csv,
              aes(x = n,# * (0.75 + 0.50*(as.numeric(algorithm) - 1)/6),
                  y = internal_time,
                  color = algorithm,
                  shape = algorithm)) + 
    #geom_jitter() +
    #stat_bin2d(binwidth = 0.25, geom="text", aes(label=..count..), position = position_dodge(0.7)) +
    #geom_point() + geom_count() +
    stat_bin_2d(geom = "point",
                aes(size=..count.., alpha = 0.80),
        position = position_dodge(0.75)) +
    scale_alpha(limits = c(0, 1), guide = FALSE) +
    # remove default fill stat_bin_2d legend
    scale_fill_gradient(guide = FALSE) +
    scale_size_area(breaks = c(2, 5, 8),
                    guide = guide_legend(title = "# of points",
                                         title.position = 'top',
                                         direction = 'horizontal',
                                         title.hjust = 0.5)) +
    scale_y_continuous(trans = "log10",
                       breaks = c(10^-3, 10^-2, 10^-1, 1, 10, 60, 600, 1800),
                       labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) +
    scale_colour_manual(name = "Algorithm",
                       labels = alg_labels,
                       values = c('cpp-mtu1' = 'black', 'cpp-mtu2' = 'darkgrey', 'eduk' = 'black', 'eduk2' = 'darkgrey', 'ordered_step_off' = 'black', 'terminating_step_off' = 'darkgrey', 'mgreendp' = 'black')) +
    scale_shape_manual(name = "Algorithm",
                       labels = alg_labels,
                       values = c('cpp-mtu1' = 16, 'cpp-mtu2' = 16, 'eduk' = 17, 'eduk2' = 17, 'ordered_step_off' = 15, 'terminating_step_off' = 15, 'mgreendp' = 4)) +
    ylab('Time to solve (seconds, log10 scale)') +
    xlab('Instance size (number of items, log2 scale)') +
    theme(legend.position = 'bottom')
  #ggsave('realistic_random.pdf', width = 0.80*210, height = 0.5*297, unit = 'mm')
  return(p);
}
@

<<rr_shared>>=
rr_get_n <- function(fname) {
  as.numeric(gsub(".*_n([1-9][0-9]+)-.*", "\\1", fname))
}

rr_csv <- read.csv("../data/realistic_random.csv", sep = ";")
# the duration of runs killed by timeout will be the timeout
rr_csv$internal_time[is.na(rr_csv$internal_time)] <- 1800
rr_csv$n <- rr_get_n(rr_csv$filename)
rr_csv$algorithm <- my_relevel(rr_csv$algorithm)
@

<<breq_shared>>=
breq_get_n <- function(fname) {
  as.numeric(gsub(".*-n([1-9][0-9]+)-.*", "\\1", fname))
}

breq_csv <- read.csv("../data/128_16_std_breqd.csv", sep = ";")
# the duration of runs killed by timeout will be the timeout
breq_csv$internal_time[is.na(breq_csv$internal_time)] <- 1800
breq_csv$n <- breq_get_n(breq_csv$filename)
breq_csv$algorithm <- my_relevel(breq_csv$algorithm)
@

<<mtu_shared>>=
mtu_csv <- read.csv("../data/mtus_pya.csv", sep = ";")
mtu_csv$internal_time[is.na(mtu_csv$internal_time)] <- 1800
@

<<fast_shared>>=
fast_csv <- read.csv("../data/pya_ukp5.csv", sep = ";")
fast_csv$X <- NULL # necessary for complete.cases to work
@

<<csp_shared>>=
csp_csv <- read.csv("../data/csp_6195.csv", sep = ";")
csp_csv$X <- NULL
csp_csv$dataset <- gsub('(.*)/.*', '\\1', csp_csv$filename)
csp_csv$dataset <- gsub('Randomly_Generated', 'Random', csp_csv$dataset)
csp_csv$dataset <- gsub('ANI_AI', 'ANI\\&AI', csp_csv$dataset)
csp_csv$dataset <- gsub('Irnich', 'GI', csp_csv$dataset)
csp_csv$dataset <- factor(csp_csv$dataset)
# forgot to add sort_time to knapsack time in the chart generation codes, now
# changing it here to avoid changing many places
csp_csv$pricing_total_time <- csp_csv$hex_sum_knapsack_time + csp_csv$hex_sum_sort_time
@

\section{Results and Analyses}
\label{sec:exp_and_res}

The experiments are split into five subsections.
Each section addresses one dataset and the results of running some selected algorithms over them.
To keep the results close to its discussion, each experiment section brings both the results and their immediate analysis.
%The big picture painted by all the experiments is discussed in \autoref{sec:discussion}.

\subsection{Results on the PYAsUKP dataset}
\label{sec:pya_exp}

The experiment presented in this section is an updated version of the experiment first presented in~\cite{sea2016}.
In this version GREENDP is considered, UKP5 is replaced by the Terminating Step-Off (TSO), and all runs were executed serially.
The same 4540 instance files were used.

\begin{figure}[!htbp]
\caption{Run times of the TSO, GREENDP (GREEN in the plot) and EDUK2 algorithms over the 4540 instances of the PYAsUKP dataset. There was no time limit. The instance classes acronyms stand for Postponed Periodicity (PP); Strongly Correlated (SC); Subset-Sum (SS); and Without Collective Dominance (WCD). 
EDUK was not included because EDUK2 supersedes EDUK. The ordered step-off run times were omitted because they were too similar to the terminating step-off run times. The GREENDP run times over subset-sum instances are not displayed because in this case the two most efficient items have the same efficiency and, therefore, GREENDP behaves as OSO. The mean labels inform the mean run times of each algorithm for the corresponding dataset, in seconds.}
\begin{center} 
<<pya_fast_fig, fig.height = 9.5>>=
fast_plot <- function(t) {
  inst_types <- c('all', 'hi', 'nsds2', 'saw', 'sc', 'ss2')

  data <- t[complete.cases(t), ] # remove SS greendp runs
  data$type <- sapply(data$filename, (function (f) {strsplit(as.character(f), "_")[[1]][1] }))
  data$type <- factor(data$type, levels = inst_types)

  data <- filter(data, algorithm != 'ordered_step_off')
  #data <- group_by(data, filename) %>%
  #  mutate(mean_methods_time = mean(internal_time))
  data <- group_by(data, filename) %>%
    mutate(eduk2_time = internal_time[algorithm == 'pyasukp'])

  data_copy <- data
  data$type <- factor('all', levels = inst_types)

  data <- rbind(data, data_copy) %>% arrange(eduk2_time) #arrange(mean_methods_time)
  data$filename <- factor(data$filename,
        levels = unique(data$filename))

  #data$algorithm <- gsub('ordered_step_off', 'O.S.', data$algorithm)
  data$algorithm <- gsub('terminating_step_off', 'TSO', data$algorithm)
  data$algorithm <- gsub('pyasukp', 'EDUK2', data$algorithm)
  data$algorithm <- gsub('mgreendp', 'GREENDP', data$algorithm)
  pya_dataset_labeller <- as_labeller(c(
    'all' = 'ALL (4540 inst.)',
    'ss2' = 'SS (400 inst.)',
    'saw' = 'SAW (1100 inst.)',
    'nsds2' = 'PP (800 inst.)',
    'sc' = 'SC (240 inst.)',
    'hi' = 'WCD (2000 inst.)'
  ))

  shape_conv <- c('TSO' = 15, 'EDUK2' = 16, 'GREENDP' = 17)

  pya_plot_labels <- data %>%
    select(filename, type, algorithm, internal_time) %>%
    dcast(filename + type ~ algorithm, value.var = 'internal_time') %>%
    select(type, GREENDP, TSO, EDUK2) %>%
    group_by(type) %>%
    summarise(
      mean_greendp = mean(GREENDP, na.rm = T),
      mean_tso = mean(TSO, na.rm = T),
      mean_eduk2 = mean(EDUK2, na.rm = T))

  lpya_plot_labels <- df2rw(pya_plot_labels)
  pya_plot_labels$label <- sapply(lpya_plot_labels, function (row) {
    padd <- 3 + nd(c(
      row[['mean_greendp']],
      row[['mean_tso']],
      row[['mean_eduk2']]))
    paste0(
      sprintf("EDUK2 mean: %*.2f\n", padd, row[['mean_eduk2']]),
      sprintf("TSO   mean: %*.2f\n", padd, row[['mean_tso']]),
      ifelse(is.na(row[['mean_greendp']]), '', sprintf("GREEN mean: %*.2f", padd, row[['mean_greendp']])))
  })
  ggplot(data,
         aes(x = as.numeric(filename),
       y = internal_time,
       color = algorithm,
       shape = algorithm)) + 
    geom_point() +
    scale_y_continuous(trans = 'log10',
           breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
           labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
  #  ggtitle('Benchmark with the 4540 PYAsUKP instances') +
    ylab('Time to solve (seconds, log10 scale)') +
    xlab('Instance indexes sorted by EDUK2 time to solve') +
    scale_colour_grey() +
    scale_shape_manual(name = "algorithm", values = shape_conv) +
    theme(legend.position = 'bottom') +
    facet_wrap( ~ type, ncol = 3, labeller = pya_dataset_labeller) +
    geom_text(
      data = pya_plot_labels,
      aes(
        x = 100,
        y = 250,
        label = label,
        hjust = 0,
        family = 'mono',
        lineheight = 0.80),
      inherit.aes = F)
}

fast_plot(fast_csv)
@

\end{center}
%\legend{Source: the author.}
\label{fig:pya_fast}
\end{figure}

In \autoref{fig:pya_fast}, the instances (x-axis) are sorted by the time EDUK2 spent to solve them.
EDUK2 B\&B phase allows EDUK2 to solve some instances (distributed between all instance sizes) faster than TSO/GREENDP.
This behavior shows that EDUK2 times for a specific instance cannot be predicted based on the number of items and distribution of the instance.
Nonetheless, when the B\&B phase has little effect, the EDUK2 DP phase (basically EDUK) spend considerably more time than TSO/GREENDP to solve the instance.

TSO and GREENDP run times form plateaus in the figure.
For each class of instances, the plateaus aggregate runs over instances with a number of items of similar magnitude.
This behavior shows that the specific items that constitute an instance affect TSO and GREENDP less than the number of items and distribution, both which are good predictors of the TSO and GREENDP run times.

The run times of TSO and GREENDP are similar, except for the SAW instances, in which GREENDP performed considerably worse than TSO.
GREENDP DP phase does not generate solutions including the best item.
The use of the best item allows the generation of more efficient solutions, which dominate less efficient solutions, reducing the total number of solutions generated and, consequently, the computational effort spent.
The authors believe that the exclusion of the best item from the DP phase is the reason that GREENDP presented run times higher than TSO over SAW instances.

When EDUK2 solves an instance in less time than TSO, the difference is often less than a second, and up to ten seconds.
When EDUK2 solves an instance in more time than TSO, the difference is often more than five seconds and up to six minutes.
Such behavior harms EDUK2 mean run time in comparison to simpler DP methods.

\subsubsection{MTU1 and MTU2 (C++ and Fortran)}
\label{sec:mtu_exp}

This section compares the performance of the C++ and Fortran implementations of MTU1 and MTU2 algorithms.
These four implementations were executed over the reduced PYAsUKP benchmark.

\begin{figure}[!htbp]
\caption{Run times of MTU1 and MTU2 (C++ and Fortran) over the 454 instances of the reduced PYAsUKP dataset. The time limit was set to 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit.}
\begin{center}
<<mtu_comp, fig.height=5.49>>=
mtu_time_comp_plot <- function (data) {
  # select columns necessary to the plot, and divide information
  # given in algorithm column in two columns: language and algorithm
  data <- select(data, algorithm, filename, internal_time)
  data$language <- gsub("cpp-mtu[12]", "C++", data$algorithm)
  data$language <- gsub("fmtu[12]", "Fortran", data$language)
  data$language <- factor(data$language, levels = c('C++', 'Fortran'))
  data$algorithm <- gsub(".*1", "MTU1", data$algorithm)
  data$algorithm <- gsub(".*2", "MTU2", data$algorithm)
  data$algorithm <- factor(data$algorithm, levels = c('MTU1', 'MTU2'))

  # compute the mean of the internal_time (for the same filename but
  # with distinct algorithms), and sort the rows by this mean
  data <- group_by(data, filename) %>%
          mutate(fname_mean_time = mean(internal_time)) %>%
      arrange(fname_mean_time)
  # make filename a factor so as.numeric of a filename will give its position
  # in the ordering described above
  data$filename <- factor(data$filename, levels = unique(data$filename))

  #mtu_plot_labels <- data %>%
  #  select(filename, language, algorithm, internal_time) %>%
  #  dcast(filename + algorithm ~ language, value.var = 'internal_time') %>%
  #  select(algorithm, `C++`, `Fortran`) %>%
  #  group_by(algorithm) %>%
  #  summarise(
  #    no_na_cpp = sum(`C++` != 1800),
  #    mean_cpp = mean(`C++`, na.rm = T),
  #    no_na_fortran = sum(`Fortran` != 1800),
  #    mean_fortran = mean(`Fortran`, na.rm = T)
  #  )
  
  #lmtu_plot_labels <- df2rw(mtu_plot_labels)
  #mtu_plot_labels$label <- sapply(lmtu_plot_labels, function (row) {
  #  n_padd <- nd(c(row[['no_na_cpp']], row[['no_na_fortran']]))
  #  m_padd <- 3 + nd(c(row[['mean_cpp']], row[['mean_fortran']]))
  #  paste0(
  #      "C++ n: ", sprintf("%*i", n_padd, row[['no_na_cpp']]),
  #    "\nF77 n: ", sprintf("%*i", n_padd, row[['no_na_fortran']]),
  #    "\nC++ mean: ", sprintf("%*.2f", m_padd, row[['mean_cpp']]),
  #    "\nF77 mean: ", sprintf("%*.2f", m_padd, row[['mean_fortran']]))
  #})
  # plot the graph, no change in data
  p <- ggplot(data,
              aes(x = as.numeric(filename),
              y = internal_time,
          color = language)) +
    geom_point(shape = 3) +
    scale_shape_identity() +
    scale_colour_manual(values = c('C++' = 'black', 'Fortran' = 'darkgrey')) +
    scale_y_continuous(trans = 'log10',
                       breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
                       labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
    ylab("Time to solve\n(seconds, log10 scale)") +
    xlab("Instances indexes sorted by mean time to solve\n(mean between the four implementations) ") +
    theme(legend.position = 'bottom') +
    facet_wrap(~ algorithm, ncol = 1)# +
    #geom_text(
    #  data = mtu_plot_labels,
    #  aes(
    #    x = 280,
    #    y = 1,
    #    label = label,
    #    hjust = 0,
    #    family = 'mono',
    #    lineheight = 0.80),
    #  inherit.aes = F)

  return(p)
}

mtu_time_comp_plot(mtu_csv)
@
\end{center}
%\legend{Source: the author.}
\label{fig:mtu}
\end{figure}

In \autoref{fig:mtu}, both MTU1 implementations show run times in the same order of magnitude.
However, considering only the instances that both MTU1 implementations solved before the timeout, the mean run time of Fortran MTU1 was 59 seconds and the mean run time of C++ MTU1 was 30 seconds.
The analysis of the individual run times shows that for many instances Fortran MTU1 spent about the double of the time spent by C++ MTU1 to solve the same instance.

For many instances, the run times of the MTU2 implementations differed in more than one order of magnitude.
The authors believe that this divergence was caused by the difference of sorting algorithms and items ordering (C++ MTU2 order items by nondecreasing weight if they share the same efficiency).
The subset-sum instances were the ones that exhibited the largest difference.
The range \([190,221]\) of the \(x\) axis of \autoref{fig:mtu} is composed of subset-sum instances.
C++ MTU2 solved all 40 subset-sum instances with a mean time of \(0.04\) seconds while Fortran MTU2 solved only eight instances with a mean time of \(155\) seconds.
Disregarding the subset-sum instances, the mean run time of Fortran MTU2 was 56 seconds and the mean run time of C++ MTU2 was 40.5 seconds.
Only the C++ implementations are used in the rest of the experiments.

<<range_190_121_stat, results = 'hide'>>=
mtu_csv_for_range <- group_by(mtu_csv, filename) %>%
  mutate(fname_mean_time = mean(internal_time)) %>%
  arrange(fname_mean_time)
unique(mtu_csv_for_range$filename)
@

%The subset-sum instances are always naturally sorted by efficiency, as all items share the same efficiency.
%The instances generated by PYAsUKP are also sorted by non-decreasing item weight, what makes them already sorted to C++ MTU2.
%The Fortran MTU2 does not seem to work well with this item ordering of the subset-sum instances.

\subsection{Results on the Realistic Random Dataset}
\label{sec:rr_exp}

The results of the experiments over the realistic random dataset are summarized in \autoref{fig:realistic_random}.

\begin{figure}[!htbp]
\caption{Run times of the algorithms over the 80 instances of the realistic random dataset. The time limit was set to 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit. Shape and color are used to distinguish between algorithms. The shape size is used to indicate the amount of overlapping points. The horizontal position of the points was adjusted for better visualization.}
\begin{center}
<<realistic_random_figure, fig.height = 5>>=
gen_graph_all_alg_discrete_x(rr_csv) +
scale_x_continuous(trans = "log2",
                   breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16, 2^17))
@
\end{center}
%\legend{Source: the author.}
\label{fig:realistic_random}
\end{figure}

The run times of both step-off algorithms and GREENDP become almost identical as the size of the input grows. % TODO: consider removing the three and presenting only one of them
As the instance size grow, EDUK run times get a bit worse than the other three aforementioned algorithms.
EDUK2 has many run times similar to the EDUK but, for some instances in each instance size, its B\&B phase solves the instance or helps to reduce the instance size considerably.
MTU1 and MTU2 have the shortest run times for the smallest instances but, as the instance size and \(w_{min}\) grow, also grow the number of instances in which MTU1 and MTU2 spend orders of magnitude more time to solve than the other algorithms.
%The pure B\&B algorithms (MTU1 and MTU2) show more variation than any other algorithms.

% TODO: check if this will added or not
\begin{comment}
<<realistic_random_table, results='asis'>>=
rr_t <- select(rr_csv, algorithm, n, internal_time)
rr_t <- group_by(rr_t, algorithm, n) %>% summarise(time = mean(internal_time))
rr_t <- dcast(rr_t, algorithm ~ n, value.var = "time")

pretiffy_alg_names_short <- function(names) {
  recode(names,
  'cpp-mtu1'             = 'MTU1 (C++)',
  'cpp-mtu2'             = 'MTU2 (C++)',
  'eduk'                 = 'EDUK',
  'eduk2'                = 'EDUK2',
  'ordered_step_off'     = 'Ord. Step-Off',
  'terminating_step_off' = 'Term. Step-Off',
  'mgreendp'             = 'GREENDP')
}

rr_t$algorithm <- pretiffy_alg_names_short(rr_t$algorithm)
rr_xt <- xtable(rr_t)
caption(rr_xt) <- paste0("Mean time to solve the 80 instances from the ",
  "realistic random dataset with each of seven selected algorithms. ",
  "Runs that exceeded the 1800 seconds timeout counted as taking ",
  "1800 seconds (only affecs MTU1 and MTU2). Each column display the mean ",
  "time to solve the ten instances with the same \\(n\\) value.")

print(rr_xt, 
  include.rownames = F,
  scalebox = 0.91
)
@
\end{comment}

Despite EDUK2 solving some instances orders of magnitude faster than the other algorithms (especially in the larger instance sizes), the mean run time of EDUK2 (8.51 seconds) is higher than the TSO mean run time (5.36 seconds).
As already observed in \autoref{sec:pya_exp}, EDUK2 often presents a few run times that are one order of magnitude higher than the highest run time of TSO, what considerably increases its mean time.

<<rr_means, results = 'hide' >>=
rr_csv2 <- read.csv("../data/realistic_random.csv", sep = ";")
rr_dcast <- dcast(rr_csv2, filename ~ algorithm, value.var = 'internal_time')
summary(rr_dcast)
summary(filter(rr_dcast, grepl('n131072', filename)))
@

\subsection{Results on the BREQ 128-16 Standard Benchmark}
\label{sec:breq_exp}

The results for the BREQ 128-16 Standard Benchmark are summarized in \autoref{fig:breq}.
The run times outline two groups with distinct time growth: a steep-growth group (which hit the time limit) and a gradual-growth group (which always take less than a second).
The steep-growth group is mainly composed of the DP algorithms: TSO, OSO, and EDUK.
The gradual-growth group is mainly composed of the B\&B and hybrid algorithms: MTU1, MTU2, and EDUK2.
GREENDP is the only algorithm with run times in both groups.

\begin{figure}[H]%[!htbp]
\caption{Run times of the algorithms over the 100 instances of the BREQ 128-16 Standard Benchmark. The time limit was set to 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit. Shape and color are used to distinguish between algorithms. The shape size is used to indicate the amount of overlapping points. The horizontal position of the points was adjusted for better visualization.}
\begin{center}
<<breq_figure, fig.height=4.5>>=
gen_graph_all_alg_discrete_x(breq_csv) +
scale_x_continuous(trans = "log2",
                   breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16,
                              2^17, 2^18, 2^19, 2^20))
@

\end{center}
%\legend{Source: the author.}
\label{fig:breq}
\end{figure}

% TODO: check if BREQ table will be kept and then remove this permanently
%<<breq_table, results='asis'>>=
%breq_t <- select(breq_csv, algorithm, n, internal_time)
%breq_t <- group_by(breq_t, algorithm, n) %>% summarise(time = mean(internal_time))
%breq_t <- dcast(breq_t, algorithm ~ n, value.var = "time")

%xtable(breq_t)
%@

EDUK2 run times are in the gradual-growth group due to its B\&B phase.
EDUK2 without its B\&B phase and bounds checking is EDUK, which is in the steep-growth group.
MTU2 core strategy improves MTU1 run times over BREQ instances.
TSO strategy yields no significant gains over OSO in BREQ instances.
GREENDP is an OSO variant which periodically computes bounds (similar to the ones used by the B\&B approach) to verify if the DP can be stopped and any remaining capacity be filled with copies of the best item.
If the bounds stop the DP early then GREENDP run time falls in the gradual-growth group. Otherwise, the run time is similar to the OSO/TSO run time for the same instance.
The results confirm the hypothesis that instances generated with this distribution would be hard to be solved by DP algorithms and easy to be solved by B\&B algorithms.

\subsection{Results on the pricing subproblems from BPP/CSP}
\label{sec:csp_experiments}

The previous experiments included datasets of UKP instances and binaries that read and solved a single instance and then returned the solving time.
In this experiment, the instances are BPP/CSP instances.
Also, the run time for each BPP/CSP instance presented is the sum of all time spent solving the multiple pricing subproblems generated by the column generation approach applied over the continuous relaxation of the set covering formulation.

% The TSO, GREENDP, MTU2 (C++ or Fortran), and EDUK/EDUK2 (PYAsUKP) implementations were not used in this experiment.
Only OSO and MTU1 (C++) algorithms appear in this experiment.
Since the instance capacity for these instances are small, GREENDP has a similar performance than OSO.
%GREENDP is the same as OSO if the two most efficient items share the same efficiency, what often happens for at least one pricing subproblem of a CSP instance.
In instances with a small number of items, MTU2 behaves almost as MTU1, the same applies to the TSO and OSO.
The authors did not succeed in integrating the PYAsUKP code (written in OCaml) with the C++/CPLEX code needed by this experiment.

%CPLEX was also used to solve the pricing problems, adding one more algorithm to the comparison.

In a pricing subproblem, the profit of the items is a real number.
Adapting MTU1 for using floating point profit values is not trivial.%, as the bound computation procedure is based on the assumption that both weight and profit values are integers.
The solution found was to multiply the items profit values by a multiplicative factor, round them down and treat them as integer profit values.
The multiplicative factor chosen was~\(2^{40}\) (approx. \(10^{12}\)).
In a pricing subproblem, the profit of the items can also be non-positive, which breaks the assumptions of some algorithms.
Items with non-positive profits are removed from the item list before passing it to the UKP solving algorithm.

\begin{figure}[!htbp]
\caption{Time spent solving the pricing subproblems from the 6195 CSP instances, with MTU1 and OSO (no sort, integer). % three selected algorithms.
The time limit was set to 30 minutes (the time limit considered the total run time, not only the time spent solving pricing subproblems). If a run is terminated by timeout, the time spent solving pricing subproblems is displayed as exactly the time limit. The labels mean: n -- number of instances in the dataset; OSO/MTU1 n -- number of instances solved before timeout by the algorithm; OSO/MTU1 mean -- mean of the algorithm run times that did not end in timeout.}
\begin{center}
<<knapsack_time_shared>>=
# Note that the points at y = 1800 were introduced to allow
# for the visualization of the runs that resulted in timeout.
# Also, note that after the visible mtu1 point closest to the top right
# all other points are below cplex points (representing the mtu1 timeouts).
# For the hardest problems, there is a small but clear pattern that shows
# advantage of the methods that sort the array by efficiency over the ones
# who does not.

plot_csp <- function (csp_csv) {
  csp_fig <- csp_csv %>%
    filter(
      #algorithm == 'cplex_cutstock'  |
      algorithm == 'mtu1_cutstock'   |
      algorithm == 'ordso_int_ns')
  csp_fig[is.na(csp_fig$pricing_total_time), ]$pricing_total_time <- 1800
  
  csp_fig <- csp_fig %>%
    group_by(filename) %>%
    mutate(mean_methods_time = mean(pricing_total_time)) %>%
    arrange(mean_methods_time)
    #mutate(mean_methods_time = pricing_total_time[algorithm == 'mtu1_cutstock']) %>%
    #arrange(mean_methods_time)
  csp_fig$filename <- factor(
    csp_fig$filename,
    levels = unique(csp_fig$filename))
  
  ggplot(csp_fig,
         aes(x = as.numeric(filename),
             y = pricing_total_time,
             color = algorithm)) +
    #xlab('Instance index when sorted by the mean time\nthe three algorithms spent solving pricing subproblems') +
    geom_point(shape = 3) +
    scale_shape_identity() +
    scale_colour_manual(name = "Algorithm", labels = c(
      'mtu1_cutstock' = 'MTU1',
      'ordso_int_ns' = 'OSO (no sort, integer)'
      # 'cplex_cutstock' = 'CPLEX'
    ), values = c(
      'mtu1_cutstock' = 'darkgrey',
      'ordso_int_ns' = 'black'
      # 'cplex_cutstock' = 'lightgrey'
    )) +
    coord_cartesian(ylim = c(0.001, 1800)) +
    scale_y_continuous(trans = 'log10',
      breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
      labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800'))#,
      #limits = c(0.0005, 1800))
}
@

%<<knapsack_time1, fig.height = 2 >>=
%csp_csv$dataset <- 'All datasets'
%plot_csp(csp_csv) +
%theme(legend.position = 'none') +
%xlab('') +
%ylab('')
%@

<<knapsack_time2, fig.height = 10 >>=
csp_plot_labels <- csp_csv %>%
  select(filename, dataset, algorithm, pricing_total_time) %>%
  dcast(filename + dataset ~ algorithm, value.var = 'pricing_total_time') %>%
  select(dataset, ordso_int_ns, mtu1_cutstock) %>%
  group_by(dataset) %>%
  summarise(
    n = length(dataset),
    no_na_ordso_int_ns = sum(!is.na(ordso_int_ns)),
    mean_ordso_int_ns = mean(ordso_int_ns, na.rm = T),
    no_na_mtu1_cutstock = sum(!is.na(mtu1_cutstock)),
    mean_mtu1_cutstock = mean(mtu1_cutstock, na.rm = T)
  )# %>%
## DAMNED BUGGED MUATATE CANT WORK WITH FUNCTIONS REFERING TWO DIFFERENT
## COLUMNS??
#  mutate(label = paste0(
#      "     n: ", sprintf("%*i", ceiling(log10(n + 1)), n),
#    "\nOSO  n: ", sprintf("%*i", ceiling(log10(n + 1)), no_na_ordso_int_ns),
#    "\nMTU1 n: ", sprintf("%*i", ceiling(log10(n + 1)), no_na_mtu1_cutstock),
#    "\nOSO  mean: ", sprintf("%f", max(mean_mtu1_cutstock, mean_ordso_int_ns)),# mean_ordso_int_ns),
#    "\nMTU1 mean: ", sprintf("%i", mw1(mean_ordso_int_ns))#, mean_mtu1_cutstock)
#  ))

nd2 <- function (a, b) { max(nd(floor(a)), nd(floor(b))) }
lcsp_plot_labels <- df2rw(csp_plot_labels)
csp_plot_labels$label <- sapply(lcsp_plot_labels, function (t) paste0(
  "     n: ", sprintf("%*i", nd(t[['n']]), t[['n']]),
  "\nOSO  n: ", sprintf("%*i", nd(t[['n']]), t[['no_na_ordso_int_ns']]),
  "\nMTU1 n: ", sprintf("%*i", nd(t[['n']]), t[['no_na_mtu1_cutstock']]),
  "\nOSO  mean: ", sprintf("%*.2f", 3 + nd(c(t[['mean_mtu1_cutstock']], t[['mean_ordso_int_ns']])), t[['mean_ordso_int_ns']]),
  "\nMTU1 mean: ", sprintf("%*.2f", 3 + nd(c(t[['mean_mtu1_cutstock']], t[['mean_ordso_int_ns']])), t[['mean_mtu1_cutstock']])))

plot_csp(csp_csv) + facet_wrap( ~ dataset, ncol = 2) +
theme(legend.position = 'bottom') +
xlab('Instance indexes sorted by the sum of the mean time of both algorithms') +
ylab('Total time (seconds, log10 scale)') +
geom_text(data = csp_plot_labels, aes(x = 125, y = 100, label = label, hjust = 0, family = 'mono', lineheight = 0.75), inherit.aes = F)
@
\end{center}
%\legend{Source: the author.}
\label{fig:csp_knapsack_time}
\end{figure}

In \autoref{fig:csp_knapsack_time}, it can be seen that, except by two recent datasets (ANI\&AI and GI), the mean time spent solving pricing problems in an instance is below one second.
Such times are explained by two main factors: i) for completeness, the authors included many classic but old datasets which nowadays are easy to solve; 
ii) these BPP/CSP datasets were meant to be hard to solve exactly, and solving the continuous relaxation of the problem takes only a fraction of that time.
In the majority of these easy datasets, the highest times presented are from MTU1, while OSO presented the lowest mean time.

Many instances of the ANI\&AI dataset exceeded the timeout when MTU1 was used to solve the pricing subproblems.
To study this behavior, next the times of individual pricing problems of the instance 1002\_80000\_DI\_12 (ANI\&AI dataset) are analyzed.
The pricing problems generated by the same BPP/CSP instance always share the same \(n\), \(c\) and items weights. The only difference between the pricing problems is the item profit values\footnote{The specific code used removes items with nonpositive profit values before the beginning of the algorithms. Consequently, \(n\) do vary, but the results are the same.}.
The pricing problems generated when solving this specific instance have \(n = 911\) and \(c = 66432\).

\begin{figure}[!htbp]
\caption{Time spent solving each individual pricing problem generated by instance 1002\_80000\_DI\_12 (ANI\&AI dataset) with MTU1 and TSO. The MTU1 run was killed by timeout (30 minutes), TSO run finished normally.}
\begin{center}
<<csp_inst, fig.height = 4>>=
mtu1_times <- read.csv("../data/mtu1_cutstock_out_ANI_AI_1002_80000_DI_12_txt.out.csv", sep = ";")
ordso_times <- read.csv("../data/ordso_int_ns_cutstock_out_ANI_AI_1002_80000_DI_12_txt.out.csv", sep = ";")
mtu1_times$alg <- 'MTU1'
ordso_times$alg <- 'OSO (no sort, integer)' # TODO: check this at end
both_times <- rbind(mtu1_times, ordso_times)

ggplot(both_times, aes(x = num_iter, y = knapsack_time + sort_time)) +
  geom_point(shape = 3) +
  ylab('Solving time (seconds, log10)') +
  xlab('Number of the pricing problem solved') +
  facet_wrap(~ alg) +
  scale_y_continuous(
    trans = 'log10',
    breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 300, 600),
    labels = c('0.001', '0.01', '0.1', '1', '10', '60', '300', '600')
  )
@
\end{center}
%\legend{Source: the author.}
\label{fig:csp_inst}
\end{figure}

In \autoref{fig:csp_inst}, the times of pricing problems generated by this instance are presented.
It can be seen that the MTU1 run exceed the time limit because the time spent solving the last pricing problems increases exponentially.
The time taken by OSO to solve pricing problems also increases, but one or two orders of magnitude (not five or six).

In \autoref{fig:twelve_pp}, it can be seen that as the profit values of the pricing problems change, their item distribution also change.
The initial distribution is consequence of the set of patterns used to initialize the column generation.
Given the bin size \(c\) and the \(n\) item sizes \(w_i\) (\(i=1,\dots,n\)), the initial set of patterns consists in one pattern for each item \(i\), with \(\floor{c/w_i}\) copies of that item.
Consequently, each item \(i\) with the same \(q = \floor{c/w_i}\) value has the same \(1/q\) profit value in the first iteration.
As patterns including items of different sizes are added, the profit of the items seems to approximate \(c/w_i\).
Instances with such distribution can be harder to solve by B\&B algorithms, as the similar efficiency weaken the capability of the bounds of reducing the solution space.
The authors also do not discard the possibility that using the multiplicative factor has reduced the quality of the MTU1 bounds (and its capability of finishing early) by cutting off some precision of the profit value.

\begin{figure}[!htbp]
\caption{Selected pricing problems generated while solving instance 1002\_80000\_DI\_12 (ANI\&AI dataset) continuous relaxation with MTU1 as pricing problem solver. The titles of the facets follow the format \emph{number of the pricing problem (how many seconds MTU1 spent to solve it)}. For this figure the time limit set was \(90\) hours.}
\begin{center}
<<twelve_pp, fig.height = 10>>=
library(ggplot2)

prefix <- '../csp_pp_csv/1002_80000_DI_12.txt.mtu1.'
times <- read.csv(paste0(prefix, 'times.csv'), sep = ";")
sel_iter <- c(1, 250, 500, 1000, 1500, 2000, 3000, 4250, 4750, 4900, 4950, 4983)#, 4900, 4910)
all_inst <- data.frame()
# create dataframe with all selected pricing problem instances
for (n in sel_iter) {
  curr_inst <- read.csv(paste0(prefix, n, ".csv"), sep = ";")
  curr_inst$n <- n
  all_inst <- rbind(all_inst, curr_inst)
}

times$solving_time <- times$knapsack_time + times$sort_time

my_labeller <- function (x) {
  # damned developers of ggplot2 convert the faceting variable to character
  x <- as.numeric(x)
  paste0(x, " (", sprintf("%.2Es", times$solving_time[x]), ")")
}

ggplot(all_inst, aes(x = w, y = fpph)) +
  geom_point(shape = 3) +
    #  scale_y_log10() +
  ylab('profit') +
  xlab('weight') +
  facet_wrap(~ n, ncol = 3, labeller = as_labeller(my_labeller))
@
\end{center}
\label{fig:twelve_pp}
\end{figure}

To measure the impacts of the multiplicative factor, OSO variants using floating point profit values and using integer profit values were used in the experiments.
The items of a pricing subproblem are always naturally sorted by increasing weight.
The OSO algorithm sorts the items by nonincreasing efficiency but does not depend on this ordering to work.
To verify if the sorting cost would pay off, variants with sorting enabled and disabled were used.
Consequently, four versions of the OSO were used in this experiment, for all combinations of profit type (the original floating point, or the converted integer), and sorting (sorting by nonincreasing efficiency, or not sorting, which is the same as sorting the items by increasing weight).

<<check_all_opt, results = 'hide'>>=
{
  # input: a dataframe with columns filename, algorithm and opt
  # output: a logical value
  # what it does: it checks if all non-NA opt values of different
  #   algorithms but the same filename (instance) match; in other
  #   words, it checks if the algorithms agree in what is the
  #   optimal value of one instance. If they agree, it returns
  #   true, false otherwise.
  # todo: if they did not agree, it should return the filenames
  #   in which they disagree (or the respective table rows)
  check_opt <- function(csv) {
    dt <- dcast(csv, filename ~ algorithm, value.var = 'opt')
    dt <- dt[, !names(dt) %in% c('filename', 'algorithm')] 
    lt <- split(dt, seq(nrow(dt)), drop = T)
    lt <- sapply(lt, function(row) {
      row_no_na <- as.numeric(row)
      row_no_na <- row_no_na[!is.na(row_no_na)]
      length(unique(row_no_na)) <= 1
    })
    length(unique(lt)) == 1
  }
  
  ft <- read.csv("../data/pya_ukp5.csv", sep = ";")
  ft$X <- NULL
  st <- read.csv("../data/mtus_pya.csv", sep = ";")
  st$X <- NULL
  bt <- read.csv("../data/128_16_std_breqd.csv", sep = ";")
  bt$X <- NULL
  rt <- read.csv("../data/realistic_random.csv", sep = ";")
  rt$X <- NULL
  
  check_opt(ft) && check_opt(st) && check_opt(bt) && check_opt(rt) # should be true
}
@
In the experiments with UKP instance datasets, all algorithms agreed on the value of the optimal solution, while sometimes returned different optimal solutions.
In this experiment, the difference in exactly which optimal solution is returned for a pricing subproblem (because of different tie-breaking or because of floating point inaccuracy) changes the next pricing problem, and which are its optimal solutions, in a feedback loop.
However, between MTU1 and the four OSO variants, for the same BPP/CSP instance, no solution of the continuous relaxation (number of rolls needed) differed more than a \(2^{-18}\) fraction of a roll.
Solving pricing problems using the multiplicative factor to work over integer profits seems to be a viable approach. % a numerically stable approach.

<<oso_means, results = 'hide'>>=
summary(dcast(
  csp_csv,
  filename ~ algorithm,
  value.var = 'pricing_total_time'
))
@
%The time spent solving all pricing problems of a specific BPP/CSP instance the four OSO variants. %The distribution of the total time spent solving pricing problems for each BPP/CSP instance was similar between the four OSO variants.
The four OSO variants spent time in the same order of magnitude to solve all pricing problems of the same BPP/CSP instances.
The mean time of the four variants differed significantly: \(1.56\)s (sort, floating point), \(1.21\)s (no sort, floating point), \(1.46\)s (sort, integer), \(1.06\)s (no sort, integer).
Using integer profits show a small but consistent improvement in the times (the time used to convert the value is included), keeping the items in the natural increasing weight order shows a greater and also consistent improvement.
The mean time reduction observed in `no sort' variants did not come from cutting the time spent sorting the items (less than 1\% of the total pricing time), but from how the DP phase of the OSO behaved with a differently sorted list.
The times of the OSO in all figures of this section are from the `no sort'/integer variant.
<<sorting_time_in_csp, results = 'hide'>>=
{
t <- csp_csv %>%
  filter(grepl('ordso_[if]', algorithm)) %>%
  group_by(algorithm) %>%
  summarise(percent_time_sort = 100*(mean(hex_sum_sort_time, na.rm = T)/mean(pricing_total_time, na.rm = T)))
format(t)
}
@

