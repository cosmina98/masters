<<graphs_shared>>=
# number of digits
nd <- function (i) { max(ceiling(log10(i + 1)), na.rm = T) }
# data frame to row list
df2rw <- function (df) { setNames(split(df, seq(nrow(df))), rownames(df)) }
@

<<rr_breq_shared_exp>>=
my_relevel <- function(f) {
  factor(f, levels = c('cpp-mtu1', 'cpp-mtu2', 'eduk', 'eduk2', 'ordered_step_off', 'terminating_step_off', 'mgreendp'))
}

# used by rr and breq
gen_graph_all_alg_discrete_x <- function(csv) {
  alg_labels <- c(
    'cpp-mtu1'             = 'MTU1 (C++)',
    'cpp-mtu2'             = 'MTU2 (C++)',
    'eduk'                 = 'EDUK',
    'eduk2'                = 'EDUK2',
    'ordered_step_off'     = 'Ordered Step-Off',
    'terminating_step_off' = 'Terminating Step-Off',
    'mgreendp'             = 'MGREENDP'
  )
  # I thought of putting one of the legends in an unused area of the plot and
  # another below the plot, but separating legends with ggplot2 needs a hack:
  # https://stackoverflow.com/questions/13143894/how-do-i-position-two-legends-independently-in-ggplot
  # which is too much effort for a little gain.
  p <- ggplot(csv,
              aes(x = n,# * (0.75 + 0.50*(as.numeric(algorithm) - 1)/6),
                  y = internal_time,
                  color = algorithm,
                  shape = algorithm)) + 
    #geom_jitter() +
    #stat_bin2d(binwidth = 0.25, geom="text", aes(label=..count..), position = position_dodge(0.7)) +
    #geom_point() + geom_count() +
    stat_bin_2d(geom = "point",
                aes(size=..count.., alpha = 0.80),
		position = position_dodge(0.75)) +
    scale_alpha(limits = c(0, 1), guide = FALSE) +
    # remove default fill stat_bin_2d legend
    scale_fill_gradient(guide = FALSE) +
    scale_size_area(breaks = c(2, 5, 8),
                    guide = guide_legend(title = "# of points",
                                         title.position = 'top',
                                         direction = 'horizontal',
                                         title.hjust = 0.5)) +
    scale_y_continuous(trans = "log10",
                       breaks = c(10^-3, 10^-2, 10^-1, 1, 10, 60, 600, 1800),
                       labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) +
    scale_colour_manual(name = "Algorithm",
                       labels = alg_labels,
                       values = c('cpp-mtu1' = 'black', 'cpp-mtu2' = 'darkgrey', 'eduk' = 'black', 'eduk2' = 'darkgrey', 'ordered_step_off' = 'black', 'terminating_step_off' = 'darkgrey', 'mgreendp' = 'black')) +
    scale_shape_manual(name = "Algorithm",
                       labels = alg_labels,
                       values = c('cpp-mtu1' = 16, 'cpp-mtu2' = 16, 'eduk' = 17, 'eduk2' = 17, 'ordered_step_off' = 15, 'terminating_step_off' = 15, 'mgreendp' = 4)) +
    ylab('Time to solve (seconds, log10 scale)') +
    xlab('Instance size (number of items, log2 scale)') +
    theme(legend.position = 'bottom')
  #ggsave('realistic_random.pdf', width = 0.80*210, height = 0.5*297, unit = 'mm')
  return(p);
}
@

<<rr_shared>>=
rr_get_n <- function(fname) {
  as.numeric(gsub(".*_n([1-9][0-9]+)-.*", "\\1", fname))
}

rr_csv <- read.csv("../data/realistic_random.csv", sep = ";")
# the duration of runs killed by timeout will be the timeout
rr_csv$internal_time[is.na(rr_csv$internal_time)] <- 1800
rr_csv$n <- rr_get_n(rr_csv$filename)
rr_csv$algorithm <- my_relevel(rr_csv$algorithm)
@

<<breq_shared>>=
breq_get_n <- function(fname) {
  as.numeric(gsub(".*-n([1-9][0-9]+)-.*", "\\1", fname))
}

breq_csv <- read.csv("../data/128_16_std_breqd.csv", sep = ";")
# the duration of runs killed by timeout will be the timeout
breq_csv$internal_time[is.na(breq_csv$internal_time)] <- 1800
breq_csv$n <- breq_get_n(breq_csv$filename)
breq_csv$algorithm <- my_relevel(breq_csv$algorithm)
@

<<mtu_shared>>=
mtu_csv <- read.csv("../data/mtus_pya.csv", sep = ";")
mtu_csv$internal_time[is.na(mtu_csv$internal_time)] <- 1800
@

<<fast_shared>>=
fast_csv <- read.csv("../data/pya_ukp5.csv", sep = ";")
fast_csv$X <- NULL # necessary for complete.cases to work
@

<<csp_shared>>=
csp_csv <- read.csv("../data/csp_6195.csv", sep = ";")
csp_csv$X <- NULL
csp_csv$dataset <- gsub('(.*)/.*', '\\1', csp_csv$filename)
csp_csv$dataset <- gsub('Randomly_Generated', 'Random', csp_csv$dataset)
csp_csv$dataset <- gsub('ANI_AI', 'ANI\\&AI', csp_csv$dataset)
csp_csv$dataset <- gsub('Irnich', 'GI', csp_csv$dataset)
csp_csv$dataset <- factor(csp_csv$dataset)
# forgot to add sort_time to knapsack time in the chart generation codes, now
# changing it here to avoid changing many places
csp_csv$pricing_total_time <- csp_csv$hex_sum_knapsack_time + csp_csv$hex_sum_sort_time
@

\section{Results and Preliminary Analyses}
\label{sec:exp_and_res}

The experiments are split into five subsections.
Each section address one dataset and the results of running some selected algorithms over them.
In order to keep the results close to its discussion, the results of each individual experiment is presented with its immediate analysis.
%The big picture painted by all the experiments is discussed in \autoref{sec:discussion}.

\subsection{PYAsUKP dataset}
\label{sec:pya_exp}

The experiment presented in this section is an updated version of the experiment first presented in~\cite{sea2016}.
In this version GREENDP is considered, UKP5 is replaced by the Terminating Step-Off (TSO), and all runs were executed serially.
The exact same 4540 instances were used.

\begin{figure}[!htbp]
\caption{Run times of the Terminating Step-off, GREENDP and EDUK2 algorithms over the 4540 instances of the PYAsUKP dataset. There was no time limit. The instance classes acronyms stand for: Postponed Periodicity (PP); Strongly Correlated (SC); Subset-Sum (SS); and Without Collective Dominance (WCD). SAW is not an acronym. EDUK was not included because EDUK2 superseeds EDUK. The ordered step-off run times were ommited because they were too similar to the terminating step-off run times. The GREENDP run times over subset-sum instances are not displayed because GREENDP fails if the two most efficient items have the same efficiency, which always happens with subset-sum instances. The mean labels inform the mean run time of each algorithm for the corresponding dataset.}
\begin{center} 
<<pya_fast_fig, fig.height = 9.5>>=
fast_plot <- function(t) {
  inst_types <- c('all', 'hi', 'nsds2', 'saw', 'sc', 'ss2')

  data <- t[complete.cases(t), ] # remove SS greendp runs
  data$type <- sapply(data$filename, (function (f) {strsplit(as.character(f), "_")[[1]][1] }))
  data$type <- factor(data$type, levels = inst_types)

  data <- filter(data, algorithm != 'ordered_step_off')
  #data <- group_by(data, filename) %>%
  #  mutate(mean_methods_time = mean(internal_time))
  data <- group_by(data, filename) %>%
    mutate(eduk2_time = internal_time[algorithm == 'pyasukp'])

  data_copy <- data
  data$type <- factor('all', levels = inst_types)

  data <- rbind(data, data_copy) %>% arrange(eduk2_time) #arrange(mean_methods_time)
  data$filename <- factor(data$filename,
        levels = unique(data$filename))

  #data$algorithm <- gsub('ordered_step_off', 'O.S.', data$algorithm)
  data$algorithm <- gsub('terminating_step_off', 'Terminating Step-Off', data$algorithm)
  data$algorithm <- gsub('pyasukp', 'EDUK2', data$algorithm)
  data$algorithm <- gsub('mgreendp', 'MGREENDP', data$algorithm)
  pya_dataset_labeller <- as_labeller(c(
    'all' = 'ALL (4540 inst.)',
    'ss2' = 'SS (400 inst.)',
    'saw' = 'SAW (1100 inst.)',
    'nsds2' = 'PP (800 inst.)',
    'sc' = 'SC (240 inst.)',
    'hi' = 'WCD (2000 inst.)'
  ))

  shape_conv <- c('Terminating Step-Off' = 15, 'EDUK2' = 16, 'MGREENDP' = 17)

  pya_plot_labels <- data %>%
    select(filename, type, algorithm, internal_time) %>%
    dcast(filename + type ~ algorithm, value.var = 'internal_time') %>%
    select(type, MGREENDP, `Terminating Step-Off`, EDUK2) %>%
    group_by(type) %>%
    summarise(
      mean_greendp = mean(MGREENDP, na.rm = T),
      mean_tso = mean(`Terminating Step-Off`, na.rm = T),
      mean_eduk2 = mean(EDUK2, na.rm = T))

  lpya_plot_labels <- df2rw(pya_plot_labels)
  pya_plot_labels$label <- sapply(lpya_plot_labels, function (row) {
    padd <- 3 + nd(c(
      row[['mean_greendp']],
      row[['mean_tso']],
      row[['mean_eduk2']]))
    paste0(
      sprintf("EDUK2 mean: %*.2f\n", padd, row[['mean_eduk2']]),
      sprintf("TSO   mean: %*.2f\n", padd, row[['mean_tso']]),
      ifelse(is.na(row[['mean_greendp']]), '', sprintf("GREEN mean: %*.2f", padd, row[['mean_greendp']])))
  })
  ggplot(data,
         aes(x = as.numeric(filename),
       y = internal_time,
       color = algorithm,
       shape = algorithm)) + 
    geom_point() +
    scale_y_continuous(trans = 'log10',
           breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
           labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
  #  ggtitle('Benchmark with the 4540 PYAsUKP instances') +
    ylab('Time to solve (seconds, log10 scale)') +
    xlab('Instance index when sorted by EDUK2 time to solve') +
    scale_colour_grey() +
    scale_shape_manual(name = "algorithm", values = shape_conv) +
    theme(legend.position = 'bottom') +
    facet_wrap( ~ type, ncol = 3, labeller = pya_dataset_labeller) +
    geom_text(
      data = pya_plot_labels,
      aes(
        x = 100,
        y = 250,
        label = label,
        hjust = 0,
        family = 'mono',
        lineheight = 0.80),
      inherit.aes = F)
}

fast_plot(fast_csv)
@

\end{center}
%\legend{Source: the author.}
\label{fig:pya_fast}
\end{figure}

In \autoref{fig:pya_fast}, the instances (x axis) are sorted by the time EDUK2 spent to solve them.
EDUK2 B\&B phase allows EDUK2 to solve some instances of every magnitude faster than TSO/GREENDP.
This behaviour shows that EDUK2 times for a specific instance can not be predicted based on the number of items and distribution of the instance, as some instances between all instances that share these characteristics will be remarkably faster to solve by B\&B due to the specific items that constitute the instance.
Nonetheless, when the B\&B phase has little effect, the EDUK2 DP phase (basically EDUK) spend considerably more time than TSO/MGREENDP to solve the instance.

TSO and MGREENDP run times form plateaus.
For each class of instances, the plateaus aggregate runs over instances with number of items of similar magnitude.
This behaviour shows that the specific items that constitute an instance affect TSO and MGREENDP less than the number of items and distribution, both which are good predictors of the TSO and MGREENDP run time.

The run times of TSO and MGREENDP are similar, except for the SAW instances, in which MGREENDP performed considerably worse than TSO.
GREENDP DP phase does not generate solutions including the best item.
The use of the best item allows the generation of more efficient solutions, which dominate less efficient solutions, and reduce the total number of solutions generated and, consequently, the computational effort spent (i.e. the use of the best item would increase the positive effects of weak solution dominance).
The authors believe that the exclusion of the best item from the DP phase is the reason that GREENDP had run times higher than TSO over SAW instances.

When EDUK2 solves an instance in less time than TSO, the difference is often less than a second, and up to ten seconds.
When EDUK2 solves an instance in more time than TSO, the difference is often more than five seconds and up to six minutes.
Such kind of behaviour harms EDUK2 mean run time in comparison to simpler DP methods.

\subsection{MTU1 and MTU2 (C++ and Fortran)}
\label{sec:mtu_exp}

The MTU1 and MTU2 implementations in C++ and Fortran were all executed over the reduced PYAsUKP benchmark.

\begin{figure}[!htbp]
\caption{Run times of MTU1 and MTU2 (C++ and Fortran) over the 454 instances of the reduced PYAsUKP dataset. The time limit was 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit.}
\begin{center}
<<mtu_comp, fig.height=5.5>>=
mtu_time_comp_plot <- function (data) {
  # select columns necessary to the plot, and divide information
  # given in algorithm column in two columns: language and algorithm
  data <- select(data, algorithm, filename, internal_time)
  data$language <- gsub("cpp-mtu[12]", "C++", data$algorithm)
  data$language <- gsub("fmtu[12]", "Fortran", data$language)
  data$language <- factor(data$language, levels = c('C++', 'Fortran'))
  data$algorithm <- gsub(".*1", "MTU1", data$algorithm)
  data$algorithm <- gsub(".*2", "MTU2", data$algorithm)
  data$algorithm <- factor(data$algorithm, levels = c('MTU1', 'MTU2'))

  # compute the mean of the internal_time (for the same filename but
  # with distinct algorithms), and sort the rows by this mean
  data <- group_by(data, filename) %>%
          mutate(fname_mean_time = mean(internal_time)) %>%
	  arrange(fname_mean_time)
  # make filename a factor so as.numeric of a filename will give its position
  # in the ordering described above
  data$filename <- factor(data$filename, levels = unique(data$filename))

  #mtu_plot_labels <- data %>%
  #  select(filename, language, algorithm, internal_time) %>%
  #  dcast(filename + algorithm ~ language, value.var = 'internal_time') %>%
  #  select(algorithm, `C++`, `Fortran`) %>%
  #  group_by(algorithm) %>%
  #  summarise(
  #    no_na_cpp = sum(`C++` != 1800),
  #    mean_cpp = mean(`C++`, na.rm = T),
  #    no_na_fortran = sum(`Fortran` != 1800),
  #    mean_fortran = mean(`Fortran`, na.rm = T)
  #  )
  
  #lmtu_plot_labels <- df2rw(mtu_plot_labels)
  #mtu_plot_labels$label <- sapply(lmtu_plot_labels, function (row) {
  #  n_padd <- nd(c(row[['no_na_cpp']], row[['no_na_fortran']]))
  #  m_padd <- 3 + nd(c(row[['mean_cpp']], row[['mean_fortran']]))
  #  paste0(
  #      "C++ n: ", sprintf("%*i", n_padd, row[['no_na_cpp']]),
  #    "\nF77 n: ", sprintf("%*i", n_padd, row[['no_na_fortran']]),
  #    "\nC++ mean: ", sprintf("%*.2f", m_padd, row[['mean_cpp']]),
  #    "\nF77 mean: ", sprintf("%*.2f", m_padd, row[['mean_fortran']]))
  #})
  # plot the graph, no change in data
  p <- ggplot(data,
              aes(x = as.numeric(filename),
	          y = internal_time,
		  color = language)) +
    geom_point(shape = 3) +
    scale_shape_identity() +
    scale_colour_manual(values = c('C++' = 'black', 'Fortran' = 'darkgrey')) +
    scale_y_continuous(trans = 'log10',
                       breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
                       labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800')) + 
    ylab("Time to solve\n(seconds, log10 scale)") +
    xlab("Instances sorted by mean time to solve\n(mean between the four implementations) ") +
    theme(legend.position = 'bottom') +
    facet_wrap(~ algorithm, ncol = 1)# +
    #geom_text(
    #  data = mtu_plot_labels,
    #  aes(
    #    x = 280,
    #    y = 1,
    #    label = label,
    #    hjust = 0,
    #    family = 'mono',
    #    lineheight = 0.80),
    #  inherit.aes = F)

  return(p)
}

mtu_time_comp_plot(mtu_csv)
@
\end{center}
%\legend{Source: the author.}
\label{fig:mtu}
\end{figure}

In \autoref{fig:mtu}, both MTU1 implementations show run times in the same order of magnitude.
However, considering only the instances that both MTU1 implementations solved before timeout, the mean run time of Fortran MTU1 was 59 seconds and the mean run time of C++ MTU1 was 30 seconds.
The analysis of the individual run times shows that for many instances Fortran MTU1 spent about the double of the time spent by C++ MTU1 to solve the same instance.

For many instances, the run times of the MTU2 implementations differed in more than one order of magnitude.
The authors believe that this divergence was caused by the difference of sorting algorithms and items ordering (C++ MTU2 order items by increasing weight if they share the same efficiency).
The subset-sum instances were the ones that exhibited the greatest difference, C++ MTU2 solved all 40 subset-sum instances with a mean time of \(0.04\) seconds while Fortran MTU2 solved only 8 instances with a mean time of \(155\) seconds.
The range \([190,221]\) of the x axis of \autoref{fig:mtu} is composed of subset-sum instances.
Diregarding the subset-sum instances, the mean run time of Fortran MTU2 was 56 seconds and the mean run time of C++ MTU2 was 40.5 seconds.
Only the C++ implementations are used in the rest of the experiments.

<<range_190_121_stat, results = 'hide'>>=
mtu_csv_for_range <- group_by(mtu_csv, filename) %>%
  mutate(fname_mean_time = mean(internal_time)) %>%
  arrange(fname_mean_time)
unique(mtu_csv_for_range$filename)
@

%The subset-sum instances are always naturally sorted by efficiency, as all items share the same efficiency.
%The instances generated by PYAsUKP are also sorted by non-decreasing item weight, what makes them perfectly sorted to C++ MTU2.
%The Fortran MTU2 does not seem to work well with this item ordering of the subset-sum instances.

\subsection{Realistic Random Dataset}
\label{sec:rr_exp}

The results of the experiments over the realistic random dataset are summarized in \autoref{fig:realistic_random}.
The run times of both step-off algorithms and MGREENDP become almost identical as the size of the input grow. % TODO: consider removing the three and presenting only one of them
EDUK run times are similar to the three above cited algorithms but, as the instance size grow, its times shift for worse. 
EDUK2 has many run times that are similar to the EDUK ones but, for some instances in each size group, its B\&B phase solves the instance or helps to considerably reduce the instance size.
MTU1 and MTU2 have the smallest run times for the smallest instances but, as the instance size and \(w_{min}\) grow, also grow the number of instances in which MTU1 and MTU2 spend orders of magnitude more time to solve than the other algorithms.
%The pure B\&B algorithms (MTU1 and MTU2) show more variation than any other algorithms.

\begin{figure}[!htbp]
\caption{Run times of all selected algorithms over the 80 instances of the realistic random dataset. The time limit was 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit. Shape and color are used to distinguish between algorithms. The shape size is used to indicate the amount of overlapping points. The horizontal position of the points was adjusted for better visualization.}
\begin{center}
<<realistic_random_figure, fig.height = 5>>=
gen_graph_all_alg_discrete_x(rr_csv) +
scale_x_continuous(trans = "log2",
                   breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16, 2^17))
@
\end{center}
%\legend{Source: the author.}
\label{fig:realistic_random}
\end{figure}

% TODO: check if this will added or not
\begin{comment}
<<realistic_random_table, results='asis'>>=
rr_t <- select(rr_csv, algorithm, n, internal_time)
rr_t <- group_by(rr_t, algorithm, n) %>% summarise(time = mean(internal_time))
rr_t <- dcast(rr_t, algorithm ~ n, value.var = "time")

pretiffy_alg_names_short <- function(names) {
  recode(names,
  'cpp-mtu1'             = 'MTU1 (C++)',
  'cpp-mtu2'             = 'MTU2 (C++)',
  'eduk'                 = 'EDUK',
  'eduk2'                = 'EDUK2',
  'ordered_step_off'     = 'Ord. Step-Off',
  'terminating_step_off' = 'Term. Step-Off',
  'mgreendp'             = 'MGREENDP')
}

rr_t$algorithm <- pretiffy_alg_names_short(rr_t$algorithm)
rr_xt <- xtable(rr_t)
caption(rr_xt) <- paste0("Mean time to solve the 80 instances from the ",
  "realistic random dataset with each of seven selected algorithms. ",
  "Runs that exceeded the 1800 seconds timeout counted as taking ",
  "1800 seconds (only affecs MTU1 and MTU2). Each column display the mean ",
  "time to solve the ten instances with the same \\(n\\) value.")

print(rr_xt, 
  include.rownames = F,
  scalebox = 0.91
)
@
\end{comment}

Despite EDUK2 solving some instances orders of magnitude faster than the other algorithms (specially in the larger instance sizes), the mean run time of EDUK2 (8.51 seconds) is higher than the TSO mean run time (5.36 seconds).
As already observed in \autoref{sec:pya_exp}, EDUK2 often have one or more run times that are one order of magnitude higher than the highest run time of TSO, what considerably increases its mean time.

<<rr_means, results = 'hide' >>=
rr_csv2 <- read.csv("../data/realistic_random.csv", sep = ";")
rr_dcast <- dcast(rr_csv2, filename ~ algorithm, value.var = 'internal_time')
summary(rr_dcast)
summary(filter(rr_dcast, grepl('n131072', filename)))
@

\subsection{BREQ 128-16 Standard Benchmark}
\label{sec:breq_exp}

The results of the experiments over the BREQ 128-16 Standard Benchmark are summarized in \autoref{fig:breq}.
The run times outline two groups with distinct time growth, a fast-growth group (which hit the time limit) and a slow-growth group (which always take less than a second).
The fast-growth group is mainly composed by the DP algorithms: TSO, OSO, and EDUK.
The slow-grouth group is mainly composed by the B\&B and hybrid algorithms: MTU1, MTU2, and EDUK2.
GREENDP is the only algorithm with run times in both groups.

\begin{figure}[!htbp]
\caption{Run times of all selected algorithms over the 100 instances of the BREQ 128-16 Standard Benchmark. The time limit was 30 minutes. Runs terminated by timeout are displayed as taking exactly the time limit. Shape and color are used to distinguish between algorithms. The shape size is used to indicate the amount of overlapping points. The horizontal position of the points was adjusted for better visualization.}
\begin{center}
<<breq_figure, fig.height=5>>=
gen_graph_all_alg_discrete_x(breq_csv) +
scale_x_continuous(trans = "log2",
                   breaks = c(2^10, 2^11, 2^12, 2^13, 2^14, 2^15, 2^16,
                              2^17, 2^18, 2^19, 2^20))
@

\end{center}
%\legend{Source: the author.}
\label{fig:breq}
\end{figure}

% TODO: check if BREQ table will be kept and then remove this permanently
%<<breq_table, results='asis'>>=
%breq_t <- select(breq_csv, algorithm, n, internal_time)
%breq_t <- group_by(breq_t, algorithm, n) %>% summarise(time = mean(internal_time))
%breq_t <- dcast(breq_t, algorithm ~ n, value.var = "time")

%xtable(breq_t)
%@

EDUK2 run times are in the slow-growth group due to its B\&B phase.
EDUK2 without its B\&B phase is basically EDUK, which is in the fast-growth group.
MTU2 core strategy improve MTU1 run times over BREQ instances.
TSO strategy yields no significant gains over OSO in BREQ instances.
GREENDP is an OSO variant which periodically compute bounds (similar to the ones used by the B\&B approach) to verify if the DP can be stopped and any remaining capacity be filled with copies of the best item.
If the bounds stop the DP early then GREENDP run time falls in the slow-growth group, otherwise the run time is similar to the OSO/TSO run time for the same instance.
The results confirm the hypothesis that this distribution would be hard to solve by DP algorithms and easy to solve by B\&B algorithms.

\subsection{Solving pricing subproblems from BPP/CSP}
\label{sec:csp_experiments}

The previous experiments included datasets of UKP instances and binaries that read and solved a single instance and then returned the solving time.
In this experiment, the instances are BPP/CSP instances.
Also, the run time for each BPP/CSP instance presented is the sum of all time spent solving the multiple pricing subproblems generated by the continuous relaxation of the Set Covering Formulation (with column generation) of the instance.

% The TSO, GREENDP, MTU2 (C++ or Fortran), and EDUK/EDUK2 (PYAsUKP) implementations were not used in this experiment.
Only the ordered step-off and MTU1 (C++) were used in this experiment.
GREENDP fails if the two most efficient items share the same efficiency, what often happens for at least one pricing subproblem of a CSP instance.
In instances with a small number of items, MTU2 behaves almost the same way that MTU1, the same applies for the terminating step-off and the ordered step-off.
The authors did not succeed in integrating the PYAsUKP code (written in OCaml) with the C++/CPLEX code needed by this experiment.

%CPLEX was also used to solve the pricing problems, adding one more algorithm to the comparison.

In a pricing subproblem, the profit of the items is a real number.
Adapting MTU1 for using floating point profit values proved to be difficult, as the bound computation procedure is based on the assumption that both weight and profit values were integer.
The solution found was to multiply the items profit values by a multiplicative factor, round them down and treat them as integer profit values.
The multiplicative factor chosen was~\(2^{40}\).
In a pricing subproblem, the profit of the items can also be non-positive, which breaks the assumptions of some algorithms.
Items with non-positive profits are removed from the item list before passing it to the UKP solving algorithm.

\begin{figure}[!htbp]
\caption{Time spent solving all pricing subproblems for each of the 6195 instances in the CSP pricing subproblem dataset, with MTU1 and TSO.% three selected algorithms.
The time limit was 30 minutes (the time limit considered the run time, not only the time solving pricing subproblems). If a run is terminated by timeout, the time spent solving pricing subproblems is displayed as exactly the time limit. The labels mean: n -- number of instances in the dataset; \emph{algorithm} n -- number of instances solved before timeout by the algorithm; \emph{algorithm} mean -- mean of the algorithm run times that did not end in timeout.}
\begin{center}
<<knapsack_time_shared>>=
# Note that the points at y = 1800 were introduced to allow
# for the visualization of the runs that resulted in timeout.
# Also, note that after the visible mtu1 point closest to the top right
# all other points are below cplex points (representing the mtu1 timeouts).
# For the hardest problems, there is a small but clear pattern that shows
# advantage of the methods that sort the array by efficiency over the ones
# who does not.

plot_csp <- function (csp_csv) {
  csp_fig <- csp_csv %>%
    filter(
      #algorithm == 'cplex_cutstock'  |
      algorithm == 'mtu1_cutstock'   |
      algorithm == 'ordso_int_ns')
  csp_fig[is.na(csp_fig$hex_sum_knapsack_time), ]$hex_sum_knapsack_time <- 1800
  
  csp_fig <- csp_fig %>%
    group_by(filename) %>%
    mutate(mean_methods_time = mean(hex_sum_knapsack_time)) %>%
    arrange(mean_methods_time)
    #mutate(mean_methods_time = hex_sum_knapsack_time[algorithm == 'mtu1_cutstock']) %>%
    #arrange(mean_methods_time)
  csp_fig$filename <- factor(
    csp_fig$filename,
    levels = unique(csp_fig$filename))
  
  ggplot(csp_fig,
         aes(x = as.numeric(filename),
             y = hex_sum_knapsack_time,
             color = algorithm)) +
    #xlab('Instance index when sorted by the mean time\nthe three algorithms spent solving pricing subproblems') +
    geom_point(shape = 3) +
    scale_shape_identity() +
    scale_colour_manual(name = "Algorithm", labels = c(
      'mtu1_cutstock' = 'MTU1',
      'ordso_int_ns' = 'Ord. Step-Off (no sort, integer)'
      # 'cplex_cutstock' = 'CPLEX'
    ), values = c(
      'mtu1_cutstock' = 'darkgrey',
      'ordso_int_ns' = 'black'
      # 'cplex_cutstock' = 'lightgrey'
    )) +
    coord_cartesian(ylim = c(0.001, 1800)) +
    scale_y_continuous(trans = 'log10',
      breaks = c(0.001, 0.01, 0.1, 1, 10, 60, 600, 1800),
      labels = c('0.001', '0.01', '0.1', '1', '10', '60', '600', '1800'))#,
      #limits = c(0.0005, 1800))
}
@

%<<knapsack_time1, fig.height = 2 >>=
%csp_csv$dataset <- 'All datasets'
%plot_csp(csp_csv) +
%theme(legend.position = 'none') +
%xlab('') +
%ylab('')
%@

<<knapsack_time2, fig.height = 10 >>=
csp_plot_labels <- csp_csv %>%
  select(filename, dataset, algorithm, hex_sum_knapsack_time) %>%
  dcast(filename + dataset ~ algorithm, value.var = 'hex_sum_knapsack_time') %>%
  select(dataset, ordso_int_ns, mtu1_cutstock) %>%
  group_by(dataset) %>%
  summarise(
    n = length(dataset),
    no_na_ordso_int_ns = sum(!is.na(ordso_int_ns)),
    mean_ordso_int_ns = mean(ordso_int_ns, na.rm = T),
    no_na_mtu1_cutstock = sum(!is.na(mtu1_cutstock)),
    mean_mtu1_cutstock = mean(mtu1_cutstock, na.rm = T)
  )# %>%
## DAMNED BUGGED MUATATE CANT WORK WITH FUNCTIONS REFERING TWO DIFFERENT
## COLUMNS??
#  mutate(label = paste0(
#      "     n: ", sprintf("%*i", ceiling(log10(n + 1)), n),
#    "\nOSO  n: ", sprintf("%*i", ceiling(log10(n + 1)), no_na_ordso_int_ns),
#    "\nMTU1 n: ", sprintf("%*i", ceiling(log10(n + 1)), no_na_mtu1_cutstock),
#    "\nOSO  mean: ", sprintf("%f", max(mean_mtu1_cutstock, mean_ordso_int_ns)),# mean_ordso_int_ns),
#    "\nMTU1 mean: ", sprintf("%i", mw1(mean_ordso_int_ns))#, mean_mtu1_cutstock)
#  ))

nd2 <- function (a, b) { max(nd(floor(a)), nd(floor(b))) }
lcsp_plot_labels <- df2rw(csp_plot_labels)
csp_plot_labels$label <- sapply(lcsp_plot_labels, function (t) paste0(
  "     n: ", sprintf("%*i", nd(t[['n']]), t[['n']]),
  "\nOSO  n: ", sprintf("%*i", nd(t[['n']]), t[['no_na_ordso_int_ns']]),
  "\nMTU1 n: ", sprintf("%*i", nd(t[['n']]), t[['no_na_mtu1_cutstock']]),
  "\nOSO  mean: ", sprintf("%*.2f", 3 + nd(c(t[['mean_mtu1_cutstock']], t[['mean_ordso_int_ns']])), t[['mean_ordso_int_ns']]),
  "\nMTU1 mean: ", sprintf("%*.2f", 3 + nd(c(t[['mean_mtu1_cutstock']], t[['mean_ordso_int_ns']])), t[['mean_mtu1_cutstock']])))

plot_csp(csp_csv) + facet_wrap( ~ dataset, ncol = 2) +
theme(legend.position = 'bottom') +
xlab('Instance index when sorted by the mean time\nthe two algorithms spent solving pricing subproblems') +
ylab('Total time (seconds, log10 scale)') +
geom_text(data = csp_plot_labels, aes(x = 125, y = 100, label = label, hjust = 0, family = 'mono', lineheight = 0.75), inherit.aes = F)
@
\end{center}
%\legend{Source: the author.}
\label{fig:csp_knapsack_time}
\end{figure}

In \autoref{fig:csp_knapsack_time}, it can be seen that except by two recent datasets (ANI\&AI and GI) the mean time solving pricing problems in an instance is below one second.
Such times are explained by two main factors: for completeness, the authors included many classic but old datasets; the datasets were meant to be hard to solve by methods solving the BPP/CSP exactly, and solving the continuous relaxation of the problem takes only a fraction of that time.
In the majority of these easy datasets, the highest times presented are from MTU1, giving OSO the lowest mean time.

The pricing problems generated by the same BPP/CSP instance share the same \(n\), \(c\) and items weights, the only significant difference between them is the item profit values\footnote{As the specific code used removes items with nonpositive profit values before the beggining of the algorithms, \(n\) and item weights do vary, but in a way not significantly different than the way they would vary in an algorithm that applies simple dominance over the item list before it starts.}.
For a single example, let us analyse the times of individual pricing problems inside the instance 1002\_80000\_DI\_12.txt of the ANI\&AI dataset, for which MTU1\_CUTSTOCK ended in timeout.
The pricing problems generated when solving this instance have \(n = 911\) and \(c = 66432\).
Before timeout, MTU1 solved about 700 of those pricing problems in much less than a millisecond each.
However, there was also a few pricing problems that took ten seconds or more to solve; run times like: 10, 12, 13, 13, 26, 32, 49, 54, and 351 seconds.
All instances 
Such behaviour corroborates with what was said about B\&B algorithms being strongly affected by the items distribution, and less by \(n\) and \(c\).

%pricing subproblems can have many optimal solutions, and different methods break this tie in different ways.
%The choice of optimal solution will affect the master problem, which will generate a slightly different pricing subproblem.
%This effect cascades and can change the profit values of all next pricing subproblems, and the number of pricing subproblems that are needed to solve (which is the same as the number of iterations, and the number of master problems solved).

To measure the impact of the conversion described above, the author used two versions of the UKP5, one using floating point profit values, and the other using integer profit values and the same conversion described above.
The items from a pricing subproblem are always naturally sorted by increasing weight.
The author executed the two UKP5 variants with sorting enabled and disabled, to verify if the sorting cost would pay off.
Consequently, four versions of UKP5 were used in the tests, for all combinations of profit type (the original floating point, or the converted integer), and sorting (sorting by non-increasing efficiency, or not sorting, which is the same that having the items sorted by increasing weight).
The variants of each one of the four versions of the UKP5 described above to solve pricing subproblems will be referred to as: 

The two traits are: the type used for the profit values (floating point or integer); and if the items were sorted by efficiency or not.

Consequently, four versions of UKP5 were used in the tests, for all combinations of profit type (the original floating point, or the converted integer), and sorting (sorting by non-increasing efficiency, or not sorting, which is the same that having the items sorted by increasing weight).
The author executed the two UKP5 variants with sorting enabled and disabled, to verify if the sorting cost would pay off.
Second, there seems to be an advantage in not sorting the items, and this difference is not caused by the time taken by the sorting procedure.
THE INSTANCES ARE NATURALLY ORDERED BY INCREASING WEIGHT

In every experiment presented in this thesis, the author verified if the optimal solution value (the value of the objective function) was the same for all methods.
Given the innacurate nature of floating point arithmetic, in this experiment, the optimal solution values differed from method to method.

The author found that differences among knapsack solutions because of precision loss, followed by the cascade effect, are common.

It is worth mentioning that almost all instances that ended in timeout are artificial instances created to be hard to solve.
Such instances were proposed in \cite{survey2014}. % TODO: chech which dataset this was and spell it out
The authors can not state that B\&B algorithms are not viable for solving pricing problems, only that there is evidence that the B\&B worst-case can arise in such circumstances.
%Unfortunately, the author of this thesis did not have the time to gather CSP problems from industrial sources, and the experimentation had to stop at the literature instances.

