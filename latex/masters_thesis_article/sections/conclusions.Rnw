\section{General Discussion and Conclusions}
\label{sec:discussion}

With exception of the BREQ dataset, a simple DP algorithm from 1966 (Terminating Step-Off, TSO) had lower mean times than the only known implementation of the current state-of-the-art-algorithm (EDUK2/PYAsUKP).
Such results bring up two main questions the authors will try to address: 1) Why the TSO was abandoned and not considered in recent comparisons? How the TSO outperformed the current state-of-the-art after five decades of study of the UKP?

The authors believe that artificial instances and limited comparisons are the answer to why TSO was not considered in recent comparisons.
As far as the authors know, the proposal of MTU1 was the last time TSO was included in a comparison\cite{mtu1}.
In the experiments presented in~\cite{mtu1}, the TSO algorithm was about four times slower than MTU1 in the instances with~\(n = 25\); about two or three times slower in the instances with~\(n = 50\); and less than two times slower in instances with~\(n = 100\).
Such instances are now too small to consider, and the relative difference between TSO and MTU1 mean times was not even one order of magnitude apart.
Also, the trend observed could indicate that for bigger instances, the OSO/TSO algorithm would have lower times than MTU1, as their relative diference was diminushing. %(e.g. the OSO/TSO algorithm could have a costly initialization process but a better average-case complexity).

An algorithm is dominated by other in the context of a dataset.
The comparisons found in the literature often: 1) only compared a newly proposed algorithm to the algorithm that `won' the last comparison; 2) proposed new artificial datasets based on some definition of being `harder to solve'.
These two characteristics leaded to the following scenario: algorithm B dominates algorithm A in the context of dataset D1; algorithm C dominates algorithm B in the context of dataset D2; nothing guarantees that algorithm A does not dominate algorithm C in the context of dataset D2.
The new dataset proposed is harder to solve because it had characteristics that made it to be harder to solve by some algorithm in the comparison (often the `loser').
It was not considered if those characteristics also negatively affect the algorithms excluded from comparison, or if they are algorithm or approach-specific.
A concrete example of this behavior in the UKP literature follows: MTU2 was compared only to MTU1, and in the context of a new dataset of large \(n\) and rich in simple and multiple dominated items\cite{mtu2}; EDUK was compared only to MTU2, and in the context of new datasets with a smaller \(n\) but with less dominated items and higher \(w_{min}\).
Even if TSO was included in the comparison between MTU1 and MTU2, it would probably perform worse than MTU2 as the dataset favored the B\&B approach.
However, as we can see from the experiments of this work, TSO did perform well on the most recent instance datasets, which favor the DP approach. 
Not specifically EDUK, but any optimized DP algorithm, would win against B\&B in the context of a dataset hard-to-solve by the B\&B approach.

The authors created their own new instance dataset (BREQ) to illustrate how such behaviour could go on.
CONTINUE HERE

The effects of artificial instances in shaping what are considered the best algorithms for UKP is not limited to the instances proposed for the UKP.
The authors of \cite{irnich} created the GI instances to be harder to solve by their their column generation implementation: 
``[...] generated new and harder CS instances. These are characterized by huge values for the capacity (in order to complicate the subproblems) and larger numbers of items with distinct lengths.''\cite[p.~18]{irnich}.
Their implementation used a DP algorithm to solve pricing problems.
The characteristics of these newly proposed BPP/CSP instances made the pricing problems harder to solve by a DP algorithm, but not necessarily by a B\&B algorithm, which is less affected by parameters like the knapsack capacity.
To evaluate which is the best UKP algorithm to solve pricing subproblems, the BPP/CSP instances had also to be representative of real-world instances, otherwise another layer of bias is laiden.
The ANI\&AI instances, which MTU1 had difficulties to solve the pricing problems, are also instances created with the purpose to be hard to solve\cite[p.~22]{survey2014}.
The definition of hard to solve is different between the GI and ANI\&AI instances, as in ANI\&AI instances the objective is to make B\&B algorithms for the BPP/CSP struggle to prove the optimality of a solution for a BPP/CSP instance.
The way such characteristic makes the pricing problems from ANI\&AI instances harder to solve by MTU1 is not so clear as in the case of GI instances and DP algorithms for the UKP.

The answer to how the TSO outperformed EDUK2 seems to be twofold: the evolution of caching techniques and the choice of data structures; the overlook of how TSO simple mechanism already exploited dominance and periodicity before some dominance relations (as threshold and collective dominance) were formally defined.

``In contrast, for UKP we found that a straightforward array-based implementation of the DP approach is faster than the list-based approach. We suspect that on a modern CPU, the smaller state graph of UKP can be accessed much faster (due to caching techniques) so that the solution of the UKP subproblems as they occur in the BP benchmark instances is possible in almost no (measurable) time.''\cite[p.~19]{irnich}

% TRANFORM IN BREQ PARAGRAPH, AND NOTE HOW 
%MTU2 is the best algorithm between eight algorithms for one dataset (see Section \ref{sec:breq_exp}), and is not competitive for other five datasets (see Section \ref{sec:pya_exp}).
%The literature review, and the further discussion of instance datasets and solving approaches, have shown \emph{how the choice of datasets defined the best algorithm through the last fifty years}.
%\emph{reevaluation of the evidence chain.}

%\item The concept of solution dominance, and its implications for periodicity bounds and the four previously established dominance relations.
The technical details of the second contribution (i.e. the concept of solution dominance) were already discussed (see Section \ref{sec:ukp5_sol_dom_expl}), but not how it impacts the previous techniques described in the literature.
The weak solution dominance reduces the further improvement that can be found by applying the four dominance relations and/or periodicity bounds in the same algorithm.
The weak solution dominance and the four dominance relations are two different ways of dealing with the same task.
The first involves keeping an index in each solution to mark which items can still be added to the solution.
The second involves keeping a global list of undominated items, 
%Applying one of the approaches reduce how much the other can further improve the algorithm.

The approach used by EDUK gives a strong guarantee that any dominated item will be discarded and never used again.
However, the weak solution dominance described in Section \ref{sec:ukp5} is implemented with almost no overhead, and seems to have a very similar impact in the removal of dominated items/solutions.
One could argue that EDUK can be at disadvantage for being implemented in a functional language, or that better implementations of the algorithm could be written, the author can not refute such claims.
Maybe new implementations of the EDUK approach can show the superiority of applying the four dominances in the EDUK fashion.
However, for the tested datasets, the weak solution dominance approach seems to be the most efficient one.

The periodicity check exists both in algorithms like EDUK/EDUK2 and the old terminating step-off.
In EDUK/EDUK2 it is a consequence of applying all four dominances repeatedly, and in the terminating step-off it is a consequence of applying weak solution dominance.
A periodicity check can save effort by stopping the computation at a capacity~\(y < c\).
However, in all algorithms that implement the periodicity check, when this early termination happens, it is because the only item that could possibly be used is the best item.
Consequently, in each one of these last positions (between \(y\) and \(c\)), the algorithm would not execute \(O(n)\) steps anymore, but only \(O(1)\) steps.
The periodicity check only saves the effort of iterating these last \(c - y\) positions.
It is a minor improvement over the application of weak solution dominance, or the application of the four item dominances.

The periodicity check (and, by consequence, the dominances) also reduces the utility of periodicity bounds.
If an upper bound on \(y^+\) could be used to stop the computation before it reaches capacity \(c\), then the periodicity check would stop the computation even before the capacity predicted by the upper bound (with slightly more overhead).
In an algorithm with periodicity checking, the utility of upper bounds on the periodicity capacity~\(y^+\) is restricted to saving memory and saving the time spent initializing such saved memory.
Note that some algorithms would not even have such benefits, as they do not allocate or initialize the memory in advance.

The authors do not discard the possibility of a new EDUK2 implementation having better results, nor dismiss the fact that EDUK2 design decision of computing all optimal solutions, can have a slowing effect over it (specially in subset-sum instances).

% MINOR CONTRIBUTIONS?
%\item Evidence that the B\&B approach worst-case can arise when solving pricing subproblems of BPP/CSP.
%\item Evidence that converting the pricing problem profit values to large integers do not cause significant loss.

%While the following quote was written in the context of the 0-1 KP, the author found it relevant to complement what was just said: ``Dynamic programming is one of our best approaches for solving difficult (KP), since this is the only solution method which gives us a worst-case guarantee on the running time, independently on whether the upper bounding tests will work well.''~\cite[p.~13]{where_hard}.

\section{Future work}
\label{sec:future_works}

\begin{itemize}
\item How similar are the datasets of the UKP and the BPP/CSP presented in the literature to the ones existent in the real-world?
\item Do the instances found in the real-world benefit some approaches over others?
\item Would a hybrid algorithm based on the `terminating step-off' and MTU2 present the same level of improvement that EDUK2 has over EDUK?
\item What would be the pratical performance of an implementation of the algorithm described in \cite{babayev}?
\item How do the traits of the optimal solution for the pricing subproblem affect the master problem? Does always returning an optimal solution with minimal weight has a negative effect? What about adding all patterns that improve the master problem solution, and not only the best pattern (i.e. optimal solution)?
\item Are the profit values (and, consequently, the items distributions) of the pricing subproblems uniform between similar BPP/CSP instances, and/or the same BPP/CSP instance? Is it possible that they converge to a specific distribution at each iteration of the column generation?
\item WOULD B\&B ALGORITHMS THAT WEREN'T DEEP FIRST DISPLAY THE SAME PROBLEMS?
\end{itemize}

\subsection{Acknowledgements}

PUT HERE ACKNOWLEDGMENT TO CNPQ, MAYBE PROJECT, POIRREZ

