\section{Methods}

\subsection{UKP instance datasets}

\subsubsection{Uncorrelated items distribution datasets}
\label{sec:inst_uncorrelated}

An uncorrelated items distribution means that the weight and the profit of the items have no correlation.
The most common procedure for generating uncorrelated instances consist in generating \(n\) random integers uniformly distributed in~\([w_{min},w_{max}]\) for the items weights, and repeating the process for~\([p_{min},p_{max}]\) and the item profits.

The average percentage of items that are not simple or multiple dominated in a uncorrelated instance is very small~\cite{zhu_dominated} (about 0.1\% for \(n = 10000\)), and grows smaller with the magnitude of~\(n\).
Consequently, the CONTINUE HERE WITH THE "NOT A GOOD INDICATOR OF GOOD UKP ALGORITHMS"
For this reason, the authors have chosen to not use uncorrelated instances in the experiments.

\subsubsection{PYAsUKP dataset}
\label{sec:pya_inst}

In~\cite[p. 9]{sea2016}, the authors described a dataset comprising 4540 instances from five smaller datasets.
This dataset was heavily based on five datasets presented in~\cite{pya}.
In~\cite{pya} these five datasets are used to compare the newly proposed state-of-the-art algorithm EDUK2 against other algorithms.
The name of the implementation of EDUK2 and the instance generator is PYAsUKP (PYAsUKP: Yet Another solver for the UKP), which is the reason we call this dataset the \emph{PYAsUKP dataset}.
The PYAsUKP dataset comprises: 400 subset-sum instances (\(10^3 \leq n \leq 10^4\); 240 strongly correlated instances (\(5\times10^3 \leq n \leq n = 10^4\)); 800 instances with postponed periodicity (\(2\times10^4 \leq n \leq 5\times10^4\)); 2000 instances without collective dominance (\(5\times10^3 \leq n \leq 5\times10^4\)); 1100 SAW instances (\(10^4 \leq n \leq 10^5\)).

\subsubsection{CSP pricing subproblem dataset}
\label{sec:csp_ukp_inst}

An often mentioned application for solving the UKP is solving the pricing subproblem generated by solving the continuous relaxation of the set covering formulation for the classic Bin Packing Problem (BPP) and Cutting Stock Problem (CSP) using the column generation approach\cite[p. 455--459]{book_ukp_2004}\cite{gg-61}.
In order to avoid that the experiments included only datasets of instances artificially designed to be `hard to solve' (by some or other method), the authors included instances generated by such application.

The authors used 

The previously mentioned Set Covering Formulation (SCF) for BPP and CSP is a tight formulation proposed in~\cite{gg-61}.
The SCF eliminated the problems of the classic formulation that was loose and had too many symmetric solutions.
However, as a consequence, the SCF needs to compute all cutting patterns, i.e. all combinations of sheet sizes that can be cut from a single master roll.
As the cutting patterns are combinations, the amount of cutting patterns can be superexponential in the number of sheet sizes. The exact number of cutting patterns is affected by the sheet sizes. The best case happens when \(\forall i.~w_i > \frac{c}{2}\), in this case the number of cutting patterns is \(n\). The worst case happens when all \(n\) sheet sizes have almost the same size (let us call this size \(w^*\)), and \(n > k = \frac{c}{w^*}\), in this case the number of cutting patterns is given by the binomial coefficient \(\binom{n}{k}\) (that computes \(n!\), which is superexponential).

%It is important to remember that, in this work, our objective is not to solve CSP but its continuous relaxation.

%The column generation approach consists in avoiding the enumeration of all~\(m\) cutting patterns.
%The SCF relaxation is initialized with a small set of cutting patterns that can be computed in polynomial time and in which each sheet size appears at least in one of the patterns.
%This reduced problem is called the \emph{master problem}.
%It is solved by using the simplex method, as it is a linear programming problem.
%A by-product of this solving process are the dual variables of the master problem model.
%Those variables are used as input for a \emph{pricing subproblem}.
%The solution of this pricing subproblem is the cutting pattern that, if added to the master problem, will give the greatest improvement to master problem optimal solution.

%The solving process alternates between solving the master problem and the pricing subproblem, until all cutting patterns that could improve the solution of the master problem are generated and added to the master problem.

%The method described above can generate thousands of UKP instances for one single instance of the CSP.
%For the same instance of the CSP, the number of UKP instances generated, and their exact profit values, can vary based on the choice of optimal solution made by the UKP solver (for the same pricing subproblem many cutting patterns can be optimal, but only one among them is added to the master problem).
%Consequently, such dataset is hard to describe (has a large and variable number of instances with variable profit values).
%The best way found by the author to ensure that the results are reproducible is making available the exact codes used in the experiment, together with the list of CSP instances from the literature used in the experiment.
%The codes are deterministic, and consequently will produce the same results if executed many times over the same CSP instance.

A recent survey on BPP and CSP gathered the instances from the literature, and also proposed new ones~\cite{survey2014}.
The total number of instances in all datasets presented in the survey is 5692.
The author of this thesis chose ten percent of those instances for the experiments presented at Section \ref{sec:csp_experiments}.
This fraction of the instances was randomly selected among instances within the same dataset or, in the larger datasets, the same generation parameters.
The address of a repository containing the data and code used in the above mentioned experiments, and the instructions to compile the code, can be found in \ref{sec:csp_appendix}.

The authors did not worry about implementing many optimizations of the master problem solver described in~\cite{gg-61,gg-63,gg-66}.
The authors believe that these optimizations do not affect considerably the structure of the pricing subproblem.

\subsubsection{Bottom Right Ellipse Quadrant instances}
\label{sec:breq_inst}

The Bottom Right Ellipse Quadrant (BREQ) is an items distribution proposed by the authors. %and first described in REF\_MASTER\_THESIS.
The items of an instance follow the BREQ distribution iff the profits and weights respect~\(p_i = p_{max} - \floor[\big]{\sqrt{p_{max}^2 - w_i^2 \times (\frac{p_{max}}{w_{max}})^2}}\), where \(w_{max}\) (\(p_{max}\)) is an upper bound on the items weight (profit).
The distribution name comes from the fact that its formula describes the bottom right quarter of an ellipse.
%This instance distribution was created to illustrate that different item distributions favors different solution approaches and, therefore, the choice of instances (or specifically, their item distribution) defines what is considered the \emph{best algorithm}.

The purpose of this items distribution is to illustrate the authors' point that artificial distributions can be construed to favor one solving approach over another.
In the case of the BREQ distribution, it favors B\&B over DP.
Distributions with the opposite property (favor DP over B\&B) are common in the recent literature.

The optimal solution of BREQ instances is often in the first fraction of the search space examinated by B\&B algorithms. 
Moreover, the lower bounds from good solutions allow B\&B methods to skip a large fraction of the search space and promptly prove optimality.
In BREQ instances, the presence of simple, multiple and collective dominance is minimal
\footnote{
If the BREQ formula did not include the rounding, the items profit would be a strictly monotonically increasing function of the items weight.
Any item distribution with this property cannot present simple, multiple or collective dominance. 
}, and the threshold dominance is common\footnote{In BREQ instances, an optimal solution will never include the item~\(i\) two or more times if there is an item~\(j\) such as that~\(\sqrt{2} \times w_i \leq w_j \leq 2 \times w_i\).}.
Such characteristics lead to optimal solutions comprised of the largest weight items, that do not reuse optimal solutions for lower capacities.
This means that solving the UKP for lower capacities as DP algorithms do is mostly a wasted effort.

%If those three dominance relations are absent, for any solution~\(s\) composed of two or more items, and for any single item~\(i\), if~\(w_s \leq w_i\) then~\(p_s < p_i\).

% Proof that this interval is tight: http://www.wolframalpha.com/input/?i=2*(100+-+sqrt(100%5E2+-+w%5E2+*+16%5E2))+%3C%3D+100+-+sqrt(100%5E2+-+((sqrt(2)*w)%5E2+*+16%5E2))

The BREQ dataset used in the experiments comprises 10 instances with distinct random seeds for each one of 10 distinct \(n\) values (\(n = 2^{n'}\), where \(n' \in \{11, 12, \dots, 20\}\)), totalling 100 instances.
The values of the remaining parameters can be computed as follows: \(p_{min} = w_{min} = 1\), \(c = 128 \times n\), \(w_{max} = c\) and \(p_{max} = 16 \times w_{max}\).
The items generation procedure follows:
generate~\(n\) unique random integer weights uniformly distributed in~\([w_{min},w_{max}]\);
for each item weight, the corresponding profit is calculated by the formula presented in the first paragraph of this section.

\subsubsection{A Realistic Random Dataset}
% PARAGRAPH ABOUT REALISTIC RANDOM INSTANCES
A dataset of \emph{realistic random} instances was also used in our experiments, our generation procedure is based on~\cite{eduk}.
Our generation procedure follows: generate a list of~\(n\) unique random integers uniformly distributed in~\([min,max]\) and sort them by increasing value; generate a second list repeating the same procedure; combine both lists into a single item list where the weight (profit) of each item~\(i \in [1,n]\) is the value at position \(i\) of the first (second) list; randomly shuffle the item list; generate a random capacity~\(c \in [c_{min},c_{max}]\) (uniform distribution).
Simple dominance can not occur in such instances, other dominances may be present.
Our dataset comprises ten instances with distinct random seeds for each one of eight \(n\) values (\(2^{n'}\), where \(n' \in \{10, 11, \dots, 17\}\)), totalling 80 instances.
The values of the remaining parameters come from \(n\): \(max = n \times 2^{10}\), \(min = \frac{max}{2^4}\), \(c_{min} = 2\times max\) and \(c_{max} = c_{min} + min\).

\subsubsection{Weakly Correlated Instances}

However, no reason was presented for not using the \emph{weakly correlated} distribution, presented in~\cite{mtu1},~\cite{mtu2},~\cite{babayev} and~\cite{eduk}, RR REMOVED.

