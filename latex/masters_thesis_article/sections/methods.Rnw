\section{Methods}
\label{sec:methods}

In this section, the algorithms and datasets used in the experiments are presented.
A rationale for not including some algorithms and datasets from the literature is also presented.

\subsection{Algorithms}

The algorithms used in the experiments can be divided in three main approachs: dynamic programming (DP); branch-and-bound (B\&B); and hybrid (combine DP and B\&B).
The DP approach has a worst-case time complexity of~\(O(nc)\) (pseudo-polynomial), and the its worst-case space complexity is~\(O(n + c)\).
The B\&B approach has an exponential worst-case time complexity (all item combinations that fit the knapsack), but the worst-case space complexity of the B\&B algorithms used in the experiments is~\(O(n)\).
The hybrid algorithms used in the experiments display the same time and space worst-case complexities than the DP approach.

MTU1 is a depth-first B\&B algorithm for the UKP\cite{mtu1}. %with worst-case space complexity \(O(n)\)\cite{mtu1}.
The MTU2 algorithm calls MTU1 over increasing fractions of the items list in an effort to avoid considering all items\cite{mtu2}.
The original Fortran77 implementations of MTU1 and MTU2 available in~\url{}, and the C++ implementations available in~\url{}, were used in the experiments.

CITE ORDERED AND TERMINATING STEP-OFFS

The algorithm proposed in~\cite{green_improv} is described as an improvement of the ordered step-off from~\cite{gg-66}.
The algorithm will be referred as GREENDP, and its implementation in C++ as MGREENDP.
GREENDP is the ordered step-off algorithm, but without using the best item in the DP, and with interruptions at each~\(w_b\) capacity positions, for checking if the DP can be stopped and the remaining capacity filled with copies of the best item.
If two or more items share the greatest efficiency, GREENDP can't compute its bound, and is the same as the ordered step-off.

The EDUK (Efficient Dynamic programming for the Unbounded Knapsack problem) was the first DP algorithm to explicitly check for threshold dominance (a concept proposed together with the algorithm) and collective dominance (that was independently discovered in~\cite{pisinger1994dominance}), it also features a sparse representation of the iteration domain\cite{ukp_new_results}\cite{eduk}\cite[p.~223]{book_ukp_2004}.
EDUK seems to be based on ideas first discussed in~\cite{algo_tech_cut}.
Before EDUK2 was proposed, it was said that ``[...] EDUK [...] seems to be the most efficient dynamic programming based method available at the moment.''\cite[p.~223]{book_ukp_2004}.

EDUK2 is an hybrid DP/B\&B algorithm which improves EDUK with a B\&B preprocessing phase~\cite{pya}.
If B\&B can solve instance using less than a parametrized number of nodes, then EDUK is never executed; otherwise, the bounds computed in the B\&B phase are used to reduce the number of items before EDUK execution and in intervals during its execution.

The implementation of EDUK and EDUK2 used in the experiments was PYAsUKP (PYAsUKP: Yet Another solver for the Unbounded Knapsack Problem), which was written in OCaml by the creators of EDUK and EDUK2. 
The authors were given access to this code by Vincent Poirriez in January 11th, 2016 by email\footnote{The code is available at Henrique Becker master's thesis code repository (\url{https://github.com/henriquebecker91/masters/blob/f5bbabf47d6852816615315c8839d3f74013af5f/codes/ocaml/pyasukp_mail.tgz}).}. The version of the code available at that time in the PYAsUKP official site had bugs\footnote{The EDUK and EDUK2 source was available to download in the following page of PYAsUKP official site: \url{http://download.gna.org/pyasukp/pyasukpsrc.html}.}.

\subsubsection{Algorithms not included in the experiments}

% TODO: DECIDE IF THE ORDERED AND THE TERMINATING WILL BE SHOWN OR ONLY THE TERMINATING
% TODO: JUSTIFY WHY ONLY THE ORDERED/TERMINATING WERE SHOWN (NOT THE PERIODIC)
% ex: the terminating step-off add O(n + c) operations in exchange for the possibility of removing O(c) operations

The authors believe that both the naïve DP algorithm for the UKP\cite[p.~311]{tchu} and an improved version of it presented in~\cite[p.~221]{garfinkel} are dominated by the ordered step-off algorithm presented in \cite[p.~15]{gg-66}. Consequently both the naïve DP algorithm and its improved version were not included in the experiments.

The UKP5 algorithm proposed in~\cite{sea2016} was found to be equivalent to the ordered step-off\footnote{The authors of this article reinvented one algorithm from~\cite{gg-66} and published a paper about it while believing it was novel~\cite{sea2016}. The authors would like to apologize to the academic and scientific community for such disregard.} and therefore only the ordered step-off was included.

The authors' implementation of the first algorithm proposed in~\cite{on_equivalent_greenberg} exceeded every time limit, the second algorithm does not work for all UKP instances\footnote{The authors' implementations of both algorithms were made available at \url{https://github.com/henriquebecker91/masters/blob/e2ff269998576cb69b8d6fb1de59fa5d3ce02852/codes/cpp/lib/greendp.hpp}.}. Both weren't included in the experiments.

In \cite{mtu1}, the B\&B algorithm proposed in~\cite{gg-63} is said to be two times slower than the algorithm proposed in~\cite{cabot}, which is found to be dominated by MTU1; also, the algorithm in~\cite{gg-63} seems to have been abandoned by its authors in favor of the ordered step-off. Also, \cite{cabot}~was behind a paywall. The B\&B algorithms proposed in~\cite{gg-63} and~\cite{cabot} were not included in the experiments.

Likewise, \cite{turnpike}~is behind a paywall, and in~\cite{green_improv} it is implied that GREENDP is an improved version of the algorithm proposed in~\cite{turnpike}, so only GREENDP was included.
The authors could not obtain the code of the algorithm proposed in~\cite{babayev} and had difficulties trying to implement it.
UKP-specific algorithms perform better than applying BKP or 0-1 KP algorithms over converted UKP instances\cite{mtu1}, so BKP and 0-1 KP algorithms were not included.
% TODO: check if BKP and 0-1 KP were written expanded before phrase above

\subsection{Instance datasets}

\subsubsection{PYAsUKP dataset}
\label{sec:pya_inst}

The PYAsUKP dataset is described in~\cite[p. 9]{sea2016}, and comprises 4540 instances from five smaller datasets.
The PYAsUKP dataset was heavily based on five datasets presented in~\cite{pya}, and used to compare EDUK2 to other UKP solving algorithms.
The instance generator used to generate this dataset share the code with the EDUK/EDUK2 implementations (PYAsUKP), which is the reason we call this dataset the \emph{PYAsUKP dataset}.
The PYAsUKP dataset comprises: 400 subset-sum instances (\(10^3 \leq n \leq 10^4\); 240 strongly correlated instances (\(5\times10^3 \leq n \leq n = 10^4\)); 800 instances with postponed periodicity (\(2\times10^4 \leq n \leq 5\times10^4\)); 2000 instances without collective dominance (\(5\times10^3 \leq n \leq 5\times10^4\)); 1100 SAW instances (\(10^4 \leq n \leq 10^5\)).
The authors calls one tenth of the PYAsUKP dataset the ADD MORE HERE?

\subsubsection{CSP pricing subproblem dataset}
\label{sec:csp_ukp_inst}

An often mentioned application for solving the UKP is solving the pricing subproblems generated by solving the continuous relaxation of the Set Covering Formulation  (SCF) for the classic Bin Packing Problem (BPP) and Cutting Stock Problem (CSP) using the column generation approach\cite[p. 455--459]{book_ukp_2004}\cite{gg-61}.
In order to analyse the performance of the algorithms in the context of this application, the authors have written a small C++ program that uses the CPLEX Solver to solve the SCF and feed the pricing problems generated to a custom UKP solving algorithm.

Eight datasets of BPP/CSP instances were used in the experiments (Falkenauer, Scholl, Wäscher, Schwerin, Hard28, Randomly Generated Instances, Augmented Non IRUP and Augmented IRUP Instances, and GI Instances).
These datasets amount to 6195 instances, all available in \url{http://or.dei.unibo.it/library/bpplib}. 
The first seven cited datasets are described in \cite[p.~22]{survey2014}, the last one (GI Instances) comes from~\cite{irnich}.
The code and data details needed to reproduce the experiment are described in \autoref{}.

\subsubsection{Bottom Right Ellipse Quadrant instances}
\label{sec:breq_inst}

The Bottom Right Ellipse Quadrant (BREQ) is an items distribution proposed by the authors in~\cite[p.~37]{ukp_hb_mastersthesis}. %and first described in REF\_MASTER\_THESIS.
The items of an instance follow the BREQ distribution iff the profits and weights respect~\(p_i = p_{max} - \floor[\big]{\sqrt{p_{max}^2 - w_i^2 \times (\frac{p_{max}}{w_{max}})^2}}\), where \(w_{max}\) (\(p_{max}\)) is an upper bound on the items weight (profit).
The distribution name comes from the fact that its formula describes the bottom right quarter of an ellipse.
%This instance distribution was created to illustrate that different item distributions favors different solution approaches and, therefore, the choice of instances (or specifically, their item distribution) defines what is considered the \emph{best algorithm}.

The purpose of this items distribution is to illustrate the authors' point that artificial distributions can be construed to favor one solving approach over another.
In the case of the BREQ distribution, it favors B\&B over DP.
Distributions with the opposite property (favor DP over B\&B) are common in the recent literature.

%The optimal solution of BREQ instances is often in the first fraction of the search space examinated by B\&B algorithms. 
%Moreover, the lower bounds from good solutions allow B\&B methods to skip a large fraction of the search space and promptly prove optimality.
%In BREQ instances, the presence of simple, multiple and collective dominance is minimal
%\footnote{
%If the BREQ formula did not include the rounding, the items profit would be a strictly monotonically increasing function of the items weight.
%Any item distribution with this property cannot present simple, multiple or collective dominance. 
%}, and the threshold dominance is common\footnote{In BREQ instances, an optimal solution will never include the item~\(i\) two or more times if there is an item~\(j\) such as that~\(\sqrt{2} \times w_i \leq w_j \leq 2 \times w_i\).}.
%Such characteristics lead to optimal solutions comprised of the largest weight items, that do not reuse optimal solutions for lower capacities.
%This means that solving the UKP for lower capacities as DP algorithms do is mostly a wasted effort.

%If those three dominance relations are absent, for any solution~\(s\) composed of two or more items, and for any single item~\(i\), if~\(w_s \leq w_i\) then~\(p_s < p_i\).

% Proof that this interval is tight: http://www.wolframalpha.com/input/?i=2*(100+-+sqrt(100%5E2+-+w%5E2+*+16%5E2))+%3C%3D+100+-+sqrt(100%5E2+-+((sqrt(2)*w)%5E2+*+16%5E2))

The BREQ dataset used in the experiments comprises 10 instances with distinct random seeds for each one of 10 distinct \(n\) values (\(n = 2^{n'}\), where \(n' \in \{11, 12, \dots, 20\}\)), totalling 100 instances.
The values of the remaining parameters can be computed as follows: \(p_{min} = w_{min} = 1\), \(c = 128 \times n\), \(w_{max} = c\) and \(p_{max} = 16 \times w_{max}\).
The items generation procedure follows:
generate~\(n\) unique random integer weights uniformly distributed in~\([w_{min},w_{max}]\);
for each item weight, the corresponding profit is calculated by the formula presented in the first paragraph of this section.

\subsubsection{A Realistic Random Dataset}
% PARAGRAPH ABOUT REALISTIC RANDOM INSTANCES
A dataset of \emph{realistic random} instances was also used in our experiments, our generation procedure is based on~\cite{eduk}.
Our generation procedure follows: generate a list of~\(n\) unique random integers uniformly distributed in~\([min,max]\) and sort them by increasing value; generate a second list repeating the same procedure; combine both lists into a single item list where the weight (profit) of each item~\(i \in [1,n]\) is the value at position \(i\) of the first (second) list; randomly shuffle the item list; generate a random capacity~\(c \in [c_{min},c_{max}]\) (uniform distribution).
Simple dominance can not occur in such instances, other dominances may be present.
Our dataset comprises ten instances with distinct random seeds for each one of eight \(n\) values (\(2^{n'}\), where \(n' \in \{10, 11, \dots, 17\}\)), totalling 80 instances.
The values of the remaining parameters come from \(n\): \(max = n \times 2^{10}\), \(min = \frac{max}{2^4}\), \(c_{min} = 2\times max\) and \(c_{max} = c_{min} + min\).

\subsubsection{Unused datasets}

The \emph{uncorrelated} and \emph{weakly correlated} item distributions were commonly used in the literature\cite{mtu1}\cite{mtu2}\cite{babayev}\cite{eduk}, but the authors decided against including them in the experiments.
The literature has already questioned the suitability of \emph{uncorrelated} item distributions datasets for the analysis of the UKP\cite{zhu_dominated}\cite{ukp_new_results}.
Uncorrelated instances often exhibit a great amount of simple and multiple dominated items, and polynomial algorithms can reduce the number of items in such instances by magnitudes.
In the authors experience, uncorrelated instances often take more time to load from disk than to solve.
The solving times of uncorrelated instances are more dependent on the implementation of polynomial-time preprocessing than dependent on the quality of the solving algorithm.
Consequently, the authors do not believe uncorrelated instances provide a good benchmark for UKP solving algorithms.

The \emph{weakly correlated} item distribution can be seen as between the uncorrelated and strongly correlated distributions.
Whereas strongly correlated instances have no simple dominance, weakly correlated can exhibit simple dominance, and multiple dominance is more common.
Without the extra dominated items, which are promptly removed by many algorithms, the weakly correlated instances are very similar to strongly correlated instances of smaller size.
Hence, the authors found redundant to present a weakly correlated dataset together with the strongly correlated datasets already present in the experiments.

