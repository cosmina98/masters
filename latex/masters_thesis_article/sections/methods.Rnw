\section{Methods}
\label{sec:methods}

This section describes the algorithms, their implementations, the instance datasets and the computer setup used.
The authors also present a rationale for not including some algorithms and datasets from the literature.

\subsection{Algorithms and their implementations}

%The algorithms used in the experiments belong to three main approaches: dynamic programming (DP); branch-and-bound (B\&B); and hybrid (combine DP and B\&B).
The usual dynamic programming (DP) algorithms for the UKP have a worst-case time complexity of~\(O(nc)\) (pseudo-polynomial), and worst-case space complexity of~\(O(n+c)\).
The branch-and-bound (B\&B) algorithms have an exponential worst-case time complexity (all item combinations that fit the knapsack), 
but the worst-case space complexity of the B\&B algorithms used in the experiments is~\(O(n)\).
%The hybrid algorithm used in the experiments displays the same time and space worst-case complexities than the DP approach.


% B&B INFO TO ADD: branching scheme, bounds used commonly, ...

MTU1 is a B\&B algorithm for the UKP~\citep{mtu1}. %with worst-case space complexity \(O(n)\)\cite{mtu1}.
In the enumeration tree of MTU1, each node at depth~\(0 \leq d < n\) has~\((c'/w_{d+1}) + 1\) childs (where \(c'\) is the remaining capacity at current node), what means each node except the root represents a valid amount of copies of the item of the current depth being packed in the knapsack (including zero copies).
In practice, by virtue of being depth-first, MTU1 has no explicit tree but only keeps the current solution, which represents the current branch from the root to a leaf.
Items are added to and removed from the solution to simulate the tree traversal.
The exploration order favor the childs representing higher amounts before lower amounts.
As the items are sorted by efficiency, this means the first node after root will be the maximum amount of copies of the best item, and the path to the first leaf will be the solution given by the classic greedy heuristic\footnote{The classic greedy heuristic for the UKP packs the most efficient item that fits the knapsack until there is no item that fits the knapsack.}.
The upper bound used by MTU1 is \(U3\) which is \(max(u1)\)

% DP INFO TO ADD: states, stages, recursive function

The MTU2 algorithm calls MTU1 over the \(x\%\) most efficient items, and if this is not sufficient to obtain a solution with proved optimality, it repeats the process with \(x\%\) increased~\citep{mtu2}.
The Fortran implementation of MTU1 and MTU2 used in our experiments was the original implementation by Martello and Toth but with all 32 bits integers or float variables/parameters replaced by their 64 bits counterparts. This version is publicly available in the authors' code repository\footnote{The MTU1 and MTU2 adapted Fortran code used in the experiments is available at \url{https://github.com/henriquebecker91/masters/tree/136c1c1fbeb6ef7baa7ab6bcc8f48cb0bb68b697/codes}}.
The C++ implementations of both algorithms were written by the authors.
The C++ and Fortran implementations of the MTU1 algorithm have no significant differences.

The MTU2 implementations differed in the algorithm used to partially sort the items and the exact ordering.
\cite{mtu2} does not specify the exact method for performing the partial sorting.
The original Fortran implementation uses a complex algorithm developed by one of the authors of MTU2 in~\cite{partial_sort_martello} to find the k\textsuperscript{th} most efficient item in an unsorted array, and then select and sort only the items that have the same or a greater efficiency.
The C++ implementation uses the \verb+std::partial_sort+ procedure of the standard C++ library \verb+algorithm+.
The Fortran implementation only sorts the items by nonincreasing efficiency; the C++ implementation breaks efficiency ties sorting by nondecreasing weight.

The ordered step-off (OSO) is a DP algorithm proposed in~\cite[p.~15]{gg-66}.
The authors already presented and discussed a revisited version of OSO in~\autoref{sec:oso_and_sol_dom}.
The terminating step-off (TSO) is the same as OSO but it includes periodicity checking.
%which executes \(\theta(n) + O(c)\) extra operations to save \(O(c)\) operations.
\cite{green_improv} proposes another variant of OSO, referred in this paper as GFDP (Greenberg and Feldman's Dynamic Programming).
The GFDP does not use the best item~\(b\), but interrupts the DP at each~\(w_b\) capacity positions to verify if the DP can stop, and the remaining capacity filled with copies of \(b\). 
If two or more items share the greatest efficiency, GFDP verification does not work, and it is the same as OSO. 

The original implementations of the step-offs and GFDP were not publicly available, so the authors wrote their own implementations in C++.
The authors' implementations of TSO and GFDP include the same tiebreaking improvement added by the authors to OSO and described in~\autoref{sec:oso_and_sol_dom}.
All C++ implementations written by the authors are available at the first author's code repository\footnote{The C++ implementations of MTU1, MTU2, the revisited ordered/terminating step-offs, and R-GFDP are available at \url{https://github.com/henriquebecker91/masters/tree/136c1c1fbeb6ef7baa7ab6bcc8f48cb0bb68b697/codes/cpp}.}.
%As the C++ implementations of MTU1 and MTU2, our implementations change the items sorting by nonincreasing efficiency (specified by these three algorithms) to sorting by nonincreasing efficiency with efficiency ties sorted by nondecreasing weight.

EDUK (Efficient Dynamic programming for the Unbounded Knapsack problem) was the first DP algorithm to explicitly check for threshold dominance (a concept proposed together with the algorithm) and collective dominance (that was independently discovered by Pisinger~\citep{pisinger1994dominance}), it also features a sparse representation of the iteration domain~\citep[p.~223]{ukp_new_results,eduk,book_ukp_2004}.
EDUK seems to be based on ideas first discussed in~\cite{algo_tech_cut}.
Before EDUK2 was proposed, it was said that ``[...] EDUK [...] seems to be the most efficient dynamic programming based method available at the moment.''~\citep[p.~223]{book_ukp_2004}.

EDUK2 is a hybrid DP/B\&B algorithm which improves EDUK with a B\&B preprocessing phase~\citep{pya}.
If B\&B can solve the instance using less than a parameterized number of nodes, then EDUK is not executed; otherwise, the bounds computed in the B\&B phase are used to reduce the number of items before EDUK execution and in intervals during its execution.

The implementation of EDUK and EDUK2 used in the experiments was PYAsUKP (PYAsUKP: Yet Another solver for the Unbounded Knapsack Problem), which was written in OCaml.
Vincent Poirriez gave access to this code to the authors, by email,
in January 11th, 2016\footnote{The code is available at Henrique Becker master's thesis code repository (\url{https://github.com/henriquebecker91/masters/blob/f5bbabf47d6852816615315c8839d3f74013af5f/codes/ocaml/pyasukp_mail.tgz}).}.
%The version of the code available at that time in the PYAsUKP official site had bugs\footnote{The EDUK and EDUK2 source was available to download in the following page of PYAsUKP official site: \url{http://download.gna.org/pyasukp/pyasukpsrc.html}.}.

Finally, the authors also implemented the mathematical formulation of the UKP (i.e., Eq. \eqref{eq:objfun}--\eqref{eq:x_integer}) using the C++ interface of both Gurobi~8.0.1~\citep{gurobi} and CPlex~12.8~\citep{cplex} to solve single UKP instances.
To make comparison to other methods fair and the results reproducible, the solvers were configured to: execute in single thread and deterministically (i.e., random seed fixed to zero); finish before time limit only with a \(0\%\) gap between the upper and lower bounds (i.e., search for a proven optimal solution); have the smallest tolerance possible to variable values deviating from integrality (without this about \(9\%\) of the results were slightly below or above the optimal value)\footnote{The exact parameters used for each solver were: GRB\_IntParam\_Threads (Gurobi) and Threads (CPLEX); GRB\_IntParam\_Seed (Gurobi) and RandomSeed (CPLEX); GRB\_DoubleParam\_MIPGap (Gurobi) and MIP::Tolerances::MIPGap (CPLEX); GRB\_DoubleParam\_IntFeasTol (Gurobi) and MIP::Tolerances::Integrality (CPLEX). All CPLEX parameters are prefixed by IloCplex::Param. The codes are available in: \url{https://github.com/henriquebecker91/masters/tree/efea8a981a72237edd17fedb4742ac568ded831c/codes/cpp/lib} (files \emph{cplex\_ukp\_model.hpp} and \emph{gurobi\_ukp\_model.hpp}). }.

\subsubsection{Algorithms deliberately ignored}

%Both the naïve DP algorithm for the UKP~\cite[p.~311]{tchu} and an improved version of it presented in~\cite[p.~221]{garfinkel} are dominated by OSO algorithm presented in~\cite[p.~15]{gg-66}.
The naïve DP algorithm for the UKP~\cite[p.~311]{tchu}, an improved version of it presented in~\cite[p.~221]{garfinkel} and the OSO~\cite[p.~15]{gg-66} are all \(O(nc)\) DP algorithms similar to each other. % These three DP algorithms are \(O(nc)\).
However, OSO does not need to execute \(n\) operations for each distinct \(c\) value and, in practice, will iterate only a small fraction of \(n\) (or even an empty list) for most \(c\) values of most instances.
The other two algorithms \emph{always} execute \(nc\) operations regardless of any instance properties. 
Preliminary tests confirmed that OSO dominated the other two algorithms and, consequently, both were not included in our experiments.
% Consequently, both the naïve DP algorithm and its improved version were not included in our experiments.

The UKP5 algorithm proposed in~\cite{sea2016} was found to be very similar to TSO\footnote{
  The authors of this article reinvented an algorithm from~\cite{gg-66} and published a paper calling it UKP5 while believing it was novel~\cite{sea2016}.
  The authors would like to apologize to the scientific community for such disregard.
  The only improvement of UKP5 over TSO is the tiebreaker change described in \autoref{sec:oso_and_sol_dom}, which the authors included in all algorithms it was applicable.}
and therefore only TSO was included.

The authors' implementation of the first algorithm proposed in~\cite{on_equivalent_greenberg} exceeded the time limits we used in the experiments, while the second algorithm does not work for all UKP instances\footnote{The authors' implementations of both algorithms were made available at \url{https://github.com/henriquebecker91/masters/blob/e2ff269998576cb69b8d6fb1de59fa5d3ce02852/codes/cpp/lib/greendp.hpp}.}.
Both weren't included in the experiments.
The Sage-3D algorithm from \cite{landa_sage} cited in \cite{ukp_hu_landa_shing_survey} needs \(\O(n w_b p_b)\) memory and time, which is prohibitive for many instances considered and, therefore, was also not included.
Its complexity is justified by the fact Sage-3D does not solve the UKP for a specific knapsack capacity, but instead builds a data structure which allows querying the solution for a specific capacity in \(O(log(p_b))\).

In~\cite{mtu1}, the B\&B algorithm proposed in~\cite{gg-63} is said to be two times slower than the algorithm proposed in~\cite{cabot}, which was found to be dominated by MTU1; 
also, the algorithm in~\cite{gg-63} seems to have been abandoned by its authors in favor of OSO. %Also, \cite{cabot}~was behind a paywall.
Thus, the B\&B algorithms proposed in~\cite{gg-63} and~\cite{cabot} were not included in the experiments.

%Likewise, \cite{turnpike}~is behind a paywall, and 
In~\cite{green_improv} it is implied that GFDP is an improved version of the algorithm proposed in~\cite{turnpike}, so only GFDP was included.
The authors could not obtain the code of the algorithm proposed in~\cite{babayev}, and they chose to not reimplement it to not risk misrepresenting it, as it was not trivial to implement.

UKP-specific algorithms perform better than applying BKP or 0-1 KP algorithms over converted UKP instances~\cite{mtu1}, so BKP and 0-1 KP algorithms were not considered.

\subsection{Instance datasets}

The datasets used in the experiments include: artificial datasets from the literature that focus on being hard-to-solve (PYAsUKP and realistic random datasets); a dataset proposed by the authors in order to prove an hypothesis (BREQ dataset); a dataset based on solving of CSP/BPP instances with the column generation technique (CSP dataset).
The reasoning for not including \emph{uncorrelated} and \emph{weakly correlated} instances is presented in the end of the section.

\subsubsection{PYAsUKP dataset}
\label{sec:pya_inst}

The PYAsUKP dataset is described in~\cite{sea2016}, and comprises 4540 instances from five smaller datasets.
The PYAsUKP dataset was heavily based on the datasets presented in~\cite{pya}, which were used to compare EDUK2 to other UKP solving algorithms.
The instance generator used to generate the 4540 instances share the code with EDUK/EDUK2 implementations (PYAsUKP), which is the reason we call this dataset the \emph{PYAsUKP dataset}.
The PYAsUKP dataset comprises: 400 subset-sum instances (\(10^3 \leq n \leq 10^4\)); 240 strongly correlated instances (\(5\times10^3 \leq n \leq n = 10^4\)); 800 instances with postponed periodicity (\(2\times10^4 \leq n \leq 5\times10^4\)); 2000 instances without collective dominance (\(5\times10^3 \leq n \leq 5\times10^4\)); 1100 SAW instances (\(10^4 \leq n \leq 10^5\)).
The PYAsUKP dataset has multiple-of-ten amounts of instances generated with different random seeds for each combination of the remaining generation parameters (\(n\), \(w_{min}\), \dots).
The authors selected the first one-tenth of the instances for each parameter combination (in total 454) and will refer to it as the \emph{reduced PYAsUKP dataset}.

\subsubsection{Realistic Random Dataset}
A dataset of \emph{realistic random} instances was used in the experiments. The generation procedure, based on~\cite{eduk}, is summarized as follows: generate two lists of~\(n\) unique random integers uniformly distributed in~\([min, max]\) and sort them by increasing value; combine both lists in an item list, by pairing up the i-th of one list to the i-th element of the other; randomly shuffle the item list; generate a random capacity~\(c \in [c_{min},c_{max}]\) (uniform distribution).
Simple dominance cannot occur in such instances; other dominances may be present.
Our dataset comprises ten instances generated with distinct random seeds for each one of eight \(n\) values (\(2^{n'}\), where \(n' \in \{10, 11, \dots, 17\}\)), totalling 80 instances.
The values of the remaining parameters come from \(n\): \(max = n \times 2^{10}\), \(min = max/2^4\), \(c_{min} = 2\times max\), and \(c_{max} = c_{min} + min\).

\subsubsection{BREQ 128-16 Standard Benchmark}
\label{sec:breq_inst}

The Bottom Right Ellipse Quadrant (BREQ) is an items distribution proposed in~\cite{ukp_hb_mastersthesis}. %and first described in REF\_MASTER\_THESIS.
The items of an instance follow the BREQ distribution iff the profits and weights respect~\(p_i = p_{max} - \floor[\big]{\sqrt{p_{max}^2 - w_i^2 \times (p_{max}/w_{max})^2}}\), where \(w_{max}\) (\(p_{max}\)) is an upper bound on the items weight (profit).
The distribution name comes from the fact that the formula describes the bottom right quarter of an ellipse.
%This instance distribution was created to illustrate that different item distributions favor different solution approaches and, therefore, the choice of instances (or specifically, their item distribution) defines what is considered the \emph{best algorithm}.

The purpose of this items distribution is to illustrate the authors' point that artificial distributions can be developed to favor one solving approach over another.
In the case of the BREQ distribution, it favors B\&B over DP.
Distributions with the opposite property (favor DP over B\&B) are common in the recent literature.

The optimal solution of BREQ instances is often in the first fraction of the search space examined by B\&B algorithms. 
Moreover, the lower bounds from good solutions allow B\&B methods to skip a large fraction of the search space and promptly prove optimality.
In BREQ instances, the presence of simple, multiple and collective dominance is minimal
\footnote{
If the BREQ formula did not include the rounding, the profit of the item would be a strictly monotonically increasing function of the items weight.
Any item distribution with this property cannot present simple, multiple, or collective dominance. 
}, but threshold dominance is very common: an optimal solution will never include the item~\(i\) two or more times if there is an item~\(j\) such as that~\(\sqrt{2} \times w_i \leq w_j \leq 2 \times w_i\).
Such characteristics lead to optimal solutions comprised of the largest weight items, which do not reuse optimal solutions for lower capacities.
This means that solving the UKP for lower capacities as DP algorithms do is mostly a wasted effort.

%If those three dominance relations are absent, for any solution~\(s\) composed of two or more items, and for any single item~\(i\), if~\(w_s \leq w_i\) then~\(p_s < p_i\).

% Proof that this interval is tight: http://www.wolframalpha.com/input/?i=2*(100+-+sqrt(100%5E2+-+w%5E2+*+16%5E2))+%3C%3D+100+-+sqrt(100%5E2+-+((sqrt(2)*w)%5E2+*+16%5E2))

The authors named the BREQ dataset used in the experiments of \emph{BREQ 128-16 Standard Benchmark}.
This dataset comprises 100 instances generated from all combinations of ten random seeds and ten distinct \(n\) values  defined as \(n = 2^{n'}\), where \(n' \in \{11, 12, \dots, 20\}\).
The values of the remaining parameters can be computed as follows: \(p_{min} = w_{min} = 1\), \(c = 128 \times n\), \(w_{max} = c\) and \(p_{max} = 16 \times w_{max}\).
The items generation procedure follows:
generate~\(n\) unique random integer weights uniformly distributed in~\([w_{min},w_{max}]\);
for each item weight, the corresponding profit is calculated by the formula presented in the first paragraph of this section.

\subsubsection{CSP pricing subproblem dataset}
\label{sec:csp_ukp_inst}

An often mentioned UKP application is to solve the pricing subproblems generated by the linear programming relaxation of the Set Covering Formulation (SCF) for the Bin Packing Problem (BPP) and Cutting Stock Problem (CSP) using the column generation approach~\citep[p. 455--459]{book_ukp_2004}\citep{gg-61}.
To analyze the performance of the algorithms in the context of this application, the authors have written a C++ program that uses the CPLEX Solver to solve the SCF and feed the pricing problems generated to a custom UKP solving algorithm.

The experiments included eight datasets of BPP/CSP instances. The datasets are: Falkenauer (160 instances), Scholl (1210 instances), Wäscher (17 instances), Schwerin (200 instances), Hard28 (28 instances), Randomly Generated Instances (3840 instances), Augmented Non IRUP and Augmented IRUP Instances (ANI \&AI, 500 instances), and Gschwind and Irnich instances (GI instances, 240).
These datasets amount to 6195 instances, all made available in~\cite{Delorme2018}. %\url{http://or.dei.unibo.it/library/bpplib}. 
The first seven datasets are described in~\cite{survey2014}, the last one (GI Instances) comes from~\cite{irnich}.
The code used to solve the SCF relaxation can be found in the first author's repository\footnote{The C++/CPLEX code used for solving SCF relaxation is available at \url{https://github.com/henriquebecker91/masters/tree/8367836344a2f615640757ffa49254758e99fe0a/codes/cpp}. The code can be compiled by executing \emph{make bin/cutstock} in the folder.
The dependencies are the Boost C++ library (see: \url{http://www.boost.org/}), and IBM ILOG CPLEX Studio 12.5 (see: \url{https://www.ibm.com/developerworks/community/blogs/jfp/entry/cplex_studio_in_ibm_academic_initiative?lang=en})}.

\subsubsection{Datasets deliberately ignored}

The \emph{uncorrelated} and \emph{weakly correlated} item distributions were commonly used in the literature~\citep{mtu1,mtu2,babayev,eduk}, but the authors decided to not include them in the experiments.
The literature has already questioned the suitability of \emph{uncorrelated} item distributions datasets for the analysis of the UKP~\citep{zhu_dominated, ukp_new_results}.
Uncorrelated instances often exhibit a vast amount of simple and multiple dominated items, and polynomial algorithms can reduce the number of items in such instances by orders of magnitude.
%In the author's experience, uncorrelated instances often take more time to load from disk than to solve.
%The solving times of uncorrelated instances are more dependent on the implementation of polynomial-time preprocessing than dependent on the quality of the solving algorithm.
%Consequently, the authors do not believe uncorrelated instances provide a good benchmark for UKP solving algorithms.

The \emph{weakly correlated} item distribution can be seen as a \emph{strongly correlated} item distribution with more dominated items.
The authors found redundant to present weakly correlated datasets in addition to the strongly correlated datasets, as preliminary results suggested that the time spent solving weakly correlated datasets was similar to the time spent solving strongly correlated datasets of smaller size.


\subsection{Computer setup}

All experiments were run using a computer with the following characteristics:
the CPU was an Intel\textsuperscript{\textregistered} Core\textsuperscript{TM} i5-4690 CPU @ 3.50GHz;
there were 8GiB RAM available (DIMM DDR3 Synchronous 1600 MHz) and three levels of cache (256KiB, 1MiB, and 6MiB, with the cores sharing only the last one).
The operating system used was GNU/Linux 4.7.0-1-ARCH x86\_64 (i.e., Arch Linux). 
Three of the four cores were isolated using the \emph{isolcpus} kernel flag (the non-isolated core was left to run the operating system). 
The \emph{taskset} utility was used to execute runs in one of the isolated cores.
All runs were executed in serial order, as the authors found that parallel executions effected the run times, even if each isolated core only hosted one run at each time~\citep[p.~87]{ukp_hb_mastersthesis}.

The OCaml code (PYAsUKP/EDUK/EDUK2) was compiled with ocamlopt and the flags suggested by the authors of the code for maximum performance (\emph{-unsafe -inline 2048}).
The Fortran code (original MTU1/MTU2) was compiled with gcc-fortran and \emph{-O3 -std=f2008} flags enabled.
The C++ code (all remaining implementations) was compiled with gcc and the \emph{-O3 -std=c++11} flags enabled.

