\section{Methods}

\subsection{Prior Work}

\subsubsection{Algorithms from the literature}

This literature review starts with~\cite{gg-61}, when the \emph{column generation} approach was proposed.
% TODO: put the phrase from introduction here
%The main utility of the column generation approach was to avoid the existence of an exponential number of variables when solving the tightest linear programming model of BPP and CSP.
%The relationship between the UKP and the BPP/CSP was already briefly described at Section~\ref{sec:motivation}, and its technical details will be described at Section~\ref{sec:csp_ukp_inst}.
%The UKP is not solved, it is only said that ``the auxiliary problem will be of the integer programming variety, but of such a special type (the `knapsack' type) that it is solvable by several methods''~\cite[p.~2]{gg-61}.
%Two years later, in~\cite{gg-63}, the authors proposed a specific algorithm for the UKP, and experiments solving BPP and CSP instances were executed.
%Some findings of this experiments will be discussed in Sections~\ref{sec:csp_ukp_inst} and~\ref{sec:csp_experiments}.

%In this paper, the algorithm for UKP that was first described at~\cite{gg-63} is discussed more profoundly.
%In~\cite{gg-66}, the one-dimensional and two-dimensional knapsack problems related to BPP and CSP were discussed.
%The author of this thesis reinvented one algorithm from~\cite{gg-66} and published a paper about it, believing it was novel~\cite{sea2016}, thus, he apologizes to the academic and scientific community for such disregard.
%Further information about the algorithms of~\cite{gg-66} and~\cite{sea2016} can be found in Section~\ref{sec:dp_algs}.
A small improvement over the algorithm of~\cite{gg-66} was proposed in~\cite{green_improv}.
%The author implemented the improved algorithm and its results can be seen in Section~\ref{sec:pya_exp}.

% MTU1 and MTU2 papers:
In the 1970's, there was a shift of attention from the DP approach to the B\&B approach.
The first algorithms using this approach seem to be the Cabot's enumeration method~\cite{cabot} and the MTU1 algorithm~\cite{mtu1}.

MTU1 was proposed in~\cite{mtu1}, with the name of KP1 at the time (we will refer to this paper as the `MTU1 paper'). % old paper of MTU1 %To the author's knowledge, the MTU1 paper was the only paper to present experimental results comparing the B\&B and DP methods, before PYAsUKP paper, in 2009\cite{pya}.
Unfortunately, by current standards, the instances used in the comparison were very small (which is understandable considering the paper publishing date). 
The numbers of items used were 25, 50 and 100, for instance; the weights (and profits) had values between 11 and 110 (in the case of the profits, 120); the knapsack capacity was chosen between~\(0.2 \sum_{i \in n}{w_i}\) and~\(\sum_{i \in n}{w_i}\); the distributions used were uncorrelated and weakly correlated (\(p_i = w_i + \alpha\), where~\(\alpha\) was randomly chosen from -10 and 10 following an uniform distribution).

The comparison presented in~\cite{mtu1} was between KP1 (MTU1), the dynamic programming algorithm called `periodic step-off' from~\cite{gg-66}, that we will call G.G. for short, and two B\&B algorithms for the 0-1 KP (for which the UKP instances were transformed in 0-1 KP instances).
The best results were from MTU1, and the second best from the G.G. algorithm.
However, the instances were too small to draw strong conclusions, and the relative difference between G.G. and MTU1 average times was not even one order of magnitude apart.
The G.G. algorithm was about four times slower than MTU1 in the instances with~\(n = 25\); about two or three times slower in the instances with~\(n = 50\); and less than two times slower in instances with~\(n = 100\).
This trend could indicate that for big instances, the G.G. algorithm would have better times than MTU1 (e.g. the G.G. algorithm could have a costly initialization process but a better average-case asymptotic complexity).

The MTU2 algorithm was designed for large instances (up to 250,000 items).
Only sorting the items list was already computationally expensive for the period, and the solutions often involved only the most efficient items.
The MTU2 main feature was grouping and sorting only the~\(k = max(100, \frac{n}{100})\) most efficient items, and calling MTU1 over them.
The UKP instance consisting of this reduced items list and the original knapsack capacity was called `tentative core problem'.
If the optimal solution of the tentative core problem was proven to be optimal for the original problem, the algorithm stopped.
Otherwise, the optimal solution of the tentative core problem was used as a lower bound to remove dominated items.
After this, the~\(k\) most efficient items outside the tentative core problem were added to it, restarting the process. 

The algorithms comparison included only MTU1 and MTU2.
The datasets used in the paper were large, but artificial and abundant in dominated items.
The MTU2 was clearly the best algorithm for the chosen datasets.

MTU2 was adopted by the subsequent works as the baseline for testing new algorithms for the UKP.
We believe this happened due to many factors, such as: the code of MTU2 was freely available; the algorithm was well and thoroughly explained in Martello and Toth's publications; it presented empirical evidence of dominating other methods and, consequently, comparing with it would remove the necessity of comparing to many other algorithms; the description of MTU2 stated that it was designed for large instances.
However, MTU2 does not completely dominate MTU1, it simply was better for the chosen instances (that were chosen with the purpose of evidencing this).
Instances in which the MTU2 needs to repeat the process of adding items to the tentative core problem many times can be more easily solved by MTU1 than by MTU2.
Unfortunately, the works that followed chose to compare their algorithms only against MTU2.

EDUK (\emph{Efficient Dynamic programming for the Unbounded Knapsack problem}), a novel DP algorithm for the UKP, was proposed in a conference paper~\cite{ukp_new_results} and then presented in a journal paper~\cite{eduk}.
EDUK is very different from the previous DP algorithms, and its main features are the application of threshold dominance (proposed in the same paper), and the use of a sparse representation of the iteration domain.
This last feature was implemented by using lazy lists, mainly because EDUK was implemented in the functional language OCaml.
EDUK is strongly based on the ideas first discussed in~\cite{algo_tech_cut}.

In~\cite{eduk}, the authors criticize the item distributions used in previous papers, especially the uncorrelated distribution.
The author of this thesis agrees with this criticism, further discussion can be found in Section~\ref{sec:inst_uncorrelated}.
However, the solution given for this problem were new datasets of artificial instances.
The new datasets do not have simple dominated items, or small efficient items, as the previous datasets, and one of them does not even have any collective dominated items.
The change in the choice of items distributions benefits DP methods (and consequently EDUK), which are better suited for such kind of instances.
When the new datasets are used, the comparison between MTU2 and EDUK shows that the average times of MTU2 are strongly dominated by the ones of EDUK.

The weakly and strongly correlated distributions are also used in \cite{eduk}, but varying the value of~\(w_{min}\).
For those instances, MTU2 dominates EDUK when the weight of the smallest item is close to one, but MTU2 times grow much faster than EDUK times when~\(w_{min}\) is increased.
Only one comparison is made against another DP algorithm.
The DP algorithm used seems to be a na{\"i}ve DP algorithm with a preprocessing phase that removes simple dominance.
The comparison uses a completely different dataset of small instances, in an effort to take into account real-world applications of the UKP, as the ones provenient from solving BPP and CSP with column generation.
The average run times in this comparison are smaller than 0.1 seconds, and the difference between the average times of EDUK and the naive DP are about 20\% (with EDUK being faster).

EDUK2 is an improvement of EDUK proposed in~\cite{pya}.
The main improvement brought up by EDUK2 is the hybridization of EDUK with the B\&B approach.
A B\&B preprocessing phase was added to EDUK.
If it solves the instance using less than a parametrized number of nodes, then EDUK is never executed; otherwise, the bounds computed in the B\&B phase are used to reduce the number of items before EDUK execution and in intervals during its execution.
The paper also proposes a new bound for a subset of the strongly correlated instances (the SAW instances), which that is the tightest bound known for such instances.
Comparisons are performed with EDUK and MTU2.
EDUK2 is clearly the winner, but the average solution times of the methods are few seconds, or less than a second.
The experiments are then remade using the same distributions with larger coefficients.
MTU2 has integer overflow problems and is left out of the comparison.
Between EDUK and EDUK2, EDUK2 has the best results, as expected. 

% TODO: summarize paragraph below in one or two lines
Both~\cite{eduk} and~\cite{pya} cite~\cite{babayev}, which presents an algorithm for solving the UKP using the Consistency Approach (CA).
The algorithm described in~\cite{babayev} was tested against MTU2 and had better times, but the instances used in the experiment make it difficult to have an idea of what would be its performance using more recent datasets (see Section~\ref{sec:babayev_uncorrelated} for further discussion).
The CA was already discussed in~\cite{on_equivalent_greenberg}.
However, the algorithm proposed in~\cite{babayev} considered performance as a priority, different from previous works that treated applying CA to the UKP as an interesting theoretical problem.
As the authors of~\cite{eduk} and~\cite{pya}, we tried to obtain a copy of the code from the authors of~\cite{babayev}, but did not obtain success.
The author of this thesis suggests the implementation and comparison of this algorithm as a future work.

% This corresponds to widely-used practices and general beliefs expressed in the literature. Usually, in the context of an LP- based branch-and-bound algorithm, where the LP relaxation is solved using column generation, cutting planes are carefully selected in order to avoid the destruction of the structure of the pricing problem. This viewpoint is shared by numerous authors. Barnhart, Hane, and Vance [BHV00], for example, mention as one of the main contributions of their branch-and-price-and-cut algorithm ``a pricing problem that does not change even as cuts are added, and similarly, a separation algorithm that does not change even as columns are added.'' (BELOV, page 3)

\subsection{Algorithms}

% TODO: remove the citations?
%Two approaches are often used for to solve UKPs, they are dynamic programming (DP)~\cite{eduk},~\cite[p. 214]{garfinkel},~\cite[p. 311]{tchu} and branch and bound (B\&B)~\cite{mtu2}. 
Two approaches are often used for to solve UKPs, they are dynamic programming (DP) and branch and bound (B\&B). 
The DP approach worst-case time complexity is~\(O(nc)\) (pseudo-polynomial).
%The same DP algorithm often has similar run times for instances with similar size (i.e. \(n\) and \(c\)) and ITEM DISTRIBUTION WAS NOT EXPLAINED.

The DP approach can be considered stable, or predictable, compared to other approaches.
Stable in the sense that its run time variation when solving many instances with the same characteristics (i.e.~\(n\),~\(c\) and items distribution) can be lower than other approaches. 
Predictable in the sense that it is easier to predict a reasonable time interval for solving an instance based in the characteristics just mentioned, than it is with other approaches.

%The DP worst-case space complexity is~\(O(n + c)\), which can be considerably greater than other approaches that do not allocate memory linear to~\(c\).
%However, the space needed can be reduced by many optimizations.
%Some of these optimizations are: using a periodicity bound as explained in Section~\ref{sec:periodicity}; using modular arithmetic to reduce~\(c\) to~\(w_{max}\) in at least one array, see~\cite[p.~17]{gg-66}; using binary heaps instead of arrays, as the heap can use less memory than an array of~\(c\) positions if~\(w_{min}\) is sufficiently big.

%The DP approach often gives an optimal solution for each capacity smaller than~\(c\).
%However, some space optimizations can remove such feature.

\begin{comment}
\subsubsection{Weak solution dominance}
\label{sec:ukp5_sol_dom_expl}

% TODO: ADD THIS SOMEPLACE ELSE
In this section we will give a more detailed explanation of the workings of the previously cited weak solution dominance.
We use the notation~\(min_{ix}(s)\) to refer to the lowest index among the items that compose the solution~\(s\).
The notation~\(max_{ix}(s)\) has analogue meaning.

When a solution~\(t\) is pruned because~\(s\) dominates~\(t\) (lines~\ref{if_less_than_opt_begin} to~\ref{if_less_than_opt_end}), some solutions~\(u\), where~\(t \subset u\), are not generated. 
If~\(s\) dominates~\(t\), and~\(t \subset u\), and~\(max_{ix}(u - t) \leq min_{ix}(t)\), then~\(u\) is not generated by UKP5. 
For example, if~\(\{3, 2\}\) is dominated, then~\(\{3, 2, 2\}\) and~\(\{3, 2, 1\}\) will never be generated by UKP5, but~\(\{3,2,3\}\) or~\(\{3,2,5\}\) could yet be generated (note that, in reality, it is the equivalent~\([3,3,2]\) and~\([5,3,2]\) that could yet be generated).
Ideally, any~\(u\) where~\(t \subset u\) should not be generated as it will be dominated by a solution~\(u'\) where~\(s \subset u'\) anyway. 
It is interesting to note that this happens eventually, as any~\(t \cap \{i\}\) where~\(i > min_{ix}(t)\) will be dominated by~\(s \cap \{i\}\) (or by a solution that dominates~\(s \cap \{i\}\)), and at some point no solution that is a superset of~\(t\) will be generated anymore.

\subsubsection{Implementation details}
\label{sec:ukp5_periodicity}

With the purpose of making the initial explanation simpler, we have omitted some steps that are relevant to the algorithm performance, but not essential for assessing its correctness. 
A complete overview of the omitted steps is presented in this section.

\end{comment}

% TODO: it's necessary to add a implementation details section?:w
% TODO: add about the efficiency weight ordereing and breaking ties
% TODO: explain the addition below in the context of the ordered step-off
%There is an \emph{else if} test at line~\ref{if_new_lower_bound_end}. 
%If~\(g[y + w_i] = g[y] + p_i\) and \(i < d[y + w_i]\) then~\(d[y] \gets i\). 
%This may seem unnecessary, as appears to be an optimization of a rare case, where two distinct item multisets have the same weight and profit. 
%Nonetheless, without this test, UKP5 was about 1800 (one thousand and eight hundreds) times slower on some subset-sum instance datasets.

\subsubsection{EDUK}
\label{sec:eduk}

The EDUK (Efficient Dynamic programming for the Unbounded Knapsack problem) is a complex DP algorithm for the UKP, first mentioned in~\cite{ukp_new_results}.
However, only in~\cite{eduk} the algorithm essentials were described for the first time.
The author of this thesis, however, is partial to the algorithm description to be found in~\cite[p.~223]{book_ukp_2004}.

Before EDUK2 was proposed, EDUK was considered by some the state-of-the-art DP algorithm for the UKP.
An example is the comment in~\cite[p.~]{book_ukp_2004}: ``[...] EDUK [...] seems to be the most efficient dynamic programming based method available at the moment.''.

A version of the original code of the EDUK and EDUK2 algorithms is available here\footnote{PYAsUKP official site: \url{http://download.gna.org/pyasukp/pyasukpsrc.html}}.
Unfortunately, this version is not stable and has some bugs.
Consequently, the author of this thesis recommends the use of the version available here\footnote{The repository of this master's thesis: \url{https://github.com/henriquebecker91/masters/blob/f5bbabf47d6852816615315c8839d3f74013af5f/codes/ocaml/pyasukp_mail.tgz}.}.
We were given access to the latter version by Vincent Poirriez in January 11th, 2016.

\begin{comment}
The EDUK algorithm sorts the item list in increasing weight order, differently from the majority of the algorithms for the UKP that use the non-increasing efficiency order.

The sparse representation of the iteration domain is achieved by using lazy lists (a functional programming concept) instead of an array of size~\(c\) (or more) to store the solutions.
Consequently, the memory use is less dependent of~\(c\) and~\(w_{max}\) than other DP algorithms.
In~\cite{algo_tech_cut}, where the sparse representation idea was first presented, the solutions are represented as pairs of weight and profit value, as the solution was a pseudo-item (i.e. a set of items that can be treated as it was a single item).
For example, a solution~\(s\) consisting of the items~\(i\) and~\(j\) is represented by the following pair:~\((w_i + w_j, p_i + p_j)\).
Adding an item to a solution is equivalent to adding the weight and profit values of a pair to another.
For some instances, especially the ones with big~\(w_{min}\) and~\(c\) values, this sparse representation allows for saving time and memory.
In UKP5, for example, the algorithm allocates memory, initializes, and iterates over many capacities~\(y\) that are accessed and then skipped immediately because a solution~\(s\) with~\(w_s = y\) does not exist.
In EDUK, such skipped capacities are never explicitly enumerated to begin with, and no memory is allocated for them, or time used iterating over them. 
A similar effect could be obtained in UKP5 by using a \verb+std::map+ instead of an \verb+std::vector+ for the data structures~\(g\) and~\(d\) .
\end{comment}

\subsubsection{Branch-and-Bound}

% TODO: unify with comment in the prior work
The B\&B approach was established in the seventies as the most efficient approach for solving the UKP, what is greatly a consequence of the datasets, items distributions, and generation parameters used at the time.
The author of this thesis believes that this claim was first made in~\cite{mtu2}, and then other papers as~\cite{babayev} began repeating it.
The fact that only the code for MTU1 and MTU2 was readily available also did not help the situation, as some began to claim that MTU2 was the \emph{de facto} standard algorithm for the UKP, see~\cite{ukp_new_results}.

The time taken by a B\&B algorithm over an instance of the UKP can be hard to predict, and is not very dependent on the magnitude of~\(n\) and~\(c\), but more dependent on the item distribution.
In the worst case, a B\&B algorithm cannot eliminate a significant portion of the search space by the use of bounds and then, consequently, it needs to examine all search space.
In the case of the UKP, the search space is clearly combinatorial (all possible items combinations that fit the knapsack), so the worst-case of an B\&B approach for the UKP can be exponential.

The B\&B algorithms for the UKP often are not affected by the magnitude of~\(c\), however they can be affect by how close~\(c\) is from a capacity that can be entirely filled by copies of the best item.
The B\&B algorithms for the UKP will often solve the problem instantly if~\(c~mod~w_b\) is small, because the greedy heuristic lower bound will probably be optimal, and will exclude the remaining search space easily.

The fact that this approach is not significantly affected by huge values of~\(n\) and~\(c\), and more by the distribution used, makes it clear why it was considered the best approach in the seventies.
The datasets used back then had large~\(n\) and~\(c\) values, and items distributions that made easy to exclude large portions of the search space with the greedy lower bound solution (the uncorrelated distribution is the perfect example).

\subsubsection{MTU1}
\label{sec:mtu1}

The MTU1 algorithm is a B\&B algorithm for the UKP that avoids the explicit unfolding of the typical B\&B tree~\cite{mtu1}.
The implicit tree used by MTU1 is described in what follows, as this makes the algorithm easier to visualize and understand.
The MTU1 sorts the items in non-increasing efficiency order before beginning.
Such ordering is needed to assure the correctness of the bounds and, consequently, of the algorithm itself.
%In the algorithms description, it is to be assumed that the items are ordered in the mentioned order
%MTU1 begins by creating a lower bound solution using a greedy heuristic procedure.
As the nodes/solutions are visited in a systematic order, for any given node/solution, it is possible to know what part of the `search space'/tree was already visited or skipped, and what part has not yet been explored.
Consequently, the tree does not need to be enumerated explicitly, the current node/solution is sufficient to know which solutions should be tried next.

\subsubsection{MTU2}
\label{sec:mtu2}

The MTU2 algorithm was first proposed in~\cite{mtu2}.
The objective of MTU2 is to improve MTU1 run time when it is used in very large instances (e.g. up to 250,000 items).
MTU2 calls MTU1 internally to solve the UKP: it can be seen as a wrapper around MTU1 that avoids unnecessary computational effort.
The two main factors that motivated the creation of MTU2 were: 1) for the majority of the instances used in the period\footnote{For an example, one of the datasets of the paper that introduced MTU2 was analyzed in Section~\ref{sec:martello_uncorrelated}.}, an optimal solution is composed of the most efficient items; 2) for some of those instances, sorting the entire items list was more expensive than solving the instance with MTU1.

% TODO: ADD ONLY THE NEEDED TO GIVE THE READER THE NOTION THAT MTU2 IS AN WRAPPER OF MTU1

\subsubsection{Hybrid (DP and B\&B)}

As expected, some algorithms try to combine the best of two most popular approaches (DP and B\&B) for better results.

%\subsection{GREENDP}

The algorithm presented in~\cite{green_improv} is an improvement on the ordered step-off from~\cite{gg-66}.
It is very similar to UKP5.
The author does not know if it could be defined as a hybrid, but a good definition for it would be a `DP algorithm with bounds'.
The algorithm was not named in the paper and will be called GREENDP for the rest of the thesis.
The implementation of the GREENDP made by the author of this thesis, and used in the experiments (Section \ref{sec:exp_and_res}), will be called MGREENDP (Modernized GREENDP, in the sense that the algorithm now uses loops instead of the \verb+goto+ directive). 

The GREENDP algorithm consists in solving the UKP by using the ordered step-off algorithm, but without using the best item in the DP, and with interruptions at each~\(w_b\) capacity positions, for checking if the DP can be stopped and the remaining capacity filled with copies of the best item.
\begin{comment}
In those interruptions, two bounds are computed.
A lower bound for solutions using the best item is computed by combining the current best solution of the DP with as many copies of the best item as possible.
An upper bound for solutions not using the best item is computed by combining the current best solution of the DP with a pseudo-item that would fill the entire capacity gap and has the same efficiency as the second best item (it could also be seen as solving a continuous relaxation of the UKP without the best item, and only for the remaining capacity).
If the algorithm discovers that the lower bound with the best item is better than the upper bound without the best item, then the lower bound solution is optimal and the DP can be stopped.

This approach using bounds is fundamentally different from the periodicity check used by UKP5 (or the periodicity check used by the `terminating step-off').
For example, the use of bounds save computational time of GREENDP when it is used to solve BREQ instances, the periodicity check do not save computational time of UKP5 when it is used to solve the same instances (see experiments of the Section~\ref{sec:breq_exp}).
However this seems to have an impact on other families of instances (see experiments of the Section~\ref{sec:pya_exp}).
\end{comment}

The bound computed by GREENDP does not work if two or more items share the greatest efficiency.
Without the bound computation, the GREENDP is the same as the ordered step-off.

\subsubsection{EDUK2}

The EDUK2 algorithm was proposed in~\cite{pya}, and it is an hybridization of EDUK (a DP algorithm) and MTU2 (a B\&B algorithm).
\begin{comment}
The author of this thesis gives here a quick overview of the hybridization, but more details can be found in the paper above mentioned.
Just as with EDUK, the author does not claim to fully comprehend the EDUK2 internals, and only summarizes what is said in the original paper.
The author recommends reading Sections~\ref{sec:eduk} (EDUK) and~\ref{sec:mtu2} (MTU2) before the explanation below, as it is strongly based in both algorithms.
The comments made about EDUK code in its own section also apply to EDUK2.

The description of the changes in EDUK caused by the hybridization follows.
The~\(k = min(n, max(100, \frac{n}{100}))\) most efficient items are gathered in a tentative core problem.
A B\&B algorithm ``similar to the one in MTU1''\footnote{Quoted from \cite{pya}.} tries to solve the tentative core problem.
This B\&B algorithm has the possibility of choosing among three bound formulas, and stops after exploring~\(B = 10,000\) nodes (of the implicit enumeration tree).
If the B\&B algorithm returns a solution with value equal to an upper bound for the whole instance, then the DP algorithm never executes.
Otherwise, the solution given by the B\&B algorithm is used as a global lower bound in the hybridized EDUK algorithm.
The hybridized EDUK algorithm works like EDUK would do, with the addition of an extra phase between the slices.
The extra phase uses the lower bound to eliminate items \emph{and solutions for lesser capacities} from the algorithm.
This phase is very similar to a phase of MTU2: an upper bound is computed for solutions using one copy of the respective item (a solution~\(s\) can be treated as a pseudo-item (\(w_s, p_s\))).
If this upper bound is equal to or smaller than the global lower bound, then the item (or solution) is abandoned by the algorithm.
A new lower bound is computed for each solution that was not removed by the process described above.
The lower bound consists in filling the remaining capacity with a greedy algorithm.
If this new lower bound is better than the global lower bound, it replaces it. 

\end{comment}

\subsubsection{Algorithms not included}

In this section, we explain why we did not include some algorithms in our comparison.

% TODO: DECIDE IF THE ORDERED AND THE TERMINATING WILL BE SHOWN OR ONLY THE TERMINATING
% TODO: JUSTIFY WHY ONLY THE ORDERED/TERMINATING WERE SHOWN (NOT THE PERIODIC)
% ex: the terminating step-off add O(n + c) operations in exchange for the possibility of removing O(c) operations
An examination of the naïve DP algorithm for the UKP~\cite[p.~311]{tchu} and its improved version presented in~\cite[p.~221]{garfinkel} shows that both algorithms are dominated by the ordered step-off algorithm presented in \cite[p.~15]{gg-66}.
A B\&B algorithm was proposed by the authors of the ordered step-off before, in~\cite{gg-63}, we believe that the B\&B algorithm is superseeded by the ordered step-off algorithm.
Also, the UKP5 algorithm proposed in \cite{sea2016} was found to be equivalent to the ordered step-off.
Since the experiments presented in this paper already include the ordered step-off, the other four algorithms mentioned are not included.

Two algorithms for the UKP are proposed in~\cite{on_equivalent_greenberg}.
Our implementation of the first algorithm\footnote{Readers who are interested in those algorithms can access our implementations made available at \url{https://github.com/henriquebecker91/masters/blob/e2ff269998576cb69b8d6fb1de59fa5d3ce02852/codes/cpp/lib/greendp.hpp}.} exceeded our time limit every time we tried to execute it over instances of recent datasets.
The second algorithm does not work for all UKP instances.
Both algorithms were not included in our experiments.

The papers~\cite{cabot} and~\cite{turnpike} proposed algorithms for the UKP, but we did not have access to them, as both are behind a paywall.
Moreover, both algorithms have experiments in the literature suggesting that they are dominated by algorithms included in our experiments\footnote{In \cite{mtu1}~it is shown that MTU1 dominates the algorithm from~\cite{cabot}, and in~\cite{green_improv} it is implied that GREENDP is an improved version of the algorithm from~\cite{turnpike}.}.

We did not obtain access to the code of the algorithm proposed in~\cite{babayev} and we had difficulties trying to implement it.

An instance of the UKP can be converted in an instance of the BKP by adding a constraint \(x_i \leq \floor{\frac{c}{w_i}}\) for each item \(i\).
Such conversion would allow us to add BKP algorithms to the comparison.
Nevertheless, we decided against adding them, since UKP algorithms are BKP algorithms fine-tuned for UKP instances and there exists evidence in the literature that BKP algorithms perform worse than UKP algorithms in UKP instances~\cite{mtu1}.

\subsection{Instance datasets}

\subsubsection{Unused datasets}

The \emph{uncorrelated} and \emph{weakly correlated} item distributions were commonly used in the literature\cite{mtu1}\cite{mtu2}\cite{babayev}\cite{eduk}, but the authors decided against including them in the experiments.
The literature has already questioned the suitability of \emph{uncorrelated} item distributions datasets for the analysis of the UKP\cite{zhu_dominated}\cite{ukp_new_results}.
Uncorrelated instances often exhibit a great amount of simple and multiple dominated items, and polynomial algorithms can reduce the number of items in such instances by magnitudes.
In the authors experience, uncorrelated instances often take more time to load from disk than to solve.
The solving times of uncorrelated instances are more dependent on the implementation of polynomial-time preprocessing than dependent on the quality of the solving algorithm.
Consequently, the authors do not believe uncorrelated instances provide a good benchmark for UKP solving algorithms.

The \emph{weakly correlated} item distribution can be seen as between the uncorrelated and strongly correlated distributions.
Whereas strongly correlated instances have no simple dominance, weakly correlated can exhibit simple dominance and multiple dominance is more common.
Without the extra dominated items, which are promptly removed by many algorithms, the weakly correlated instances are very similar to strongly correlated instances of smaller size.
Hence, the authors found redundant to present a weakly correlated dataset together with the strongly correlated datasets already present in the experiments.


\subsubsection{PYAsUKP dataset}
\label{sec:pya_inst}

In~\cite[p. 9]{sea2016}, the authors described a dataset comprising 4540 instances from five smaller datasets.
This dataset was heavily based on five datasets presented in~\cite{pya}.
In~\cite{pya} these five datasets are used to compare the newly proposed state-of-the-art algorithm EDUK2 against other algorithms.
The name of the implementation of EDUK2 and the instance generator is PYAsUKP (PYAsUKP: Yet Another solver for the UKP), which is the reason we call this dataset the \emph{PYAsUKP dataset}.
The PYAsUKP dataset comprises: 400 subset-sum instances (\(10^3 \leq n \leq 10^4\); 240 strongly correlated instances (\(5\times10^3 \leq n \leq n = 10^4\)); 800 instances with postponed periodicity (\(2\times10^4 \leq n \leq 5\times10^4\)); 2000 instances without collective dominance (\(5\times10^3 \leq n \leq 5\times10^4\)); 1100 SAW instances (\(10^4 \leq n \leq 10^5\)).

\subsubsection{CSP pricing subproblem dataset}
\label{sec:csp_ukp_inst}

An often mentioned application for solving the UKP is solving the pricing subproblem generated by solving the continuous relaxation of the set covering formulation for the classic Bin Packing Problem (BPP) and Cutting Stock Problem (CSP) using the column generation approach\cite[p. 455--459]{book_ukp_2004}\cite{gg-61}.
In order to avoid that the experiments included only datasets of instances artificially designed to be `hard to solve' (by some or other method), the authors included instances generated by such application.

The authors used 

The previously mentioned Set Covering Formulation (SCF) for BPP and CSP is a tight formulation proposed in~\cite{gg-61}.
The SCF eliminated the problems of the classic formulation that was loose and had too many symmetric solutions.
However, as a consequence, the SCF needs to compute all cutting patterns, i.e. all combinations of sheet sizes that can be cut from a single master roll.
As the cutting patterns are combinations, the amount of cutting patterns can be superexponential in the number of sheet sizes. The exact number of cutting patterns is affected by the sheet sizes. The best case happens when \(\forall i.~w_i > \frac{c}{2}\), in this case the number of cutting patterns is \(n\). The worst case happens when all \(n\) sheet sizes have almost the same size (let us call this size \(w^*\)), and \(n > k = \frac{c}{w^*}\), in this case the number of cutting patterns is given by the binomial coefficient \(\binom{n}{k}\) (that computes \(n!\), which is superexponential).

%It is important to remember that, in this work, our objective is not to solve CSP but its continuous relaxation.

%The column generation approach consists in avoiding the enumeration of all~\(m\) cutting patterns.
%The SCF relaxation is initialized with a small set of cutting patterns that can be computed in polynomial time and in which each sheet size appears at least in one of the patterns.
%This reduced problem is called the \emph{master problem}.
%It is solved by using the simplex method, as it is a linear programming problem.
%A by-product of this solving process are the dual variables of the master problem model.
%Those variables are used as input for a \emph{pricing subproblem}.
%The solution of this pricing subproblem is the cutting pattern that, if added to the master problem, will give the greatest improvement to master problem optimal solution.

%The solving process alternates between solving the master problem and the pricing subproblem, until all cutting patterns that could improve the solution of the master problem are generated and added to the master problem.

%The method described above can generate thousands of UKP instances for one single instance of the CSP.
%For the same instance of the CSP, the number of UKP instances generated, and their exact profit values, can vary based on the choice of optimal solution made by the UKP solver (for the same pricing subproblem many cutting patterns can be optimal, but only one among them is added to the master problem).
%Consequently, such dataset is hard to describe (has a large and variable number of instances with variable profit values).
%The best way found by the author to ensure that the results are reproducible is making available the exact codes used in the experiment, together with the list of CSP instances from the literature used in the experiment.
%The codes are deterministic, and consequently will produce the same results if executed many times over the same CSP instance.

A recent survey on BPP and CSP gathered the instances from the literature, and also proposed new ones~\cite{survey2014}.
The total number of instances in all datasets presented in the survey is 5692.
The author of this thesis chose ten percent of those instances for the experiments presented at Section \ref{sec:csp_experiments}.
This fraction of the instances was randomly selected among instances within the same dataset or, in the larger datasets, the same generation parameters.
The address of a repository containing the data and code used in the above mentioned experiments, and the instructions to compile the code, can be found in \ref{sec:csp_appendix}.

The authors did not worry about implementing many optimizations of the master problem solver described in~\cite{gg-61,gg-63,gg-66}.
The authors believe that these optimizations do not affect considerably the structure of the pricing subproblem.

\subsubsection{Bottom Right Ellipse Quadrant instances}
\label{sec:breq_inst}

The Bottom Right Ellipse Quadrant (BREQ) is an items distribution proposed by the authors. %and first described in REF\_MASTER\_THESIS.
The items of an instance follow the BREQ distribution iff the profits and weights respect~\(p_i = p_{max} - \floor[\big]{\sqrt{p_{max}^2 - w_i^2 \times (\frac{p_{max}}{w_{max}})^2}}\), where \(w_{max}\) (\(p_{max}\)) is an upper bound on the items weight (profit).
The distribution name comes from the fact that its formula describes the bottom right quarter of an ellipse.
%This instance distribution was created to illustrate that different item distributions favors different solution approaches and, therefore, the choice of instances (or specifically, their item distribution) defines what is considered the \emph{best algorithm}.

The purpose of this items distribution is to illustrate the authors' point that artificial distributions can be construed to favor one solving approach over another.
In the case of the BREQ distribution, it favors B\&B over DP.
Distributions with the opposite property (favor DP over B\&B) are common in the recent literature.

The optimal solution of BREQ instances is often in the first fraction of the search space examinated by B\&B algorithms. 
Moreover, the lower bounds from good solutions allow B\&B methods to skip a large fraction of the search space and promptly prove optimality.
In BREQ instances, the presence of simple, multiple and collective dominance is minimal
\footnote{
If the BREQ formula did not include the rounding, the items profit would be a strictly monotonically increasing function of the items weight.
Any item distribution with this property cannot present simple, multiple or collective dominance. 
}, and the threshold dominance is common\footnote{In BREQ instances, an optimal solution will never include the item~\(i\) two or more times if there is an item~\(j\) such as that~\(\sqrt{2} \times w_i \leq w_j \leq 2 \times w_i\).}.
Such characteristics lead to optimal solutions comprised of the largest weight items, that do not reuse optimal solutions for lower capacities.
This means that solving the UKP for lower capacities as DP algorithms do is mostly a wasted effort.

%If those three dominance relations are absent, for any solution~\(s\) composed of two or more items, and for any single item~\(i\), if~\(w_s \leq w_i\) then~\(p_s < p_i\).

% Proof that this interval is tight: http://www.wolframalpha.com/input/?i=2*(100+-+sqrt(100%5E2+-+w%5E2+*+16%5E2))+%3C%3D+100+-+sqrt(100%5E2+-+((sqrt(2)*w)%5E2+*+16%5E2))

The BREQ dataset used in the experiments comprises 10 instances with distinct random seeds for each one of 10 distinct \(n\) values (\(n = 2^{n'}\), where \(n' \in \{11, 12, \dots, 20\}\)), totalling 100 instances.
The values of the remaining parameters can be computed as follows: \(p_{min} = w_{min} = 1\), \(c = 128 \times n\), \(w_{max} = c\) and \(p_{max} = 16 \times w_{max}\).
The items generation procedure follows:
generate~\(n\) unique random integer weights uniformly distributed in~\([w_{min},w_{max}]\);
for each item weight, the corresponding profit is calculated by the formula presented in the first paragraph of this section.

\subsubsection{A Realistic Random Dataset}
% PARAGRAPH ABOUT REALISTIC RANDOM INSTANCES
A dataset of \emph{realistic random} instances was also used in our experiments, our generation procedure is based on~\cite{eduk}.
Our generation procedure follows: generate a list of~\(n\) unique random integers uniformly distributed in~\([min,max]\) and sort them by increasing value; generate a second list repeating the same procedure; combine both lists into a single item list where the weight (profit) of each item~\(i \in [1,n]\) is the value at position \(i\) of the first (second) list; randomly shuffle the item list; generate a random capacity~\(c \in [c_{min},c_{max}]\) (uniform distribution).
Simple dominance can not occur in such instances, other dominances may be present.
Our dataset comprises ten instances with distinct random seeds for each one of eight \(n\) values (\(2^{n'}\), where \(n' \in \{10, 11, \dots, 17\}\)), totalling 80 instances.
The values of the remaining parameters come from \(n\): \(max = n \times 2^{10}\), \(min = \frac{max}{2^4}\), \(c_{min} = 2\times max\) and \(c_{max} = c_{min} + min\).


