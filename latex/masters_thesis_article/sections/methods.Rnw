\section{Methods}

an experimental study of the UKP

mainly consists of the execution os algorithms over datasets.
in this section, the algorithms and datasets are briefly presented

all algorithms are taken from the literature; one receive small improvements

all datasets taken from the literature, except for one that is proposed

\subsection{Algorithms}

The algorithms used in the experiments can be divided in three main approachs: dynamic programming (DP); branch-and-bound (B\&B); and hybrid (both DP and B\&B).
The DP approach worst-case time complexity is~\(O(nc)\) (pseudo-polynomial), and the its worst-case space complexity is~\(O(n + c)\).
The B\&B approach has an exponential worst-case assimptotic complexity (all item combinations that fit the knapsack), the worst-case space complexity of the B\&B algorithms used in the experiments is an optimal \(O(n)\).
Hybrid approachs often display the same time and space worst-case complexities than the DP approach.

The EDUK (Efficient Dynamic programming for the Unbounded Knapsack problem) was the first DP algorithm to explicitly check for threshold dominance (a concept proposed together with the algorithm) and collective dominance (that was independently discovered by Pisinger\cite{pisinger1994dominance}), it also features a sparse representation of the iteration domain\cite{ukp_new_results}\cite{eduk}\cite[p.~223]{book_ukp_2004}.
EDUK seems to be based on ideas first discussed in~\cite{algo_tech_cut}.
Before EDUK2 was proposed, it was said that ``[...] EDUK [...] seems to be the most efficient dynamic programming based method available at the moment.''\cite[p.~223]{book_ukp_2004}.

EDUK2 is an hybrid DP/B\&B algorithm which improves EDUK with a B\&B preprocessing phase~\cite{pya}.
If B\&B can solve instance using less than a parametrized number of nodes, then EDUK is never executed; otherwise, the bounds computed in the B\&B phase are used to reduce the number of items before EDUK execution and in intervals during its execution.

The implementation of EDUK and EDUK2 used in the experiments was PYAsUKP (PYAsUKP: Yet Another solver for the Unbounded Knapsack Problem), which was written in OCaml by the creators of EDUK and EDUK2. 
The authors were given access to this code by Vincent Poirriez in January 11th, 2016 by email\footnote{The code is available at Henrique Becker master's thesis code repository (\url{https://github.com/henriquebecker91/masters/blob/f5bbabf47d6852816615315c8839d3f74013af5f/codes/ocaml/pyasukp_mail.tgz}).}. The version of the code available at that time in the PYAsUKP official site had bugs\footnote{The EDUK and EDUK2 source was available to download in the following page of PYAsUKP official site: \url{http://download.gna.org/pyasukp/pyasukpsrc.html}.}.

A small improvement over the algorithm of~\cite{gg-66} was proposed in~\cite{green_improv}.
The author implemented the improved algorithm and its results can be seen in Section~\ref{sec:pya_exp}.

MTU1 is a depth-first B\&B algorithm for the UKP with worst-case space complexity \(O(n)\)\cite{mtu1}.
The MTU2 algorithm calls MTU1 over increasing fractions of the items list in an effort to avoid considering all items\cite{mtu2}.
The original implementations of MTU1 and MTU2 available in CONTINUE HERE

The algorithm presented in~\cite{green_improv} is an improvement on the ordered step-off from~\cite{gg-66}.
The author does not know if it could be defined as a hybrid, but a good definition for it would be a `DP algorithm with bounds'.
The algorithm was not named in the paper, it will be referred as GREENDP in this work.
The implementation of GREENDP used in the experiments will be referred as MGREENDP.
The GREENDP algorithm consists in solving the UKP by using the ordered step-off algorithm, but without using the best item in the DP, and with interruptions at each~\(w_b\) capacity positions, for checking if the DP can be stopped and the remaining capacity filled with copies of the best item.
The bound computed by GREENDP does not work if two or more items share the greatest efficiency.
Without the bound computation, the GREENDP is the same as the ordered step-off.

\subsubsection{Algorithms not included}

% TODO: DECIDE IF THE ORDERED AND THE TERMINATING WILL BE SHOWN OR ONLY THE TERMINATING
% TODO: JUSTIFY WHY ONLY THE ORDERED/TERMINATING WERE SHOWN (NOT THE PERIODIC)
% ex: the terminating step-off add O(n + c) operations in exchange for the possibility of removing O(c) operations
Many algorithms mentioned in the literature were not used in the experiments.
Such algorithms are mentioned below, and reasons for not including them are given.

The authors believe that both the na√Øve DP algorithm for the UKP\cite[p.~311]{tchu} and an improved version of it presented in~\cite[p.~221]{garfinkel} are dominated by the ordered step-off algorithm presented in \cite[p.~15]{gg-66}.
In \cite{mtu1}, the B\&B algorithm proposed in~\cite{gg-63} is said to be two times slower than the algorithm proposed in~\cite{cabot}, which is found to be dominated by MTU1; also, the algorithm in~\cite{gg-63} seems to have been abandoned by its authors in favor of the ordered step-off.
The UKP5 algorithm proposed in~\cite{sea2016} was found to be equivalent to the ordered step-off\footnote{The authors of this article reinvented one algorithm from~\cite{gg-66} and published a paper about it while believing it was novel~\cite{sea2016}. The authors would like to apologize to the academic and scientific community for such disregard.}

The authors' implementation of the first algorithm proposed in~\cite{on_equivalent_greenberg} exceeded every time limit, the second algorithm does not work for all UKP instances\footnote{The authors' implementations of both algorithms were made available at \url{https://github.com/henriquebecker91/masters/blob/e2ff269998576cb69b8d6fb1de59fa5d3ce02852/codes/cpp/lib/greendp.hpp}.}.

Both~\cite{cabot} and~\cite{turnpike} are behind a paywall, and algorithms described in other papers seem to superseed them\footnote{In~\cite{mtu1}, it is shown that MTU1 dominates the algorithm from~\cite{cabot}, and in~\cite{green_improv} it is implied that GREENDP is an improved version of the algorithm from~\cite{turnpike}.}.
The authors could not obtain the code of the algorithm proposed in~\cite{babayev} and had difficulties trying to implement it.
UKP-specific algorithms perform better than applying BKP or 0-1 KP algorithms over converted UKP instances\cite{mtu1}, so BKP and 0-1 KP algorithms were not included.
% TODO: check if BKP and 0-1 KP were written expanded before phrase above

\subsection{Instance datasets}

\subsubsection{Unused datasets}

The \emph{uncorrelated} and \emph{weakly correlated} item distributions were commonly used in the literature\cite{mtu1}\cite{mtu2}\cite{babayev}\cite{eduk}, but the authors decided against including them in the experiments.
The literature has already questioned the suitability of \emph{uncorrelated} item distributions datasets for the analysis of the UKP\cite{zhu_dominated}\cite{ukp_new_results}.
Uncorrelated instances often exhibit a great amount of simple and multiple dominated items, and polynomial algorithms can reduce the number of items in such instances by magnitudes.
In the authors experience, uncorrelated instances often take more time to load from disk than to solve.
The solving times of uncorrelated instances are more dependent on the implementation of polynomial-time preprocessing than dependent on the quality of the solving algorithm.
Consequently, the authors do not believe uncorrelated instances provide a good benchmark for UKP solving algorithms.

The \emph{weakly correlated} item distribution can be seen as between the uncorrelated and strongly correlated distributions.
Whereas strongly correlated instances have no simple dominance, weakly correlated can exhibit simple dominance and multiple dominance is more common.
Without the extra dominated items, which are promptly removed by many algorithms, the weakly correlated instances are very similar to strongly correlated instances of smaller size.
Hence, the authors found redundant to present a weakly correlated dataset together with the strongly correlated datasets already present in the experiments.


\subsubsection{PYAsUKP dataset}
\label{sec:pya_inst}

In~\cite[p. 9]{sea2016}, the authors described a dataset comprising 4540 instances from five smaller datasets.
This dataset was heavily based on five datasets presented in~\cite{pya}.
In~\cite{pya} these five datasets are used to compare the newly proposed state-of-the-art algorithm EDUK2 against other algorithms.
The name of the implementation of EDUK2 and the instance generator is PYAsUKP (PYAsUKP: Yet Another solver for the UKP), which is the reason we call this dataset the \emph{PYAsUKP dataset}.
The PYAsUKP dataset comprises: 400 subset-sum instances (\(10^3 \leq n \leq 10^4\); 240 strongly correlated instances (\(5\times10^3 \leq n \leq n = 10^4\)); 800 instances with postponed periodicity (\(2\times10^4 \leq n \leq 5\times10^4\)); 2000 instances without collective dominance (\(5\times10^3 \leq n \leq 5\times10^4\)); 1100 SAW instances (\(10^4 \leq n \leq 10^5\)).

\subsubsection{CSP pricing subproblem dataset}
\label{sec:csp_ukp_inst}

An often mentioned application for solving the UKP is solving the pricing subproblem generated by solving the continuous relaxation of the set covering formulation for the classic Bin Packing Problem (BPP) and Cutting Stock Problem (CSP) using the column generation approach\cite[p. 455--459]{book_ukp_2004}\cite{gg-61}.
In order to avoid that the experiments included only datasets of instances artificially designed to be `hard to solve' (by some or other method), the authors included instances generated by such application.

The authors used 

The previously mentioned Set Covering Formulation (SCF) for BPP and CSP is a tight formulation proposed in~\cite{gg-61}.
The SCF eliminated the problems of the classic formulation that was loose and had too many symmetric solutions.
However, as a consequence, the SCF needs to compute all cutting patterns, i.e. all combinations of sheet sizes that can be cut from a single master roll.
As the cutting patterns are combinations, the amount of cutting patterns can be superexponential in the number of sheet sizes. The exact number of cutting patterns is affected by the sheet sizes. The best case happens when \(\forall i.~w_i > \frac{c}{2}\), in this case the number of cutting patterns is \(n\). The worst case happens when all \(n\) sheet sizes have almost the same size (let us call this size \(w^*\)), and \(n > k = \frac{c}{w^*}\), in this case the number of cutting patterns is given by the binomial coefficient \(\binom{n}{k}\) (that computes \(n!\), which is superexponential).

%It is important to remember that, in this work, our objective is not to solve CSP but its continuous relaxation.

%The column generation approach consists in avoiding the enumeration of all~\(m\) cutting patterns.
%The SCF relaxation is initialized with a small set of cutting patterns that can be computed in polynomial time and in which each sheet size appears at least in one of the patterns.
%This reduced problem is called the \emph{master problem}.
%It is solved by using the simplex method, as it is a linear programming problem.
%A by-product of this solving process are the dual variables of the master problem model.
%Those variables are used as input for a \emph{pricing subproblem}.
%The solution of this pricing subproblem is the cutting pattern that, if added to the master problem, will give the greatest improvement to master problem optimal solution.

%The solving process alternates between solving the master problem and the pricing subproblem, until all cutting patterns that could improve the solution of the master problem are generated and added to the master problem.

%The method described above can generate thousands of UKP instances for one single instance of the CSP.
%For the same instance of the CSP, the number of UKP instances generated, and their exact profit values, can vary based on the choice of optimal solution made by the UKP solver (for the same pricing subproblem many cutting patterns can be optimal, but only one among them is added to the master problem).
%Consequently, such dataset is hard to describe (has a large and variable number of instances with variable profit values).
%The best way found by the author to ensure that the results are reproducible is making available the exact codes used in the experiment, together with the list of CSP instances from the literature used in the experiment.
%The codes are deterministic, and consequently will produce the same results if executed many times over the same CSP instance.

A recent survey on BPP and CSP gathered the instances from the literature, and also proposed new ones~\cite{survey2014}.
The total number of instances in all datasets presented in the survey is 5692.
The author of this thesis chose ten percent of those instances for the experiments presented at Section \ref{sec:csp_experiments}.
This fraction of the instances was randomly selected among instances within the same dataset or, in the larger datasets, the same generation parameters.
The address of a repository containing the data and code used in the above mentioned experiments, and the instructions to compile the code, can be found in \ref{sec:csp_appendix}.

The authors did not worry about implementing many optimizations of the master problem solver described in~\cite{gg-61,gg-63,gg-66}.
The authors believe that these optimizations do not affect considerably the structure of the pricing subproblem.

\subsubsection{Bottom Right Ellipse Quadrant instances}
\label{sec:breq_inst}

The Bottom Right Ellipse Quadrant (BREQ) is an items distribution proposed by the authors. %and first described in REF\_MASTER\_THESIS.
The items of an instance follow the BREQ distribution iff the profits and weights respect~\(p_i = p_{max} - \floor[\big]{\sqrt{p_{max}^2 - w_i^2 \times (\frac{p_{max}}{w_{max}})^2}}\), where \(w_{max}\) (\(p_{max}\)) is an upper bound on the items weight (profit).
The distribution name comes from the fact that its formula describes the bottom right quarter of an ellipse.
%This instance distribution was created to illustrate that different item distributions favors different solution approaches and, therefore, the choice of instances (or specifically, their item distribution) defines what is considered the \emph{best algorithm}.

The purpose of this items distribution is to illustrate the authors' point that artificial distributions can be construed to favor one solving approach over another.
In the case of the BREQ distribution, it favors B\&B over DP.
Distributions with the opposite property (favor DP over B\&B) are common in the recent literature.

The optimal solution of BREQ instances is often in the first fraction of the search space examinated by B\&B algorithms. 
Moreover, the lower bounds from good solutions allow B\&B methods to skip a large fraction of the search space and promptly prove optimality.
In BREQ instances, the presence of simple, multiple and collective dominance is minimal
\footnote{
If the BREQ formula did not include the rounding, the items profit would be a strictly monotonically increasing function of the items weight.
Any item distribution with this property cannot present simple, multiple or collective dominance. 
}, and the threshold dominance is common\footnote{In BREQ instances, an optimal solution will never include the item~\(i\) two or more times if there is an item~\(j\) such as that~\(\sqrt{2} \times w_i \leq w_j \leq 2 \times w_i\).}.
Such characteristics lead to optimal solutions comprised of the largest weight items, that do not reuse optimal solutions for lower capacities.
This means that solving the UKP for lower capacities as DP algorithms do is mostly a wasted effort.

%If those three dominance relations are absent, for any solution~\(s\) composed of two or more items, and for any single item~\(i\), if~\(w_s \leq w_i\) then~\(p_s < p_i\).

% Proof that this interval is tight: http://www.wolframalpha.com/input/?i=2*(100+-+sqrt(100%5E2+-+w%5E2+*+16%5E2))+%3C%3D+100+-+sqrt(100%5E2+-+((sqrt(2)*w)%5E2+*+16%5E2))

The BREQ dataset used in the experiments comprises 10 instances with distinct random seeds for each one of 10 distinct \(n\) values (\(n = 2^{n'}\), where \(n' \in \{11, 12, \dots, 20\}\)), totalling 100 instances.
The values of the remaining parameters can be computed as follows: \(p_{min} = w_{min} = 1\), \(c = 128 \times n\), \(w_{max} = c\) and \(p_{max} = 16 \times w_{max}\).
The items generation procedure follows:
generate~\(n\) unique random integer weights uniformly distributed in~\([w_{min},w_{max}]\);
for each item weight, the corresponding profit is calculated by the formula presented in the first paragraph of this section.

\subsubsection{A Realistic Random Dataset}
% PARAGRAPH ABOUT REALISTIC RANDOM INSTANCES
A dataset of \emph{realistic random} instances was also used in our experiments, our generation procedure is based on~\cite{eduk}.
Our generation procedure follows: generate a list of~\(n\) unique random integers uniformly distributed in~\([min,max]\) and sort them by increasing value; generate a second list repeating the same procedure; combine both lists into a single item list where the weight (profit) of each item~\(i \in [1,n]\) is the value at position \(i\) of the first (second) list; randomly shuffle the item list; generate a random capacity~\(c \in [c_{min},c_{max}]\) (uniform distribution).
Simple dominance can not occur in such instances, other dominances may be present.
Our dataset comprises ten instances with distinct random seeds for each one of eight \(n\) values (\(2^{n'}\), where \(n' \in \{10, 11, \dots, 17\}\)), totalling 80 instances.
The values of the remaining parameters come from \(n\): \(max = n \times 2^{10}\), \(min = \frac{max}{2^4}\), \(c_{min} = 2\times max\) and \(c_{max} = c_{min} + min\).


