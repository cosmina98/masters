\section{Introduction}
\label{sec:introduction}

The objective of this work is to provide an extensive comparison of the exact algorithms for solving the Unbounded Knapsack Problem (UKP).
The UKP is similar to the Bounded Knapsack Problem (BKP) and the 0-1 Knapsack Problem (0-1 KP).
The only difference between the UKP and the BKP (or the 0-1 KP) is that the UKP has an unlimited quantity of each item available.
The UKP is a weakly NP-Hard problem, as are the BKP and the 0-1 KP.
%~\cite{where_hard}.
%The UKP can also be seen as a special case of the BKP in which, for each item type, there are more copies available than is possible to fit in the knapsack capacity.

The UKP is the pricing subproblem generated by solving the continuous relaxation of the set covering formulation for the unidimensional Bin Packing Problem (BPP) and Cutting Stock Problem (CSP) using the column generation approach~\cite{gg-61,gg-63}.
The BPP and the CSP are classical optimization problems in the area of operations research~\cite{survey2014}.
The best lower bounds known for the BPP and CSP are their continuous relaxations of the set covering formulation.
This is the tightest formulation for these problems but it has an exponential number of columns, and thus is solved by using the column generation approach~\cite{gg-61}.

%\subsection{Prior work}

Next the authors present some selected papers and a summary of their relevance for the UKP.

\begin{table}[!htb]
\renewcommand{\arraystretch}{1.5}
\label{tab:prior_work}
%\begin{adjustbox}{max width=\textwidth, center}
\begin{tabu}{clp{10cm}}
Year & Ref. & Notes\\
\hline
1961 & \cite{gg-61} & The column generation approach for BPP and CSP continuous relaxation is proposed; the UKP is the pricing problem of such approach.\\
1966 & \cite{gg-66} & The ordered and the terminating step-off algorithms (dynamic programming algorithms to solve the UKP) are proposed. \\
1977 & \cite{mtu1} & The MTU1 (branch-and-bound algorithm) is proposed, and then compared with the previous algorithms over artificial instances up to a hundred items, obtaining slightly better results \\
1990 & \cite{mtu2} & Datasets of instances with up to 250,000 items (but rich in simple and multiple dominances) are proposed. MTU2 is proposed as an improvement of MTU1 for such dataset.\\
%1997 & \cite{zhu_dominated} & Uncorrelated datasets are shown to  \\
%1997 & \cite{babayev} & A new solving approach, more similar to dynamic programming than B\&B \\
1998 & \cite{ukp_new_results,eduk} & The EDUK (a dynamic programming algorithm) is proposed. Threshold dominance is proposed. EDUK is the first algorithm to exploit collective and threshold dominances. Old datasets receive some criticism for their high percentage of dominated items. New artificial datasets without the same flaws of the previous datasets are proposed. EDUK is compared to MTU2.\\
%2000 & \cite{eduk} & The new DP method only compares to B\&B and naive DP, the old non-naive DP algorithms were forgotten or excluded because of previous experiments. \\
2004 & \cite{book_ukp_2004} & A book on knapsack problems cite EDUK as state-of-the-art dynamic programming for the UKP. \\
2009 & \cite{pya} & EDUK2 is proposed. It consists in EDUK hybridized with B\&B. The datasets are updated to be `harder'. MTU2 is used in some comparisons, but not in all because it has the risk of integer overflow. EDUK2 is compared to EDUK.\\ %Such datasets are hard for B\&B, and the hybrid method is only compared to B\&B. The hybrid method is the new state-of-the-art. \\
2016 & \cite{sea2016} & The terminating step-off is reinvented (with the name of UKP5) and outperforms the hybrid method in the updated datasets.\\
\end{tabu}
%\end{adjustbox}
\end{table}

In the remainder of this section we discuss the mathematical formulation and well-known properties of the problem.

\subsection{Formulation and notation}
\label{sec:formulation}

An instance of the UKP is a pair of a capacity~\(c\) and a list of~\(n\) items.
Each item~\(i\) can be referenced by its index in the item list~\(i \in \{1,~\dots,~n\}\), and has a weight value~\(w_i\), and a profit value~\(p_i\).
A solution is an item multiset (i.e., a set that allows multiple copies of the same element).
The sum of the items weight, or profit, of a solution~\(s\) is denoted by~\(w_s\), or~\(p_s\), and is referred to as the weight, or profit, of the solution.
%A solution~\(s\) is valid iff~\(w_s \leq c\).
%An optimal solution~\(s^*\) is a valid solution with the greatest profit value among all valid solutions.
%To solve an instance of the UKP is to find an optimal solution for that instance.
%The profit value shared by all optimal solutions is denoted by \(p_{s^*}\).
%We refer to the profit value shared among all optimal solutions for a capacity~\(y\) as~\(opt(y)\), if we omit the capacity then~\(c\) is implied.

The mathematical formulation of UKP is:

\begin{align}
  \mbox{maximize} &\sum_{i=1}^n p_i x_i\label{eq:objfun}\\
\mbox{subject~to} &\sum_{i=1}^n w_i x_i \leq c\label{eq:capcons},\\
            &x_i \in \mathbb{N}_0.\label{eq:x_integer}
\end{align}

The quantities of each item~\(i\) in a solution are denoted by~\(x_i\), and are restricted to the non-negative integers, as~Eq.~\eqref{eq:x_integer} indicates. 
We assume that the capacity~\(c\), the quantity of items~\(n\) and the weights of the items~\(w_i\) are positive integers, while the profit values of the items~\(p_i\) are positive real numbers.

The efficiency of an item~\(i\) is its profit-to-weight ratio (\(p_i/w_i\)).%, and is denoted by~\(e_i\). 
We use~\(w_{min}\), or~\(w_{max}\), to denote the lowest weight among all items, or the highest weight among all items, within an instance of the UKP.
We refer to the item with the greatest efficiency among all items of a specific instance as the \emph{best item} (or~\(b\)). If more than one item shares the greatest efficiency, then the item with the lowest weight among them is considered the best item type. 
If more than an item has both previously stated characteristics, then the first item with both characteristics in the items list is the best item.


\subsection{Properties of the UKP}
\label{sec:well_known_prop}

Algorithms that solve the UKP often exploit two properties to reduce the problem size: \emph{dominance} and \emph{periodicity}.
Dominance relations are exploited to reduce~\(n\), and periodicity is exploited to reduce~\(c\).

\subsubsection{Simple, multiple, collective, and threshold dominances}

Any item~\(j\) that does not appear in all optimal solutions of an instance can be excluded without affecting our capability of solving such instance.
Given two items~\(i\) and~\(j : j \neq i\), if~\(w_i \leq w_j\) and~\(p_i \geq p_j\), then~\(j\) cannot be in all optimal solutions, since for any optimal solution that contains~\(j\) there will exist another optimal solution with~\(i\) in place of~\(j\).
Consequently, \(j\)~can be ignored when solving an instance that contains both~\(i\) and~\(j\).
Such relationship between~\(i\) and~\(j\) is called \emph{simple dominance}, more specifically we can say that~\(i\) simple dominates~\(j\) (or that~\(j\) is simple dominated by~\(i\)).

Given a positive integer~\(\alpha\), if~\(\alpha \times w_i \leq w_j\) and~\(\alpha \times p_i \geq p_j\), then~\(\alpha\) copies of~\(i\) can replace one of~\(j\),  and~\(j\) can be ignored.
Such relationship is called \emph{multiple dominance}, and it generalizes simple dominance in which \(\alpha = 1\).

Given a valid solution~\(s\), if~\(w_s \leq w_j\) and~\(p_s \geq p_j\), then the items that compose~\(s\) can replace~\(j\), and~\(j\) can be ignored.
Such relationship is called \emph{collective dominance}~\cite{ukp_new_results}, and it generalizes multiple dominance in which~\(s\) consists of~\(\alpha\) copies of~\(i\).

If~\(w_s \leq \beta \times w_j\) and~\(p_s \geq \beta \times p_j\), then the items that compose~\(s\) can replace~\(\beta\) copies of~\(j\), and solutions including~\(\beta\) or more copies of~\(j\) can be ignored.
Such relationship is called \emph{threshold dominance}, and it generalizes collective dominance in which \(\beta = 1\).
Threshold dominance with~\(\beta > 1\) does not allow to exclude an item~\(j\) as a preprocessing phase, but it reduces the search space by allowing the algorithm to ignore all solutions in which~\(x_j \geq \beta\).
The~\autoref{fig:dominances} presents a small set of items in which these four dominance relations can be observed.

\begin{figure}
  \centering
  \caption{Examples of the simple, multiple, collective, threshold and solution dominances. The solution dominance is proposed by the authors in~\autoref{sec:oso_and_sol_dom}. The item set considered in the three graphs is: (3, 2), (5, 5), (6, 1), (12, 9), (14, 11), (16, 13), (17, 19). An item or solution dominates any item or solution that have the same weight or more and the same profit or less (dashed lines). The dominated items and solutions are labeled with the type(s) of dominance they are subject to. Source: primary.}
  \label{fig:dominances}
  \hspace{0.4cm}
\edef \scale {0.15}
\edef \myrad {0.5}
\begin{tikzpicture}[scale=\scale]

% simple & multiple dominance

\drawaxis{24}{20}{weight}{profit}

\draw [fill] (3, 2) circle [radius=\myrad];

\draw (0, 0) -- (15,15);
\draw [fill] (5, 5) circle [radius=\myrad];
\node [above left, font=\tiny] at (5, 5) {(5, 5)};
\draw [dashed] (5, 0) -- (5, 5) -- (24, 5);

\draw [dashed] (10, 0) -- (10, 10) -- (24, 10);
%\draw [fill=lightgray] (10 - \myrad, 10 - \myrad) rectangle +(2*\myrad, 2*\myrad);
\drawrectangle{10}{10}{\myrad}{white}
\node [above left, font=\tiny] at (10, 10) {(10, 10)};

\draw [dashed] (15, 0) -- (15, 15) -- (24, 15);
%\draw [blue,fill=white] (15, 15) circle [radius=\myrad];
\drawrectangle{15}{15}{\myrad}{white}
\node [above left, font=\tiny] at (15, 15) {(15, 15)};

%\draw [fill=red] (7, 2) circle [radius=\myrad];
%\node [right, font=\small] at (7, 2) {simple};
\draw [fill=lightgray] (6, 1) circle [radius=\myrad];
\node [right, font=\small, align=left] at (6, 1) {simple};
\draw [fill=lightgray] (12, 9) circle [radius=\myrad];
\node [right, font=\small] at (12, 9) {multiple};
\draw [fill] (14, 11) circle [radius=\myrad];
\draw [fill=lightgray] (16, 13) circle [radius=\myrad];
\node [right, font=\small] at (16, 13) {multiple};
\draw [fill] (17, 19) circle [radius=\myrad];

% collective & threshold dominance
\begin{scope}[shift={(25,0)}]
\drawaxis{24}{20}{weight}{}

\draw [] (0,0) -- (6,4);
\draw [dashed] (5, 0) -- (5, 5) -- (24, 5);
\draw [fill] (3, 2) circle [radius=\myrad];
\node [below, font=\tiny] at (3, 2) {(3, 2)};
\drawrectangle{6}{4}{\myrad}{lightgray}
\node [below right, font=\tiny] at (6, 4) {(6, 4)};
\node [right, font=\small] at (6,4) {threshold};

\draw [] (0,0) -- (10,10) -- (13,12);
\draw [fill] (5, 5) circle [radius=\myrad];
\node [above left, font=\tiny] at (5, 5) {(5, 5)};

%\draw [blue,fill=white] (10, 10) circle [radius=\myrad];
\drawrectangle{10}{10}{\myrad}{white}
\node [above left, font=\tiny] at (10, 10) {(10, 10)};

\draw [dashed] (13, 0) -- (13, 12) -- (24, 12);
%\draw [blue,fill=white] (13, 12) circle [radius=\myrad];
\drawrectangle{13}{12}{\myrad}{white}
\node [above left, font=\tiny] at (13, 12) {(13, 12)};

%\draw [fill] (7, 2) circle [radius=\myrad];
%\draw [fill] (11, 4) circle [radius=\myrad];
\draw [fill=lightgray] (6, 1) circle [radius=\myrad];
%\node [right, font=\small, align=left] at (6, 1) {simple};
\draw [fill] (12, 9) circle [radius=\myrad];
\draw [fill=lightgray] (14, 11) circle [radius=\myrad];
\node [right, font=\small] at (14, 11) {collective};
\draw [fill] (16, 13) circle [radius=\myrad];
\draw [fill] (17, 19) circle [radius=\myrad];
\end{scope}

% solution dominance
\begin{scope}[shift={(50 ,0)}]
\drawaxis{24}{20}{weight}{}

%\draw [dashed] (5, 0) -- (5, 5) -- (24, 5);
%\draw [] (0,0) -- (6,4);
\node [below, font=\tiny] at (3, 2) {(3, 2)};
\draw [fill] (3, 2) circle [radius=\myrad];
%\node [right, font=\tiny] at (6, 4) {(6, 4)};
%\draw [blue,fill=lightgray] (6, 4) circle [radius=\myrad];
%\drawrectangle{6}{4}{\myrad}{lightgray}
%\node [below right, font=\small] at (6,4) {threshold};

\draw [] (0, 0) -- (15,15) -- (18,17);

\draw [fill] (5, 5) circle [radius=\myrad];
\node [above left, font=\tiny] at (5, 5) {(5, 5)};

%\draw [blue,fill=white] (10, 10) circle [radius=\myrad];
\drawrectangle{10}{10}{\myrad}{white}
\node [above left, font=\tiny] at (10, 10) {(10, 10)};

%\draw [blue,fill=white] (15, 15) circle [radius=\myrad];
\drawrectangle{15}{15}{\myrad}{white}
\node [above left, font=\tiny] at (15, 15) {(15, 15)};

%\draw [blue,fill=red] (20, 20) circle [radius=\myrad];
%\node [above, font=\small] at (20, 20) {threshold};
%\node [right, font=\tiny] at (20, 20) {(20, 20)};

%\draw [fill] (7, 2) circle [radius=\myrad];
%\draw [fill=lightgray] (11, 4) circle [radius=\myrad];
%\node [right, font=\small, align=left] at (11, 4) {simple};
\draw [fill] (6, 1) circle [radius=\myrad];
\draw [fill] (12, 9) circle [radius=\myrad];
\draw [fill] (14, 11) circle [radius=\myrad];
\draw [fill] (16, 13) circle [radius=\myrad];

%\draw [blue,fill=lightgray] (18, 17) circle [radius=\myrad];
\drawrectangle{18}{17}{\myrad}{lightgray}
\node [right, font=\tiny] at (18,17) {(18, 17)};
\node [below right, font=\small] at (18,17) {solution};

\node [left, font=\tiny] at (17, 19) {(17, 19)};
\draw [fill] (17, 19) circle [radius=\myrad];
\draw [dashed] (17, 0) -- (17, 19) -- (24, 19);
\end{scope}

\draw [fill] (-1, -4) circle [radius=\myrad];
\node [right, font=\footnotesize] at (-1,-4) {undominated item};
\draw [fill=lightgray] (18, -4) circle [radius=\myrad];
\node [right, font=\footnotesize] at (18,-4) {dominated item};
\drawrectangle{35}{-4}{\myrad}{white}
\node [right, font=\footnotesize] at (35,-4) {undominated solution};
\drawrectangle{57}{-4}{\myrad}{lightgray}
\node [right, font=\footnotesize] at (57,-4) {dominated solution};

\end{tikzpicture}
\end{figure}

% TODO: maybe create and add image
%\begin{figure}
%  \centering
%  \includegraphics[width=1\textwidth]{threshold_dominance}
%  \caption{Classical graphic representation of the threshold dominance. Source: \cite{eduk}. The \(t_i\) is the \emph{t}hreshold of item \(i\), i.e. the weight of the smallest solution composed only of copies of \(i\) that is dominated by another item/solution.}
%\end{figure}

\subsubsection{Periodicity}

%After such capacity, all items but the best one are dominated.
%In other words, for any capacity greater than~\(y^+\), an optimal solution can be reached by adding copies of \(b\) to an optimal solution of capacity~\(y^+\) or lower.
The \emph{periodicity} property shows the existence of a capacity \(y^+\) such that, for every capacity~\(y'\)~:~\(y'>y^+\), there exists an optimal solution for capacity~\(y'\) that is the same as an optimal solution for capacity~\(y' - w_b\) except it includes one more copy of the best item~\(b\)~\cite[p.~10]{gg-66}.
If~\(y^+ < c\), then the UKP can be solved for capacity~\(y^{*} = c - \ceil{(c - y^+)/w_b}w_b\), and the gap between~\(y^{*}\) and~\(c\) filled with exactly~\((c - y^{*})/w_b\) copies of the best item \(b\), effectively reducing the knapsack size from~\(c\) to~\(y^*\) (see~\autoref{fig:periodicity}).
However, the effort necessary to compute the exact value of \(y^+\) (and \(y^*\)) is about the same as solving the UKP for all capacities \(y \leq y^+\) while checking for threshold dominance.
Consequently, \(y^+\) is implicitly determined by some dynamic programming algorithms (as EDUK) while solving instances in which \(y^+ < c\).
An upper bound on~\(y^+\) is less valuable than~\(y^+\) itself, but it can be computed in polynomial time, as a preprocessing phase.
The authors discuss the usefulness of computing or bounding~\(y^+\) in~\autoref{sec:critique_of_periodicity}.

\begin{figure}
  \centering
  \caption{
    Capacities and their connection with periodicity in an instance where \(y^+ < c\).
    % Algorithms that solve the UKP only for an specific capacity can solve for \(y^*\) instead of \(c\).
    % Algorithms that solve the UKP for all capacities up to \(c\) can solve up to \(y^*\) instead, but if they solve up to \(y^+\) then an optimal solution for any subsequent capacity can be computed in \(O(1)\).
    Source: primary.}
  \hspace{0.4cm}
  \label{fig:periodicity}
\edef \scale {0.5}
\begin{tikzpicture}[scale=\scale]
\drawhvectorfill{(0, 0)}{22}{1}{1}{white}
\node [scale=\scale, font=\Huge] at (0 + 0.5, 0.5) {\(0\)};
\node [scale=\scale, font=\Huge] at (1 + 0.5, 0.5) {\(1\)};
\node [scale=\scale, font=\Huge] at (2 + 0.5, 0.5) {\(2\)};
\node [scale=\scale, font=\Huge] at (3 + 0.5, 0.5) {\(3\)};
\node [scale=\scale, font=\Huge] at (4 + 0.5, 0.5) {\(...\)};
\node [scale=\scale, font=\Huge] at (8 + 0.5, 0.5) {\(y^*\)};
\node [scale=\scale, font=\Huge] at (10 + 0.5, 0.5) {\(y^+\)};
\node [scale=\scale, font=\Huge] at (20 + 0.5, 0.5) {\(c\)};
\node [scale=\scale, font=\Huge] at (21 + 0.5, 0.5) {\(...\)};
\node [scale=\scale, font=\Huge] at (22 + 0.5, 0.5) {\(\infty\)};
\draw[dashed, ultra thick, ->] (20.4, 1) to [out=135,in=45] (14.6, 1);
\node [scale=\scale, below, font=\Huge] at (17.4, 2) {\(-w_b\)};
\draw[dashed, ultra thick, ->] (14.4, 1) to [out=135,in=45] (8.6, 1);
\node [scale=\scale, below, font=\Huge] at (11.4, 2) {\(-w_b\)};
%\draw[dashed, ultra thick, ->] (12.4, 1) to [out=135,in=45] (8.6, 1);
%\node [scale=\scale, above, font=\Huge] at (10.5, 2) {\(-w_b\)};
\draw[decorate, decoration={brace, mirror, raise=5pt}] (0.1, 0) -- (4.9, 0);
\node [below, font=\small, align=center] at (2.5, -0.5) {not relevant for \\ periodicity};
\draw[decorate, decoration={brace, mirror, raise=5pt}] (5.1, 0) -- (10.9, 0);
\node [below, font=\small, align=center] at (8, -0.5) {\([y^+ + 1 - w_{b}, y^+]\)};
\draw[decorate, decoration={brace, mirror, raise=5pt}] (11.1, 0) -- (22.9, 0);
\node [below, font=\small, align=center] at (17, -0.5)
{ optimal solutions can be computed \\
  in \(O(1)\) if optimal solutions for range \\
  \([y^+ + 1 - w_{b}, y^+]\) are known};
\end{tikzpicture}
\end{figure}

The periodicity property is a direct consequence of the threshold dominance.
Except by the best item~\(b\), every item~\(j : j \neq b\) is threshold dominated for some~\(\alpha_j\).
For example, given that~\(y\) is the lowest common multiple of~\(w_b\) and~\(w_j\),~\(\beta_j = y/w_b\) and~\(\alpha_j = y/w_j\), then~\(\beta_j \times w_b \leq \alpha_j \times w_j \equiv y \leq y\) and~\(\beta_j \times p_b \geq \alpha_j \times p_j\). %, since \(e_b \geq e_j\) (from the definition of the best item).
Consequently, the threshold dominance defines a constraint~\(x_j < \alpha_j\) on every non-best item~\(j\).
All solutions that do not break such constraints and do not make use of the best item weight less than~\(y'' = \sum \alpha_j \times w_j\).
Consequently, an optimal solution for any capacity greater than \(y''\) can be obtained by the procedure described in the previous paragraph.
The capacity~\(y''\) is an upper bound on~\(y^+\)~\cite[p.~215]{book_ukp_2004}.

\section{The revisited ordered step-off and (weak) solution dominance}
\label{sec:oso_and_sol_dom}

Given two valid solutions~\(s\) and~\(t\), if~\(w_s \leq w_t\) and~\(p_s \geq p_t\), then solution~\(s\) can replace solution~\(t\), and any solution that is a superset of \(t\) can be ignored (see \autoref{fig:dominances}).
The authors call this relationship \emph{solution dominance}, and it generalizes threshold dominance in which solution~\(t\) can only consist of copies of the same item.
As far the authors know, the concept of solution dominance was not formalized before.
The authors believe such overlook happened because: 1) the original concept of dominance focused on discarding single items from further consideration; 2) an algorithm that fully exploits such dominance relation has not been proposed.

The ordered step-off (OSO) is a dynamic programming algorithm for the UKP~\cite{gg-66}.
The authors revisited the OSO and added the \emph{else if} found in lines \ref{added_elsif}~to~\ref{if_new_lower_bound_end} of Algorithm 1.
When two or more solutions generated have the same weight and profit (a special case of solution dominance), the original algorithm arbitrarily keeps the first solution generated and ignores the others.
In the same situation, the revisited algorithm keeps the solution that will generate less descendents (smallest \(d[y]\)), therefore minimizing the computational effort.
This change of tiebreaker reduced the run times of the algorithm over subset-sum and strongly correlated instances by orders of magnitude.
Such performance gain can be explained by the fact that, in subset-sum instances, all solutions with the same weight have the same profit and, in strongly correlated instances, all solutions with the same weight and the same number of items have the same profit.
%Without this change, between all solutions of the same weight, the algorithm would keep the first solution generated with the greatest profit value.
%With the change, between all solutions with the same weight, the algorithm keeps the first solution generated with the greatest profit value and smallest index of the last item added to the solution.

\begin{algorithm}[!htb]
\caption{The revisited ordered step-off (R-OSO)}
\begin{algorithmic}[1]
\Procedure{oso}{$n, c, w, p$}
  \State~Sort \(w\) and \(p\) by non-increasing item efficiency\footnotemark and find $w_{min}, w_{max}$ 
  \State~\(g \gets\) array of~\(c\) positions, initialized with~\(0\)\label{create_g}
  \State~\(d \gets\) array of~\(c\) positions, uninitialized\label{create_d}
  
  \For{\(i \gets n\) to \(1\)}\label{begin_trivial_bounds}\Comment{Stores one-item solutions}
%  \For{\(i \gets 1\) to \(n\)}\label{begin_trivial_bounds}\Comment{Stores one-item solutions}
%    \If{\(g[w_i] < p_i\)}
      \State~\(g[w_i] \gets p_i\)
      \State~\(d[w_i] \gets i\)
%    \EndIf
  \EndFor\label{end_trivial_bounds}

  \State~\(opt \gets 0\)\label{init_opt}

  %\For{\(y \gets w_{min}\) to \(c - w_{min}\)}\label{main_ext_loop_begin}%\Comment{Can end earlier because of periodicity check}
  \For{\(y \gets w_{min}\) to \(c\)}\label{main_ext_loop_begin}%\Comment{Can end earlier because of periodicity check}
    \If{\(g[y] \leq opt\)}\label{if_less_than_opt_begin}\Comment{Prunes dominated solutions}
        \State \textbf{continue}\label{alg:continue}\Comment{Ends current iteration and begins the next}
    \EndIf\label{if_less_than_opt_end}
    
    \State~\(opt \gets g[y]\)\label{update_opt}
    
    \For{\(i=1\) to \(d[y]\)}\label{main_inner_loop_begin}\Comment{Creates new solutions (never symmetric)}
      \If{\(y + w_i > c\)}\Comment{To avoid accessing invalid positions}
        \State~\textbf{continue}
      \EndIf
      \If{\(g[y + w_i] < g[y] + p_i\)}\label{if_new_lower_bound_begin}
        \State~\(g[y + w_i] \gets g[y] + p_i\)
        \State~\(d[y + w_i] \gets i\)
      \ElsIf{\(g[y + w_i] = g[y] + p_i\) \textbf{and} \(i < d[y + w_i]\)}\label{added_elsif}
        \State~\(d[y + w_i] \gets i\)
      \EndIf\label{if_new_lower_bound_end}
    \EndFor\label{main_inner_loop_end}
  \EndFor\label{main_ext_loop_end}

%  \For{\(y \gets c - w_{min} + 1\) to \(c\)}
%    \If{\(g[y] > opt\)}
%      \State~\(opt \gets g[y]\)
%    \EndIf
%  \EndFor
  \State \textbf{return}~\(opt\)

%  \For{\(y \gets c-w_{min}+1, c\)}\label{get_y_opt_loop_begin}\Comment{Removal of dominated solutions}
%    \If{\(g[y] > opt\)}\label{last_loop_inner_if}
%      \State~\(opt \gets g[y]\)
%      \State~\(y_{opt} \gets y\)
%    \EndIf
%  \EndFor\label{get_y_opt_loop_end}
\EndProcedure
\end{algorithmic}
\end{algorithm}
\footnotetext{If two or more items share the same efficiency they are sorted by non-decreasing weight.}

The (R-)OSO implements what the authors chose to call \emph{weak solution dominance}.
The implementation of \emph{weak solution dominance} has no overhead but also does not avoid generating all supersets of dominated solutions (only some of them).
The notation~\(min_{ix}(s)\) refer to index of the item of lowest index present in solution~\(s\); the notation~\(max_{ix}(s)\) has the analogue meaning.
If a solution~\(t\) is pruned because~\(s\) dominates~\(t\) (lines~\ref{if_less_than_opt_begin} to~\ref{if_less_than_opt_end}), only the solutions~\(u : t \subset u \land min_{ix}(u) \leq min_{ix}(t)\) are guaranteed to not be generated.
CONTINUE IN THIS PHRASE For example, if~\(\{3, 2\}\) is dominated, then~\(\{3, 2, 2\}\) and~\(\{3, 2, 1\}\) will never be generated by the OSO, but~\(\{3,2,3\}\) or~\(\{3,2,5\}\) could have been already generated or (note that, in reality, it is the ordered equivalent~\([3,3,2]\) and~\([5,3,2]\) that could yet be generated).
Ideally, any~\(u : t \subset u\) should not be generated as it would be dominated by a solution~\(u' : s \subset u'\). 
This eventually happens, as any~\(t \cap \{i\} : i > min_{ix}(t)\) will be dominated by~\(s \cap \{i\}\) (or by a solution that dominates~\(s \cap \{i\}\)), and at some point no solution that is a superset of~\(t\) will be generated anymore.

\section{A critique of periodicity tests and bounds}
\label{sec:critique_of_periodicity}

As another contribution of this paper, the authors point out the overlooked negligibility of periodicity checks and periodicity bounds.
Many DP algorithms for the UKP present some form of implicit periodicity checking based on the application of the item dominances.
Two examples of such algorithms are: the terminating step-off (TSO) and the EDUK algorithms.
%These two algorithms have different mechanisms for checking periodicity, but both consist in \(O(y + n)\) extra operations, in which \(y\) is the last capacity considered before stopping the algorithm.
%To base the discussion, an explanation of the EDUK periodicity check mechanism follows: EDUK keeps a global items list, from which it removes items if they are simple, multiple, collective, or threshold dominated; if such list is reduced to a single item (that only can be the best item), the algorithm stops. 
%The TSO is a variant of the OSO which detects if the best item is the only item that can yet be added to solutions, and stops the algorithm if this is the case.
%The TSO periodicity checking mechanism is not the same as the one used by EDUK, as the TSO does remove items from a global item list, but instead checks the \(d\) array (the same described above in Algorithm 1) and an auxiliary array (which only exists in TSO and not in OSO).

If one of these periodicity checks stop its respective algorithm at capacity \(y\), it saves the effort of iterating the last \(c - y\) positions.
However, in the range \((y,c)\) the algorithm would not execute \(O(n)\) steps for each capacity anymore, but only \(O(1)\) steps, as only the best item is still relevant.
Consequently, periodicity checks often demand about \(n\) and up to \(y \leq c\) extra operations to save up to \(c - y\) operations.
Even in the best case, that is when \(y\) is close to zero, the magnitude of the amount of effort saved (about \(c\) operations) is small in comparison to \(O(nc)\) (unless for toy instances with very small \(n\)).
In the worst case, that is when \(y\) is close to \(c\), then the doing the periodicity check wastes more effort than it saves.
This way, periodicity checks are a small optimization in comparison to the application of item/solution dominance that makes them possible.

If periodicity checks save little effort, then the computating of upper bounds on~\(y^+\) should save up even less effort.
When the global item list of EDUK is reduced to the best item, the current capacity is probably a far better upper bound on \(y^+\) than any upper bound computed by a polynomial algorithm.
In a dynamic programming algorithm with periodicity checking, the utility of upper bounds on~\(y^+\) is restricted to saving memory and saving the time spent initializing it (which is insignificant).
While branch-and-bound algorithms does not have periodicity checking, they also do not obtain any considerable benefit from periodicity bounds, as their bound computation already supersedes them.
%If an upper bound on \(y^+\) can be computed, item dominance and the periodicity checks stops the algorithm before the capacity predicted by the upper bound.

