\section{Introduction}
\label{sec:introduction}

The objective of this work is to provide a more complete comparison of the exact algorithms for solving the Unbounded Knapsack Problem (UKP) than the comparisons found in the recent literature.
The UKP is very similar to the Bounded Knapsack Problem (BKP) and the 0-1 Knapsack Problem (0-1 KP).
The only difference between the UKP and the BKP (or the 0-1 KP) is that the UKP has an unlimited quantity of each item available.
The UKP is a weakly NP-Hard problem, as are the BKP and the 0-1 KP.
%The UKP can also be seen as a special case of the BKP in which, for each item type, there are more copies available than is possible to fit in the knapsack capacity.

% TODO: consider removing the phrase below and keeping the rest of the paragraph
The UKP is the pricing subproblem generated by solving the continuous relaxation of the set covering formulation for the unidimensional Bin Packing Problem (BPP) and Cutting Stock Problem (CSP) using the column generation approach.
The BPP and the CSP are classical problems in the area of operations research\cite{gg-61,gg-63,survey2014}.
The best lower bounds known for the optimal solution value of the BPP and the CSP are the optimal solution value for their continuous relaxations.
The tightest formulation for the BPP and the CSP has an exponential number of columns and, because of this, is solved by using the column generation approach~\cite{gg-61}.
The UKP is the pricing subproblem of this column generation approach.

\subsection{Formulation and notation}
\label{sec:formulation}

An instance of the UKP is a pair of a capacity~\(c\) and a list of~\(n\) items.
Each item can be referenced by its index in the item list~\(i \in \{1,~\dots,~n\}\).
Each item~\(i\) has a weight value~\(w_i\), and a profit value~\(p_i\).
A solution is an item multiset (i.e, a set that allows multiple copies of the same element).
The sum of the items weight, or profit, of a solution~\(s\) is denoted by~\(w_s\), or~\(p_s\); and will be referred to as the weight, or profit, of the solution.
A solution~\(s\) is valid iff~\(w_s \leq c\).
An optimal solution~\(s^*\) is a valid solution with the greatest profit value among all valid solutions.
To solve an instance of the UKP is to find an optimal solution for that instance.
%The profit value shared by all optimal solutions is denoted by \(p_{s^*}\).
%We refer to the profit value shared among all optimal solutions for a capacity~\(y\) as~\(opt(y)\), if we omit the capacity then~\(c\) is implied.

The mathematical formulation of UKP is:

\begin{align}
  \mbox{maximize} &\sum_{i=1}^n p_i x_i\label{eq:objfun}\\
\mbox{subject~to} &\sum_{i=1}^n w_i x_i \leq c\label{eq:capcons},\\
            &x_i \in \mathbb{N}_0.\label{eq:x_integer}
\end{align}

The quantities of each item~\(i\) in a solution are denoted by~\(x_i\), and are restricted to the non-negative integers, as~\eqref{eq:x_integer} indicates. 
We assume that the capacity~\(c\), the quantity of items~\(n\) and the weights of the items~\(w_i\) are positive integers. 
We assume that the profit values of the items~\(p_i\) are positive real numbers.

The efficiency of an item~\(i\) is its profit-to-weight ratio (\(\frac{p_i}{w_i}\)), and is denoted by~\(e_i\). 
We use~\(w_{min}\), or~\(w_{max}\), to denote the lowest weight among all items, or the highest weight among all items, within an instance of the UKP.
We refer to the item with greatest efficiency among all items of an specific instance as the \emph{best item} (or~\(b\)); if more than one item shares the greatest efficiency, then the item with the lowest weight among them will be considered the best item type; if more than an item has both previously stated characteristics, then the first item with both characteristics in the items list is the best item.

\subsection{Prior work}

In order to contextualize the reader, the authors present some selected papers related to the UKP and a summary of their relevance for the UKP.

\begin{table}[!htb]
\label{tab:prior_work}
%\begin{adjustbox}{max width=\textwidth, center}
\begin{tabu}{clp{10cm}}
Year & Ref. & Notes\\
\hline
1961 & \cite{gg-61} & The column generation approach for BPP and CSP continuous relaxation is proposed, the UKP is the pricing problem of such approach.\\
1966 & \cite{gg-66} & The ordered and terminating step-off algoritms (dynamic programming algorithms to solve the UKP) are proposed. \\
1977 & \cite{mtu1} & The MTU1 (branch-and-bound algorithm) is proposed, and then compared with the previous algorithms over artificial instances up to a hundred items, obtaining marginally better results \\
1990 & \cite{mtu2} & Datasets of instances with up to 250,000 items (but rich in simple and multiple dominance) are proposed. MTU2 is proposed as an improvement of MTU1 for such dataset.\\
%1997 & \cite{zhu_dominated} & Uncorrelated datasets are shown to  \\
%1997 & \cite{babayev} & A new solving approach, more similar to dynamic programming than B\&B \\
1998 & \cite{ukp_new_results,eduk} & The EDUK (a dynamic programming algorithm) is proposed. Old datasets receive some criticism for their dominance. New artificial datasets without the same flaws of the previous datasets are proposed. EDUK is compared to MTU2. The threshold dominance is proposed. EDUK is the first algorithm to exploit collective and threshold dominance.\\
%2000 & \cite{eduk} & The new DP method only compares to B\&B and naive DP, the old non-naive DP algorithms were forgotten or excluded because of previous experiments. \\
2004 & \cite{book_ukp_2004} & A book on knapsack problems cite EDUK as state-of-the-art dynamic programming for the UKP. \\
2009 & \cite{pya} & EDUK2 is proposed, it consists in EDUK hybridized with B\&B. The datasets are updated to be `harder'. MTU2 is used in some comparisons, but not all because it has the risk of integer overflow. EDUK2 is compared to EDUK.\\ %Such datasets are hard for B\&B, and the hybrid method is only compared to B\&B. The hybrid method is the new state-of-the-art. \\
2016 & \cite{sea2016} & The terminating step-off is reinvented (with the name of UKP5) and outperforms the hybrid method in the updated datasets.\\
\end{tabu}
%\end{adjustbox}
\end{table}

\subsection{Properties of the UKP}
\label{sec:well_known_prop}

Algorithms that solve the UKP often exploit two properties to reduce the problem size, these properties are the \emph{dominance} and the \emph{periodicity}.
Dominance relations are exploited to reduce~\(n\), and periodicity is exploited to reduce~\(c\).

\subsubsection{Simple, multiple, collective and threshold dominance}

Any item~\(j\) that does not appear in all optimal solutions of an instance can be excluded without affecting our capability of solving such instance.
Given two items~\(i\) and~\(j\), if~\(w_i \leq w_j\) and~\(p_i \geq p_j\), then~\(j\) cannot be in all optimal solutions, since that for any optimal solution that contains~\(j\) there will exist another optimal solution with~\(i\) in the place of~\(j\).
Consequently, \(j\)~can be ignored when solving an instance that contains both~\(i\) and~\(j\).
Such relationship between~\(i\) and~\(j\) is called \emph{simple dominance}, more specifically we can say that~\(i\) simple dominate~\(j\) (or that~\(j\) is simple dominated by~\(i\)).

Given a positive integer~\(\alpha\), if~\(\alpha \times w_i \leq w_j\) and~\(\alpha \times p_i \geq p_j\), then~\(\alpha\) copies of~\(i\) can replace one of~\(j\),  and~\(j\) can be safely ignored.
Such relationship is called \emph{multiple dominance}, and it generalizes simple dominance in which \(\alpha = 1\).

Given a valid solution~\(s\), if~\(w_s \leq w_j\) and~\(p_s \geq p_j\), then the items that compose~\(s\) can replace~\(j\), and~\(j\) can be safely ignored.
Such relationship is called \emph{collective dominance}, and it generalizes multiple dominance in which~\(s\) consists of~\(\alpha\) copies of~\(i\).

If~\(w_s \leq \alpha \times w_j\) and~\(p_s \geq \alpha \times p_j\), then the items that compose~\(s\) can replace~\(\alpha\) copies of~\(j\), and solutions including~\(\alpha\) or more copies of~\(j\) can be safely ignored.
Such relationship is called \emph{threshold dominance}, and it generalizes collective dominance in which \(\alpha = 1\).
Threshold dominance with~\(\alpha > 1\) does not allow to exclude an item~\(j\) before starting the solving algorithm, but it reduces the search space by allowing the algorithm to ignore all solutions in which~\(x_j \geq \alpha\).

%\begin{figure}
%  \centering
%  \includegraphics[width=1\textwidth]{smc_dominance}
%  \caption{Classical graphic representation of the simple, multiple and threshold dominances. Source: \cite{eduk}. All points in the shaded area created by an item or solution triangle are dominated.}
%\end{figure}

% TODO: maybe create and add image
%\begin{figure}
%  \centering
%  \includegraphics[width=1\textwidth]{threshold_dominance}
%  \caption{Classical graphic representation of the threshold dominance. Source: \cite{eduk}. The \(t_i\) is the \emph{t}hreshold of item \(i\), i.e. the weight of the smallest solution composed only of copies of \(i\) that is dominated by another item/solution.}
%\end{figure}

\subsubsection{Periodicity}

The \emph{periodicity} property states that, for each possible list of items, there exists a capacity~\(y^+\), for which every capacity~\(y'\) (\(y' > y^+\)) have an optimal solution that is an optimal solution for capacity~\(y' - w_b\) with one more copy of the best item added.
In other words, for capacities greater than~\(y^+\), we can find an optimal solution by adding copies of the best item to an optimal solution from capacity~\(y^+\) or lower.
If~\(y^+ < c\), then the UKP can be solved for the capacity~\(y^{*} = c - \ceil{\frac{c - y^+}{w_b}}w_b\), and the gap between~\(y^{*}\) and~\(c\) filled with exactly~\(\frac{c - y^{*}}{w_b}\) copies of the best item, effectively reducing the knapsack size of the instance from~\(c\) to~\(y^*\).

The periodicity property is a direct consequence of the threshold dominance.
Except by the best item~\(b\), every item~\(j\) (\(j \neq b\)) is threshold dominated for some~\(\alpha_j\).
For example, given that~\(y\) is the lowest common multiple of~\(w_b\) and~\(w_j\),~\(\beta_j = \frac{y}{w_b}\) and~\(\alpha_j = \frac{y}{w_j}\), then~\(\beta_j \times w_b \leq \alpha_j \times w_j\) and~\(\beta_j \times p_b \geq \alpha_j \times p_j\). %, since \(e_b \geq e_j\) (from the definition of the best item).
Consequently, the threshold dominance defines a constraint~\(x_j < \alpha_j\) on every item~\(j\), and solutions that break such constraints can be safely ignored.
All solutions that do not break such constraints and do not make use of the best item weight less than~\(y' = \sum \alpha_j \times w_j\).
Consequently, an optimal solution for any capacity greater than \(y'\) can be obtained in the fashion described in the last paragraph.
The capacity~\(y'\) is a upper bound on~\(y^+\)~\cite[p.~215]{book_ukp_2004}. %The computation of the exact value of~\(y^+\) is very expensive.
Computing~\(y^+\) would equal to solving the UKP for capacities~\(y^+\) and lower while checking for threshold dominance.
An upper bound on~\(y^+\) is less valuable than~\(y^+\) itself, but it can be computed in polynomial time, before starting the solving process.

\subsection{The ordered step-off and (weak) solution dominance}

The ordered step-off is a simple dynamic programming algorithm for the UKP from \cite{gg-66}, it is described in Algorithm 1.%\autoref{alg:oso}. % ref not working with algorithm

\begin{algorithm}[!htb]
\caption{The ordered step-off (revisited)}
\begin{algorithmic}[1]
\Procedure{oso}{$n, c, w, p$}
  \State~Sort \(w\) and \(p\) by increasing item efficiency and find $w_{min}, w_{max}$ 
  \State~\(g \gets\) array of~\(c\) positions each one initialized with~\(0\)\label{create_g}
  \State~\(d \gets\) array of~\(c\) positions, uninitialized\label{create_d}
  
  \For{\(i \gets 1, n\)}\label{begin_trivial_bounds}\Comment{Stores one-item solutions}
    \If{\(g[w_i] < p_i\)}
      \State~\(g[w_i] \gets p_i\)
      \State~\(d[w_i] \gets i\)
    \EndIf
  \EndFor\label{end_trivial_bounds}

  \State~\(opt \gets 0\)\label{init_opt}

  \For{\(y \gets w_{min}, c - w_{min}\)}\label{main_ext_loop_begin}%\Comment{Can end earlier because of periodicity check}
    \If{\(g[y] \leq opt\)}\label{if_less_than_opt_begin}\Comment{Prunes dominated solutions}
    	\State \textbf{continue}\label{alg:continue}\Comment{Ends current iteration and begins the next}
    \EndIf\label{if_less_than_opt_end}
    
    \State~\(opt \gets g[y]\)\label{update_opt}
    
    \For{\(i=1,d[y]\)}\label{main_inner_loop_begin}\Comment{Creates new solutions (never symmetric)}
      \If{\(g[y + w_i] < g[y] + p_i\)}\label{if_new_lower_bound_begin}
        \State~\(g[y + w_i] \gets g[y] + p_i\)
        \State~\(d[y + w_i] \gets i\)
%      \ElsIf{\(g[y + w_i] = g[y] + p_i \land i < d[y + w_i]\)}
%        \State~\(d[y + w_i] \gets i\)
      \EndIf\label{if_new_lower_bound_end}
    \EndFor\label{main_inner_loop_end}
  \EndFor\label{main_ext_loop_end}

  \For{\(y \gets c - w_{min} + 1, c\)}
    \If{\(g[y] > opt\)}
      \State~\(opt \gets g[y]\)
    \EndIf
  \EndFor
  \State \textbf{return}~\(opt\)

%  \For{\(y \gets c-w_{min}+1, c\)}\label{get_y_opt_loop_begin}\Comment{Removal of dominated solutions}
%    \If{\(g[y] > opt\)}\label{last_loop_inner_if}
%      \State~\(opt \gets g[y]\)
%      \State~\(y_{opt} \gets y\)
%    \EndIf
%  \EndFor\label{get_y_opt_loop_end}
\EndProcedure
\end{algorithmic}
\end{algorithm}

Solution dominance is a dominance relation proposed by the authors.
Solution dominance generalize the four previous described dominances.
A solution \(t\) is solution dominated by a different solution \(s\) iff \(w_s \leq w_t\) and \(p_s \geq p_t\).
If a solution \(q\) is solution dominated, then any solution that is a superset of \(q\) could be discarded (or not generated) without risk of discarding all optimal solutions.
Without a way to enforce such dominance relation (with reasonable overhead), the concept of solution dominance is of little relevance.
However, the authors will show below 

WORK IN PROGRESS IGNORE UNTIL END OF INTRODUCTION

We use the notation~\(min_{ix}(s)\) to refer to the lowest index among the items that compose the solution~\(s\).
The notation~\(max_{ix}(s)\) has analogue meaning.

When a solution~\(t\) is pruned because~\(s\) dominates~\(t\) (lines~\ref{if_less_than_opt_begin} to~\ref{if_less_than_opt_end}), some solutions~\(u\), where~\(t \subset u\), are not generated. 
If~\(s\) dominates~\(t\), and~\(t \subset u\), and~\(max_{ix}(u - t) \leq min_{ix}(t)\), then~\(u\) is not generated by the ordered step-off. 
For example, if~\(\{3, 2\}\) is dominated, then~\(\{3, 2, 2\}\) and~\(\{3, 2, 1\}\) will never be generated by ordered step-off, but~\(\{3,2,3\}\) or~\(\{3,2,5\}\) could yet be generated (note that, in reality, it is the equivalent~\([3,3,2]\) and~\([5,3,2]\) that could yet be generated).
Ideally, any~\(u\) where~\(t \subset u\) should not be generated as it will be dominated by a solution~\(u'\) where~\(s \subset u'\) anyway. 
It is interesting to note that this happens eventually, as any~\(t \cap \{i\}\) where~\(i > min_{ix}(t)\) will be dominated by~\(s \cap \{i\}\) (or by a solution that dominates~\(s \cap \{i\}\)), and at some point no solution that is a superset of~\(t\) will be generated anymore.

%After examination, it becomes clear that these two old algorithms indirectly apply all four dominances.
%By \emph{indirectly}, we means that, in the course of these algorithms execution, they eventually stop using dominated items to create new solutions, and that happens without testing items for each one of the four dominance relations previously explained.%, and then removing the dominated items from a global items list.
%The approach used by these old algorithms is focused on solutions, not individual items.
%Consequently, it would be more adequate to say that they make use of some sort of \emph{solution dominance}.
%Ideally, given a solution~\(s\) and a solution~\(t\) (both \(s\) and \(t\) can be composed of any items), if \(w_s \leq w_t\) and \(p_s \geq p_t\), then an optimized algorithm could not generate any solutions that are a superset of~\(t\) (as the respective supersets of~\(s\) would dominate them anyway) without loss to the value of the optimal solution..
%Such dominance relation would generalize all previous dominances, as the dominated items can be seen as single item solutions (or multiple item solutions, in the case of threshold dominance).

%Those old algorithms do not apply this ideal solution dominance, but a weaker version of it.
%This weaker version of solution dominance does not avoid generating every solution that is a superset from a dominated solution, but it can be implemented with almost no overhead.
%As this is algorithm-specific, it will be further discussed in Section~\ref{sec:ukp5_sol_dom_expl}.

%\subsubsection{Periodicity and periodicity bounds}
%\label{sec:periodicity}

%The UKP has an obvious periodic property that for every knapsack capacity that is a multiple of the best item's weight, the optimal solution for that knapsack capacity is~\(\frac{c}{w_b}\) copies of the best item (it's clearly the most efficient use that can be made of such capacity). The UKP has also a not-so-obvious and more interesting periodic property that states that:


%We are not aware of the existence of an algorithm for solving the UKP that compute the exact value of~\(y^+\) before starting to solve the UKP.
%We know of algorithms that compute an \emph{upper bound} on the~\(y^+\) capacity value, as the one presented in the previous paragraph (

%Given a solution \(s\) in which \(x_j = \alpha_j - 1\) for each item \(j \neq b\), no item \(j\) can be added \(s\) without breaking the bounds.
%For any capacity \(y\) in which \(c \geq y > w_s\), an optimal solution can be found by computing the optimal solution for \(c - w_s\)

%The threshold dominance property states that, if a solution~\(s\) dominates solution~\(t\), and~\(t\) contains only~\(n\) copies of the same item type~\(j\), then solutions with~\(n\) or more copies of~\(j\) can be be replaced by solutions using~\(s\) instead (without loss to the value of the optimal solution).
%The periodicity property states that after some capacity~\(y^+\) we can obtain optimal solutions only by adding copies of the best item to the optimal solutions from capacities \(y^+\) or below (all other items are not relevant anymore).
%The link between these two properties is that \emph{for a sufficiently large capacity, solutions composed of copies of the best item will threshold dominate solutions composed of copies of any other item}.

%If one algorithm checks for threshold dominance periodically, it can stop when all non-best items have been threshold dominated by the best item.
%Such algorithm would not benefit much from computing an upper bound on the~\(y\) capacity value.
%If this algorithm setup phase (e.g. allocating and initializing memory) is linear in the knapsack capacity~\(c\) and the upper bound on the~\(y\) capacity value is considerably smaller than \(c\), then the algorithm could benefit from the upper bound. 

%There exist many proposed periodicity bounds, but some are time-consuming, as the~\(O(n^2)\) periodicity bound presented in~\cite{badbound1}.
%Others depend on specific instance characteristics to be tight, as the ones presented in~\cite{badbound2} and~\cite{pya}.
%For reasons that will be made clear in the conclusions, the author did not found relevant to present a review on periodicity bounds in this work.

The weak solution dominance reduces the further improvement that can be found by applying the four dominance relations and/or periodicity bounds in the same algorithm, and vice-versa.
The weak solution dominance and the four dominance relations are two different ways of dealing with the same task.
The first involves keeping an index in each solution to mark which items can still be added to the solution.
The second involves keeping a global list of undominated items, 

The approach used by EDUK gives a strong guarantee that any dominated item will be discarded and never used again.
However, the weak solution dominance described in Section \ref{sec:ukp5} is implemented with almost no overhead, and seems to have a very similar impact in the removal of dominated items/solutions.

The periodicity check exists both in algorithms like EDUK/EDUK2 and the terminating step-off.
In EDUK/EDUK2 it is a consequence of applying all four dominances repeatedly, and in the terminating step-off it is a consequence of applying weak solution dominance.
A periodicity check can save effort by stopping the computation at a capacity~\(y < c\).
However, in all algorithms that implement the periodicity check, when this early termination happens, it is because the only item that could possibly be used is the best item.
Consequently, in each one of these last positions (between \(y\) and \(c\)), the algorithm would not execute \(O(n)\) steps anymore, but only \(O(1)\) steps.
The periodicity check only saves the effort of iterating these last \(c - y\) positions.
It is a minor improvement over the application of weak solution dominance, or the application of the four item dominances.

The periodicity check (and, by consequence, the dominances) also reduces the utility of periodicity bounds.
If an upper bound on \(y^+\) could be used to stop the computation before it reaches capacity \(c\), then the periodicity check would stop the computation even before the capacity predicted by the upper bound (with slightly more overhead).
In an algorithm with periodicity checking, the utility of upper bounds on the periodicity capacity~\(y^+\) is restricted to saving memory and saving the time spent initializing such saved memory.
Note that some algorithms would not even have such benefits, as they do not allocate or initialize the memory in advance.

